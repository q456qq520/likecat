<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://q456qq520.github.io</id>
    <title>LIKECAT</title>
    <updated>2023-02-28T10:30:14.349Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://q456qq520.github.io"/>
    <link rel="self" href="https://q456qq520.github.io/atom.xml"/>
    <subtitle>一条小咸鱼</subtitle>
    <logo>https://q456qq520.github.io/images/avatar.png</logo>
    <icon>https://q456qq520.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, LIKECAT</rights>
    <entry>
        <title type="html"><![CDATA[《从根儿上理解MySQL》读书笔记(五)]]></title>
        <id>https://q456qq520.github.io/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-wu/</id>
        <link href="https://q456qq520.github.io/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-wu/">
        </link>
        <updated>2023-02-27T10:18:30.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="第22章-undo日志上">第22章 undo日志（上）</h2>
]]></summary>
        <content type="html"><![CDATA[<h2 id="第22章-undo日志上">第22章 undo日志（上）</h2>
<!-- more -->
<h3 id="221-事务回滚的需求">22.1 事务回滚的需求</h3>
<p>我们说过事务需要保证原子性，也就是事务中的操作要么全部完成，要么什么也不做。但是偏偏有时候事务执行到一半会出现一些情况，比如：</p>
<p>情况一：事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。<br>
情况二：程序员可以在事务执行过程中手动输入ROLLBACK语句结束当前的事务的执行。</p>
<p>这两种情况都会导致事务执行到一半就结束，但是事务执行过程中可能已经修改了很多东西，为了保证事务的原子性，我们需要把东西改回原先的样子，这个过程就称之为回滚（英文名：rollback），这样就可以造成一个假象：这个事务看起来什么都没做，所以符合原子性要求。</p>
<p>你插入了一条记录，回滚操作对应的就是把这条记录删除掉；你更新了一条记录，回滚操作对应的就是把该记录更新为旧值；你删除了一条记录，回滚操作对应的自然就是把该记录再插进去。每当我们要对一条记录做改动时（这里的改动可以指INSERT、DELETE、UPDATE），都需要把回滚时所需的东西都给记下来。比方说：</p>
<ul>
<li>你插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。</li>
<li>你删除了一条记录，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。</li>
<li>你修改了一条记录，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。</li>
</ul>
<p>数据库把这些为了回滚而记录的这些东东称之为<mark>撤销日志</mark>，英文名为<mark>undo log</mark>，称之为undo日志。这里需要注意的一点是，由于查询操作（SELECT）并不会修改任何用户记录，所以在查询操作执行时，并不需要记录相应的undo日志。</p>
<h3 id="222-事务id">22.2 事务id</h3>
<h4 id="2221-给事务分配id的时机">22.2.1 给事务分配id的时机</h4>
<p>一个事务可以是一个只读事务，或者是一个读写事务：</p>
<ul>
<li>可以通过<mark>START TRANSACTION READ ONLY</mark>语句开启一个只读事务。<br>
  在只读事务中不可以对普通的表（其他事务也能访问到的表）进行增、删、改操作，但可以对临时表做增、删、改操作。</li>
<li>可以通过<mark>START TRANSACTION READ WRITE</mark>语句开启一个读写事务，或者使用BEGIN、START TRANSACTION语句开启的事务默认也算是读写事务。<br>
  在读写事务中可以对表执行增删改查操作。</li>
</ul>
<p>如果某个事务执行过程中对某个表执行了增、删、改操作，那么InnoDB存储引擎就会给它分配一个独一无二的事务id，分配方式如下：</p>
<ul>
<li>对于只读事务来说，只有在它第一次对某个用户创建的临时表执行增、删、改操作时才会为这个事务分配一个事务id，否则的话是不分配事务id的。</li>
<li>对于读写事务来说，只有在它第一次对某个表（包括用户创建的临时表）执行增、删、改操作时才会为这个事务分配一个事务id，否则的话也是不分配事务id的。</li>
</ul>
<h4 id="2221-事务id是怎么生成的">22.2.1 事务id是怎么生成的</h4>
<p>这个事务id本质上就是一个数字，它的分配策略和我们前面提到的对隐藏列row_id（当用户没有为表创建主键和UNIQUE键时InnoDB自动创建的列）的分配策略大抵相同，具体策略如下：</p>
<ul>
<li>服务器会在内存中维护一个全局变量，每当需要为某个事务分配一个事务id时，就会把该变量的值当作事务id分配给该事务，并且把该变量自增1。</li>
<li>每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为<mark>5</mark>的页面中一个称之为<mark>Max Trx ID</mark>的属性处，这个属性占用8个字节的存储空间。</li>
<li>当系统下一次重新启动时，会将上面提到的Max Trx ID属性加载到内存中，将该值加上256之后赋值给我们前面提到的全局变量（因为在上次关机时该全局变量的值可能大于Max Trx ID属性值）。</li>
</ul>
<p>这样就可以保证整个系统中分配的事务id值是一个递增的数字。先被分配id的事务得到的是较小的事务id，后被分配id的事务得到的是较大的事务id。</p>
<h4 id="2222-trx_id隐藏列">22.2.2 trx_id隐藏列</h4>
<p>聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为trx_id、roll_pointer的隐藏列，如果用户没有在表中定义主键以及UNIQUE键，还会自动添加一个名为row_id的隐藏列。所以一条记录在页面中的真实结构看起来就是这样的：<br>
<img src="https://q456qq520.github.io/post-images/1677551326518.png" alt="" loading="lazy"></p>
<h3 id="223-undo日志的格式">22.3 undo日志的格式</h3>
<p>为了实现事务的原子性，InnoDB存储引擎在实际进行增、删、改一条记录时，都需要先把对应的undo日志记下来。一般每对一条记录做一次改动，就对应着一条undo日志，，但在某些更新记录的操作中，也可能会对应着2条undo日志。一个事务在执行过程中可能新增、删除、更新若干条记录，也就是说需要记录很多条对应的undo日志，这些undo日志会被从0开始编号，也就是说根据生成的顺序分别被称为第0号undo日志、第1号undo日志、...、第n号undo日志等，这个编号也被称之为undo no。</p>
<p>这些undo日志是被记录到类型为<mark>FIL_PAGE_UNDO_LOG</mark>（对应的十六进制是0x0002）的页面中。这些页面可以从系统表空间中分配，也可以从一种专门存放undo日志的表空间，也就是所谓的undo tablespace中分配。</p>
<p>我们先来创建一个名为undo_demo的表：</p>
<pre><code class="language-mysql">CREATE TABLE undo_demo (
    id INT NOT NULL,
    key1 VARCHAR(100),
    col VARCHAR(100),
    PRIMARY KEY (id),
    KEY idx_key1 (key1)
)Engine=InnoDB CHARSET=utf8;
</code></pre>
<p>现在我们查看一下undo_demo对应的table id是多少：</p>
<pre><code class="language-mysql">mysql&gt; SELECT * FROM information_schema.innodb_sys_tables WHERE name = 'xiaohaizi/undo_demo';
+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+
| TABLE_ID | NAME                | FLAG | N_COLS | SPACE | FILE_FORMAT | ROW_FORMAT | ZIP_PAGE_SIZE | SPACE_TYPE |
+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+
|      138 | xiaohaizi/undo_demo |   33 |      6 |   482 | Barracuda   | Dynamic    |             0 | Single     |
+----------+---------------------+------+--------+-------+-------------+------------+---------------+------------+
1 row in set (0.01 sec)
</code></pre>
<h3 id="224-insert操作对应的undo日志">22.4 INSERT操作对应的undo日志</h3>
<p>当我们向表中插入一条记录时会有乐观插入和悲观插入的区分，但是不管怎么插入，最终导致的结果就是这条记录被放到了一个数据页中。如果希望回滚这个插入操作，那么把这条记录删除就好了，也就是说在写对应的undo日志时，主要是把这条记录的主键信息记上。所以InnoDB设计了一个类型为<mark>TRX_UNDO_INSERT_REC</mark>的undo日志，它的完整结构如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1677551633425.png" alt="" loading="lazy"></p>
<ul>
<li>undo no在一个事务中是从0开始递增的，也就是说只要事务没提交，每生成一条undo日志，那么该条日志的undo no就增1。</li>
<li>如果记录中的主键只包含一个列，那么在类型为TRX_UNDO_INSERT_REC的undo日志中只需要把该列占用的存储空间大小和真实值记录下来，如果记录中的主键包含多个列，那么每个列占用的存储空间大小和对应的真实值都需要记录下来（图中的len就代表列占用的存储空间大小，value就代表列的真实值）。</li>
</ul>
<blockquote>
<p>当我们向某个表中插入一条记录时，实际上需要向聚簇索引和所有的二级索引都插入一条记录。不过记录undo日志时，我们只需要考虑向聚簇索引插入记录时的情况就好了，因为其实聚簇索引记录和二级索引记录是一一对应的，我们在回滚插入操作时，只需要知道这条记录的主键信息，然后根据主键信息做对应的删除操作，做删除操作时就会顺带着把所有二级索引中相应的记录也删除掉。DELETE操作和UPDATE操作对应的undo日志也都是针对聚簇索引记录而言的</p>
</blockquote>
<h4 id="2241-roll_pointer隐藏列的含义">22.4.1 roll_pointer隐藏列的含义</h4>
<p>这个占用7个字节的字段本质上就是一个指向记录对应的undo日志的一个指针，undo日志被存放到了类型为FIL_PAGE_UNDO_LOG的页面中。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1677551914332.png" alt="" loading="lazy"></figure>
<p><mark>roll_pointer本质就是一个指针，指向记录对应的undo日志。</mark></p>
<h3 id="225-delete操作对应的undo日志">22.5 DELETE操作对应的undo日志</h3>
<p>插入到页面中的记录会根据记录头信息中的next_record属性组成一个单向链表，我们把这个链表称之为正常记录链表；被删除的记录其实也会根据记录头信息中的next_record属性组成一个链表，只不过这个链表中的记录占用的存储空间可以被重新利用，所以也称这个链表为垃圾链表。Page Header部分有一个称之为PAGE_FREE的属性，它指向由被删除记录组成的垃圾链表中的头节点。</p>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1677552050687.png" alt="" loading="lazy"></figure>
<p>假设现在我们准备使用DELETE语句把正常记录链表中的最后一条记录给删除掉，其实这个删除的过程需要经历两个阶段：</p>
<p>阶段一：仅仅将记录的delete_mask标识位设置为1，其他的不做修改（其实会修改记录的trx_id、roll_pointer这些隐藏列的值）。InnoDB把这个阶段称之为delete mark。也就是正常记录链表中的最后一条记录的delete_mask值被设置为1，但是并没有被加入到垃圾链表。也就是此时记录处于一个中间状态</p>
<p>阶段二：当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉。所谓真正的删除就是把该记录从正常记录链表中移除，并且加入到垃圾链表中，然后还要调整一些页面的其他信息，比如页面中的用户记录数量PAGE_N_RECS、上次插入记录的位置PAGE_LAST_INSERT、垃圾链表头节点的指针PAGE_FREE、页面中可重用的字节数量PAGE_GARBAGE、还有页目录的一些信息等等。InnoDB把这个阶段称之为<mark>purge</mark>。</p>
<p>我们还要注意一点，将被删除记录加入到垃圾链表时，实际上加入到链表的头节点处，会跟着修改PAGE_FREE属性的值。</p>
<blockquote>
<p>小贴士：页面的Page Header部分有一个PAGE_GARBAGE属性，该属性记录着当前页面中可重用存储空间占用的总字节数。每当有已删除记录被加入到垃圾链表后，都会把这个PAGE_GARBAGE属性的值加上该已删除记录占用的存储空间大小。PAGE_FREE指向垃圾链表的头节点，之后每当新插入记录时，首先判断PAGE_FREE指向的头节点代表的已删除记录占用的存储空间是否足够容纳这条新插入的记录，如果不可以容纳，就直接向页面中申请新的空间来存储这条记录。如果可以容纳，那么直接重用这条已删除记录的存储空间，并且把PAGE_FREE指向垃圾链表中的下一条已删除记录。但是这里有一个问题，如果新插入的那条记录占用的存储空间大小小于垃圾链表的头节点占用的存储空间大小，那就意味头节点对应的记录占用的存储空间里有一部分空间用不到，这部分空间就被称之为碎片空间。那这些碎片空间岂不是永远都用不到了么？其实也不是，这些碎片空间占用的存储空间大小会被统计到PAGE_GARBAGE属性中，这些碎片空间在整个页面快使用完前并不会被重新利用，不过当页面快满时，如果再插入一条记录，此时页面中并不能分配一条完整记录的空间，这时候会首先看一看PAGE_GARBAGE的空间和剩余可利用的空间加起来是不是可以容纳下这条记录，如果可以的话，InnoDB会尝试重新组织页内的记录，重新组织的过程就是先开辟一个临时页面，把页面内的记录依次插入一遍，因为依次插入时并不会产生碎片，之后再把临时页面的内容复制到本页面，这样就可以把那些碎片空间都解放出来（很显然重新组织页面内的记录比较耗费性能）。</p>
</blockquote>
<p>在删除语句所在的事务提交之前，只会经历阶段一，也就是delete mark阶段（提交之后我们就不用回滚了，所以只需考虑对删除操作的阶段一做的影响进行回滚）。InnoDB设计了一种称之为<mark>TRX_UNDO_DEL_MARK_REC</mark>类型的undo日志，它的完整结构如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1677552432693.png" alt="" loading="lazy"></p>
<ul>
<li>在对一条记录进行delete mark操作前，需要把该记录的旧的trx_id和roll_pointer隐藏列的值都给记到对应的undo日志中来，就是我们图中显示的old trx_id和old roll_pointer属性。这样有一个好处，那就是可以通过undo日志的old roll_pointer找到记录在修改之前对应的undo日志。执行完delete mark操作后，它对应的undo日志和INSERT操作对应的undo日志就串成了一个链表，这个链表就称之为版本链</li>
<li>与类型为TRX_UNDO_INSERT_REC的undo日志不同，类型为TRX_UNDO_DEL_MARK_REC的undo日志还多了一个索引列各列信息的内容，也就是说如果某个列被包含在某个索引中，那么它的相关信息就应该被记录到这个索引列各列信息部分，所谓的相关信息包括该列在记录中的位置（用pos表示），该列占用的存储空间大小（用len表示），该列实际值（用value表示）。所以索引列各列信息存储的内容实质上就是&lt;pos, len, value&gt;的一个列表。这部分信息主要是用在事务提交后，对该中间状态记录做真正删除的阶段二，也就是purge阶段中使用的。</li>
</ul>
<h3 id="226-update操作对应的undo日志">22.6 UPDATE操作对应的undo日志</h3>
<p>在执行UPDATE语句时，InnoDB对更新主键和不更新主键这两种情况有截然不同的处理方案。</p>
<h5 id="2261-不更新主键的情况">22.6.1 不更新主键的情况</h5>
<p>在不更新主键的情况下，又可以细分为被更新的列占用的存储空间不发生变化和发生变化的情况。</p>
<ol>
<li>
<p>就地更新（in-place update）<br>
更新记录时，对于被更新的每个列来说，如果更新后的列和更新前的列占用的存储空间都一样大，那么就可以进行就地更新，也就是直接在原记录的基础上修改对应列的值。</p>
</li>
<li>
<p>先删除掉旧记录，再插入新记录<br>
在不更新主键的情况下，如果有任何一个被更新的列更新前和更新后占用的存储空间大小不一致，那么就需要先把这条旧的记录从聚簇索引页面中删除掉，然后再根据更新后列的值创建一条新的记录插入到页面中。<br>
我们这里所说的删除并不是delete mark操作，而是真正的删除掉，也就是把这条记录从正常记录链表中移除并加入到垃圾链表中，并且修改页面中相应的统计信息（比如PAGE_FREE、PAGE_GARBAGE等这些信息）。不过这里做真正删除操作的线程并不是在介绍DELETE语句中做purge操作时使用的另外专门的线程，而是由用户线程同步执行真正的删除操作，真正删除之后紧接着就要根据各个列更新后的值创建的新记录插入。</p>
<p>这里如果新创建的记录占用的存储空间大小不超过旧记录占用的空间，那么可以直接重用被加入到垃圾链表中的旧记录所占用的存储空间，否则的话需要在页面中新申请一段空间以供新记录使用，如果本页面内已经没有可用的空间的话，那就需要进行页面分裂操作，然后再插入新记录。</p>
</li>
</ol>
<p>针对UPDATE不更新主键的情况（包括上面所说的就地更新和先删除旧记录再插入新记录），InnoDB设计了一种类型为<mark>TRX_UNDO_UPD_EXIST_REC</mark>的undo日志，它的完整结构如下：<br>
<img src="https://q456qq520.github.io/post-images/1677552943688.png" alt="" loading="lazy"></p>
<ul>
<li>n_updated属性表示本条UPDATE语句执行后将有几个列被更新，后边跟着的&lt;pos, old_len, old_value&gt;分别表示被更新列在记录中的位置、更新前该列占用的存储空间大小、更新前该列的真实值。</li>
<li>如果在UPDATE语句中更新的列包含索引列，那么也会添加索引列各列信息这个部分，否则的话是不会添加这个部分的。</li>
</ul>
<h5 id="2262-更新主键的情况">22.6.2 更新主键的情况</h5>
<p>在聚簇索引中，记录是按照主键值的大小连成了一个单向链表的，如果我们更新了某条记录的主键值，意味着这条记录在聚簇索引中的位置将会发生改变，比如你将记录的主键值从1更新为10000，如果还有非常多的记录的主键值分布在1 ~ 10000之间的话，那么这两条记录在聚簇索引中就有可能离得非常远，甚至中间隔了好多个页面。针对UPDATE语句中更新了记录主键值的这种情况，InnoDB在聚簇索引中分了两步处理：</p>
<ul>
<li>
<p>将旧记录进行delete mark操作<br>
这里是delete mark操作！也就是说在UPDATE语句所在的事务提交前，对旧记录只做一个delete mark操作，在事务提交后才由专门的线程做purge操作，把它加入到垃圾链表中。</p>
<blockquote>
<p>之所以只对旧记录做delete mark操作，是因为别的事务同时也可能访问这条记录，如果把它真正的删除加入到垃圾链表后，别的事务就访问不到了。这个功能就是所谓的MVCC</p>
</blockquote>
</li>
<li>
<p>根据更新后各列的值创建一条新记录，并将其插入到聚簇索引中（需重新定位插入的位置）。<br>
由于更新后的记录主键值发生了改变，所以需要重新从聚簇索引中定位这条记录所在的位置，然后把它插进去。</p>
</li>
</ul>
<p>针对UPDATE语句更新记录主键值的这种情况，在对该记录进行delete mark操作前，会记录一条类型为<mark>TRX_UNDO_DEL_MARK_REC</mark>的undo日志；之后插入新记录时，会记录一条类型为<mark>TRX_UNDO_INSERT_REC</mark>的undo日志，也就是说每对一条记录的主键值做改动时，会记录2条undo日志。</p>
<h2 id="第23章-undo日志下">第23章 undo日志（下）</h2>
<h3 id="231-通用链表结构">23.1 通用链表结构</h3>
<p>在写入undo日志的过程中会使用到多个链表，很多链表都有同样的节点结构，如图所示：<br>
<img src="https://q456qq520.github.io/post-images/1677554741854.png" alt="" loading="lazy"><br>
在某个表空间内，我们可以通过一个页的页号和在页内的偏移量来唯一定位一个节点的位置，这两个信息也就相当于指向这个节点的一个指针。所以：</p>
<ul>
<li>Pre Node Page Number和Pre Node Offset的组合就是指向前一个节点的指针</li>
<li>Next Node Page Number和Next Node Offset的组合就是指向后一个节点的指针。</li>
</ul>
<p>为了更好的管理链表，InnoDB还提出了一个基节点的结构，里边存储了这个链表的头节点、尾节点以及链表长度信息，基节点的结构示意图如下：<br>
<img src="https://q456qq520.github.io/post-images/1677554824666.png" alt="" loading="lazy"></p>
<ul>
<li>List Length表明该链表一共有多少节点。</li>
<li>First Node Page Number和First Node Offset的组合就是指向链表头节点的指针。</li>
<li>Last Node Page Number和Last Node Offset的组合就是指向链表尾节点的指针。</li>
</ul>
<h3 id="232-fil_page_undo_log页面">23.2 FIL_PAGE_UNDO_LOG页面</h3>
<p>一种称之为FIL_PAGE_UNDO_LOG类型的页面是专门用来存储undo日志的，简称为Undo页面，这种类型的页面的通用结构如下图所示（以默认的16KB大小为例）：</p>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1677555270281.png" alt="" loading="lazy"></figure>
<p>Undo Page Header是Undo页面所特有的，我们来看一下它的结构：<br>
<img src="https://q456qq520.github.io/post-images/1677555427248.png" alt="" loading="lazy"></p>
<ul>
<li>
<p>TRX_UNDO_PAGE_TYPE：本页面准备存储什么种类的undo日志。<br>
TRX_UNDO_INSERT（使用十进制1表示）：类型为TRX_UNDO_INSERT_REC的undo日志属于此大类，一般由INSERT语句产生，或者在UPDATE语句中有更新主键的情况也会产生此类型的undo日志。<br>
TRX_UNDO_UPDATE（使用十进制2表示），除了类型为TRX_UNDO_INSERT_REC的undo日志，其他类型的undo日志都属于这个大类，比如我们前面说的TRX_UNDO_DEL_MARK_REC、TRX_UNDO_UPD_EXIST_REC什么的，一般由DELETE、UPDATE语句产生的undo日志属于这个大类。</p>
<blockquote>
<p>之所以把undo日志分成两个大类，是因为类型为TRX_UNDO_INSERT_REC的undo日志在事务提交后可以直接删除掉，而其他类型的undo日志还需要为所谓的MVCC服务，不能直接删除掉</p>
</blockquote>
</li>
<li>
<p>TRX_UNDO_PAGE_START：表示在当前页面中是从什么位置开始存储undo日志的，或者说表示第一条undo日志在本页面中的起始偏移量。</p>
</li>
<li>
<p>TRX_UNDO_PAGE_FREE：与上面的TRX_UNDO_PAGE_START对应，表示当前页面中存储的最后一条undo日志结束时的偏移量，或者说从这个位置开始，可以继续写入新的undo日志。</p>
</li>
<li>
<p>TRX_UNDO_PAGE_NODE：代表一个List Node结构</p>
</li>
</ul>
<h3 id="233-undo页面链表">23.3 Undo页面链表</h3>
<h4 id="2331-单个事务中的undo页面链表">23.3.1 单个事务中的Undo页面链表</h4>
<p>因为一个事务可能包含多个语句，而且一个语句可能对若干条记录进行改动，而对每条记录进行改动前，都需要记录1条或2条的undo日志，所以在一个事务执行过程中可能产生很多undo日志，这些日志可能一个页面放不下，需要放到多个页面中，这些页面就通过我们上面介绍的TRX_UNDO_PAGE_NODE属性连成了链表：<br>
<img src="https://q456qq520.github.io/post-images/1677555758250.png" alt="" loading="lazy"></p>
<p>在一个事务执行过程中，可能混着执行INSERT、DELETE、UPDATE语句，也就意味着会产生不同类型的undo日志。但是同一个Undo页面要么只存储TRX_UNDO_INSERT大类的undo日志，要么只存储TRX_UNDO_UPDATE大类的undo日志，反正不能混着存，所以在一个事务执行过程中就可能需要2个Undo页面的链表，一个称之为insert undo链表，另一个称之为update undo链表。</p>
<p>另外，InnoDB规定对普通表和临时表的记录改动时产生的undo日志要分别记录，所以在一个事务中最多有4个以Undo页面为节点组成的链表</p>
<p>当然，并不是在事务一开始就会为这个事务分配这4个链表，具体分配策略如下：</p>
<ul>
<li>刚刚开启事务时，一个Undo页面链表也不分配。</li>
<li>当事务执行过程中向普通表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个普通表的insert undo链表。</li>
<li>当事务执行过程中删除或者更新了普通表中的记录之后，就会为其分配一个普通表的update undo链表。</li>
<li>当事务执行过程中向临时表中插入记录或者执行更新记录主键的操作之后，就会为其分配一个临时表的insert undo链表。</li>
<li>当事务执行过程中删除或者更新了临时表中的记录之后，就会为其分配一个临时表的update undo链表。</li>
</ul>
<h4 id="2332-多个事务中的undo页面链表">23.3.2 多个事务中的Undo页面链表</h4>
<p>为了尽可能提高undo日志的写入效率，不同事务执行过程中产生的undo日志需要被写入到不同的Undo页面链表中。</p>
<h3 id="234-undo日志具体写入过程">23.4 undo日志具体写入过程</h3>
<h4 id="2341-段segment的概念">23.4.1 段（Segment）的概念</h4>
<p>段是一个逻辑上的概念，本质上是由若干个零散页面和若干个完整的区组成的。比如一个B+树索引被划分成两个段，一个叶子节点段，一个非叶子节点段，这样叶子节点就可以被尽可能的存到一起，非叶子节点被尽可能的存到一起。每一个段对应一个INODE Entry结构，这个INODE Entry结构描述了这个段的各种信息，比如段的ID，段内的各种链表基节点，零散页面的页号有哪些等信息。为了定位一个INODE Entry，InnoDB设计了一个Segment Header的结构：<br>
<img src="https://q456qq520.github.io/post-images/1677556137591.png" alt="" loading="lazy"></p>
<ul>
<li>Space ID of the INODE Entry：INODE Entry结构所在的表空间ID。</li>
<li>Page Number of the INODE Entry：INODE Entry结构所在的页面页号。</li>
<li>Byte Offset of the INODE Ent：INODE Entry结构在该页面中的偏移量</li>
</ul>
<h4 id="2342-undo-log-segment-header">23.4.2 Undo Log Segment Header</h4>
<p>每一个Undo页面链表都对应着一个段，称之为<mark>Undo Log Segment</mark>。也就是说链表中的页面都是从这个段里边申请的，所以他们在Undo页面链表的第一个页面，也就是上面提到的first undo page中设计了一个称之为<mark>Undo Log Segment Header</mark>的部分，这个部分中包含了该链表对应的段的segment header信息以及其他的一些关于这个段的信息。<br>
<img src="https://q456qq520.github.io/post-images/1677556260660.png" alt="" loading="lazy"></p>
<ol>
<li>TRX_UNDO_STATE：本Undo页面链表处在什么状态。
<ul>
<li>TRX_UNDO_ACTIVE：活跃状态，也就是一个活跃的事务正在往这个段里边写入undo日志。</li>
<li>TRX_UNDO_CACHED：被缓存的状态。处在该状态的Undo页面链表等待着之后被其他事务重用。</li>
<li>TRX_UNDO_TO_FREE：对于insert undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。</li>
<li>TRX_UNDO_TO_PURGE：对于update undo链表来说，如果在它对应的事务提交之后，该链表不能被重用，那么就会处于这种状态。</li>
<li>TRX_UNDO_PREPARED：包含处于PREPARE阶段的事务产生的undo日志。</li>
</ul>
</li>
<li>TRX_UNDO_LAST_LOG：本Undo页面链表中最后一个Undo Log Header的位置。</li>
<li>TRX_UNDO_FSEG_HEADER：本Undo页面链表对应的段的Segment Header信息。</li>
<li>TRX_UNDO_PAGE_LIST：Undo页面链表的基节点。</li>
</ol>
<h4 id="2343-undo-log-header">23.4.3 Undo Log Header</h4>
<p>一个事务在向Undo页面中写入undo日志时的方式是十分简单暴力的，就是直接往写，写完一条紧接着写另一条，各条undo日志之间是亲密无间的。写完一个Undo页面后，再从段里申请一个新页面，然后把这个页面插入到Undo页面链表中，继续往这个新申请的页面中写。</p>
<p>InnoDB认为同一个事务向一个Undo页面链表中写入的undo日志算是一个组。在每写入一组undo日志时，都会在这组undo日志前先记录一下关于这个组的一些属性，InnoDB把存储这些属性的地方称之为Undo Log Header。所以Undo页面链表的第一个页面在真正写入undo日志前，其实都会被填充Undo Page Header、Undo Log Segment Header、Undo Log Header这3个部分，如图所示：<br>
<img src="https://q456qq520.github.io/post-images/1677556642689.png" alt="" loading="lazy"></p>
<p>这个Undo Log Header具体的结构如下：<br>
<img src="https://q456qq520.github.io/post-images/1677572594825.png" alt="" loading="lazy"></p>
<ul>
<li>TRX_UNDO_TRX_ID：生成本组undo日志的事务id。</li>
<li>TRX_UNDO_TRX_NO：事务提交后生成的一个需要序号，使用此序号来标记事务的提交顺序（先提交的此序号小，后提交的此序号大）。</li>
<li>TRX_UNDO_DEL_MARKS：标记本组undo日志中是否包含由于Delete mark操作产生的undo日志。</li>
<li>TRX_UNDO_LOG_START：表示本组undo日志中第一条undo日志的在页面中的偏移量。</li>
<li>TRX_UNDO_XID_EXISTS：本组undo日志是否包含XID信息。</li>
<li>TRX_UNDO_DICT_TRANS：标记本组undo日志是不是由DDL语句产生的。</li>
<li>TRX_UNDO_TABLE_ID：如果TRX_UNDO_DICT_TRANS为真，那么本属性表示DDL语句操作的表的table id。</li>
<li>TRX_UNDO_NEXT_LOG：下一组的undo日志在页面中开始的偏移量。</li>
<li>TRX_UNDO_PREV_LOG：上一组的undo日志在页面中开始的偏移量。</li>
<li>TRX_UNDO_HISTORY_NODE：一个12字节的List Node结构，代表一个称之为History链表的节点。</li>
</ul>
<h3 id="235-重用undo页面">23.5 重用Undo页面</h3>
<p>为了能提高并发执行的多个事务写入undo日志的性能，InnoDB决定为每个事务单独分配相应的Undo页面链表（最多可能单独分配4个链表）。但是这样也造成了一些问题，比如其实大部分事务执行过程中可能只修改了一条或几条记录，针对某个Undo页面链表只产生了非常少的undo日志，这些undo日志可能只占用一丢丢存储空间，每开启一个事务就新创建一个Undo页面链表（虽然这个链表中只有一个页面）来存储这么一丢丢undo日志岂不是太浪费了。InnoDB在事务提交后在某些情况下重用该事务的Undo页面链表。一个Undo页面链表是否可以被重用的条件很简单：</p>
<ul>
<li>该链表中只包含一个Undo页面。<br>
如果一个事务执行过程中产生了非常多的undo日志，那么它可能申请非常多的页面加入到Undo页面链表中。在该事物提交后，如果将整个链表中的页面都重用，那就意味着即使新的事务并没有向该Undo页面链表中写入很多undo日志，那该链表中也得维护非常多的页面，那些用不到的页面也不能被别的事务所使用，这样就造成了另一种浪费。InnoDB规定只有在Undo页面链表中只包含一个Undo页面时，该链表才可以被下一个事务所重用。</li>
<li>该Undo页面已经使用的空间小于整个页面空间的3/4。<br>
Undo页面链表按照存储的undo日志所属的大类可以被分为insert undo链表和update undo链表两种，这两种链表在被重用时的策略也是不同的，我们分别看一下：<br>
insert undo链表中只存储类型为TRX_UNDO_INSERT_REC的undo日志，这种类型的undo日志在事务提交之后就没用了，就可以被清除掉。所以在某个事务提交后，重用这个事务的insert undo链表（这个链表中只有一个页面）时，可以直接把之前事务写入的一组undo日志覆盖掉，从头开始写入新事务的一组undo日志。<br>
在一个事务提交后，它的update undo链表中的undo日志也不能立即删除掉（这些日志用于MVCC）。所以如果之后的事务想重用update undo链表时，就不能覆盖之前事务写入的undo日志。这样就相当于在同一个Undo页面中写入了多组的undo日志</li>
</ul>
<h3 id="236-回滚段">23.6 回滚段</h3>
<h4 id="2361-回滚段的概念">23.6.1 回滚段的概念</h4>
<p>我们现在知道一个事务在执行过程中最多可以分配4个Undo页面链表，在同一时刻不同事务拥有的Undo页面链表是不一样的，所以在同一时刻系统里其实可以有许许多多个Undo页面链表存在。为了更好的管理这些链表，InnoDB又设计了一个称之为<mark>Rollback Segment Header</mark>的页面，在这个页面中存放了各个Undo页面链表的frist undo page的页号，他们把这些页号称之为<mark>undo slot</mark>。</p>
<figure data-type="image" tabindex="4"><img src="https://q456qq520.github.io/post-images/1677575600323.png" alt="" loading="lazy"></figure>
<p>每一个Rollback Segment Header页面都对应着一个段，这个段就称为<mark>Rollback Segment</mark>，翻译过来就是<mark>回滚段</mark>。与我们之前介绍的各种段不同的是，这个Rollback Segment里其实只有一个页面。</p>
<ul>
<li>TRX_RSEG_MAX_SIZE：本Rollback Segment中管理的所有Undo页面链表中的Undo页面数量之和的最大值。换句话说，本Rollback Segment中所有Undo页面链表中的Undo页面数量之和不能超过TRX_RSEG_MAX_SIZE代表的值。</li>
<li>TRX_RSEG_HISTORY_SIZE：History链表占用的页面数量。</li>
<li>TRX_RSEG_HISTORY：History链表的基节点。</li>
<li>TRX_RSEG_FSEG_HEADER：本Rollback Segment对应的10字节大小的Segment Header结构，通过它可以找到本段对应的INODE Entry。</li>
<li>TRX_RSEG_UNDO_SLOTS：各个Undo页面链表的first undo page的页号集合，也就是undo slot集合。<br>
一个页号占用4个字节，对于16KB大小的页面来说，这个TRX_RSEG_UNDO_SLOTS部分共存储了1024个undo slot，所以共需1024 × 4 = 4096个字节。</li>
</ul>
<h4 id="2362-从回滚段中申请undo页面链表">23.6.2 从回滚段中申请Undo页面链表</h4>
<p>初始情况下，由于未向任何事务分配任何Undo页面链表，所以对于一个Rollback Segment Header页面来说，它的各个undo slot都被设置成了一个特殊的值：FIL_NULL（对应的十六进制就是0xFFFFFFFF），表示该undo slot不指向任何页面。</p>
<p>开始有事务需要分配Undo页面链表了，就从回滚段的第一个undo slot开始，看看该undo slot的值是不是FIL_NULL：</p>
<ul>
<li>如果是FIL_NULL，那么在表空间中新创建一个段（也就是Undo Log Segment），然后从段里申请一个页面作为Undo页面链表的first undo page，然后把该undo slot的值设置为刚刚申请的这个页面的地址，这样也就意味着这个undo slot被分配给了这个事务。</li>
<li>如果不是FIL_NULL，说明该undo slot已经指向了一个undo链表，也就是说这个undo slot已经被别的事务占用了，那就跳到下一个undo slot，判断该undo slot的值是不是FIL_NULL，重复上面的步骤。</li>
</ul>
<p>一个Rollback Segment Header页面中包含1024个undo slot，如果这1024个undo slot的值都不为FIL_NULL，这就意味着这1024个undo slot都已经名花有主（被分配给了某个事务），此时由于新事务无法再获得新的Undo页面链表，就会回滚这个事务并且给用户报错：</p>
<pre><code class="language-java">Too many active concurrent transactions
</code></pre>
<p>当一个事务提交时，它所占用的undo slot有两种命运：</p>
<ul>
<li>
<p>如果该undo slot指向的Undo页面链表符合被重用的条件（就是我们上面说的Undo页面链表只占用一个页面并且已使用空间小于整个页面的3/4）。<br>
该undo slot就处于被缓存的状态，InnoDB规定这时该Undo页面链表的TRX_UNDO_STATE属性（该属性在first undo page的Undo Log Segment Header部分）会被设置为TRX_UNDO_CACHED。<br>
被缓存的undo slot都会被加入到一个链表，根据对应的Undo页面链表的类型不同，也会被加入到不同的链表：<br>
1、如果对应的Undo页面链表是insert undo链表，则该undo slot会被加入insert undo cached链表。<br>
2、如果对应的Undo页面链表是update undo链表，则该undo slot会被加入update undo cached链表。<br>
一个回滚段就对应着上述两个cached链表，如果有新事务要分配undo slot时，先从对应的cached链表中找。如果没有被缓存的undo slot，才会到回滚段的Rollback Segment Header页面中再去找。</p>
</li>
<li>
<p>如果该undo slot指向的Undo页面链表不符合被重用的条件，那么针对该undo slot对应的Undo页面链表类型不同，也会有不同的处理：<br>
如果对应的Undo页面链表是insert undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_FREE，之后该Undo页面链表对应的段会被释放掉（也就意味着段中的页面可以被挪作他用），然后把该undo slot的值设置为FIL_NULL。<br>
如果对应的Undo页面链表是update undo链表，则该Undo页面链表的TRX_UNDO_STATE属性会被设置为TRX_UNDO_TO_PRUGE，则会将该undo slot的值设置为FIL_NULL，然后将本次事务写入的一组undo日志放到所谓的History链表中（需要注意的是，这里并不会将Undo页面链表对应的段给释放掉，因为这些undo日志还有用呢～）。</p>
</li>
</ul>
<h4 id="2363-多个回滚段">23.6.3 多个回滚段</h4>
<p>我们说一个事务执行过程中最多分配4个Undo页面链表，而一个回滚段里只有1024个undo slot，很显然undo slot的数量有点少啊。我们即使假设一个读写事务执行过程中只分配1个Undo页面链表，那1024个undo slot也只能支持1024个读写事务同时执行。</p>
<p>InnoDB一口气定义了128个回滚段，也就相当于有了<mark>128 × 1024 = 131072个undo slot</mark>。假设一个读写事务执行过程中只分配1个Undo页面链表，那么就可以同时支持131072个读写事务并发执行。</p>
<p>每个回滚段都对应着一个Rollback Segment Header页面，有128个回滚段，自然就要有128个Rollback Segment Header页面，于是InnoDB在系统表空间的第<mark>5</mark>号页面的某个区域包含了128个8字节大小的格子，每个8字节的格子的构造就像这样：<br>
<img src="https://q456qq520.github.io/post-images/1677577317818.png" alt="" loading="lazy"></p>
<ul>
<li>4字节大小的Space ID，代表一个表空间的ID。</li>
<li>4字节大小的Page number，代表一个页号。</li>
</ul>
<p>也就是说每个8字节大小的格子相当于一个指针，指向某个表空间中的某个页面，这些页面就是Rollback Segment Header。这里需要注意的一点事，要定位一个Rollback Segment Header还需要知道对应的表空间ID，<mark>这也就意味着不同的回滚段可能分布在不同的表空间中</mark>。</p>
<p>所以通过上面的叙述我们可以大致清楚，在系统表空间的第5号页面中存储了128个Rollback Segment Header页面地址，每个Rollback Segment Header就相当于一个回滚段。在Rollback Segment Header页面中，又包含1024个undo slot，每个undo slot都对应一个Undo页面链表。我们画个示意图：<br>
<img src="https://q456qq520.github.io/post-images/1677577529720.png" alt="" loading="lazy"></p>
<h4 id="2364-回滚段的分类">23.6.4 回滚段的分类</h4>
<p>我们把这128个回滚段给编一下号，最开始的回滚段称之为第0号回滚段，之后依次递增，最后一个回滚段就称之为第127号回滚段。这128个回滚段可以被分成两大类：</p>
<ul>
<li>
<p>第0号、第33～127号回滚段属于一类。其中第0号回滚段必须在系统表空间中（就是说第0号回滚段对应的Rollback Segment Header页面必须在系统表空间中），第33～127号回滚段既可以在系统表空间中，也可以在自己配置的undo表空间中。</p>
</li>
<li>
<p>第1～32号回滚段属于一类。这些回滚段必须在临时表空间（对应着数据目录中的ibtmp1文件）中。</p>
</li>
</ul>
<p>也就是说如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段，再分别到这两个回滚段中分配对应的undo slot。</p>
<p>为什么要把针对普通表和临时表来划分不同种类的回滚段呢？这个还得从Undo页面本身说起，我们说Undo页面其实是类型为FIL_PAGE_UNDO_LOG的页面的简称，说到底它也是一个普通的页面。我们前面说过，在修改页面之前一定要先把对应的redo日志写上，这样在系统奔溃重启时才能恢复到奔溃前的状态。</p>
<p>我们向Undo页面写入undo日志本身也是一个写页面的过程，InnoDB为此还设计了许多种redo日志的类型，比方说MLOG_UNDO_HDR_CREATE、MLOG_UNDO_INSERT、MLOG_UNDO_INIT等等，也就是说我们对Undo页面做的任何改动都会记录相应类型的redo日志。但是对于临时表来说，因为修改临时表而产生的undo日志只需要在系统运行过程中有效，如果系统奔溃了，那么在重启时也不需要恢复这些undo日志所在的页面，所以在写针对临时表的Undo页面时，并不需要记录相应的redo日志。</p>
<p><code>总结一下针对普通表和临时表划分不同种类的回滚段的原因：在修改针对普通表的回滚段中的Undo页面时，需要记录对应的redo日志，而修改针对临时表的回滚段中的Undo页面时，不需要记录对应的redo日志。</code></p>
<blockquote>
<p>小贴士：实际上在MySQL 5.7.21这个版本中，如果我们仅仅对普通表的记录做了改动，那么只会为该事务分配针对普通表的回滚段，不分配针对临时表的回滚段。但是如果我们仅仅对临时表的记录做了改动，那么既会为该事务分配针对普通表的回滚段，又会为其分配针对临时表的回滚段（不过分配了回滚段并不会立即分配undo slot，只有在真正需要Undo页面链表时才会去分配回滚段中的undo slot）。</p>
</blockquote>
<h3 id="237-为事务分配undo页面链表详细过程">23.7 为事务分配Undo页面链表详细过程</h3>
<ol>
<li>事务在执行过程中对普通表的记录首次做改动之前，首先会到系统表空间的第5号页面中分配一个回滚段（其实就是获取一个Rollback Segment Header页面的地址）。一旦某个回滚段被分配给了这个事务，那么之后该事务中再对普通表的记录做改动时，就不会重复分配了。</li>
<li>在分配到回滚段后，首先看一下这个回滚段的两个cached链表有没有已经缓存了的undo slot，比如如果事务做的是INSERT操作，就去回滚段对应的insert undo cached链表中看看有没有缓存的undo slot；如果事务做的是DELETE操作，就去回滚段对应的update undo cached链表中看看有没有缓存的undo slot。如果有缓存的undo slot，那么就把这个缓存的undo slot分配给该事务。</li>
<li>如果没有缓存的undo slot可供分配，那么就要到Rollback Segment Header页面中找一个可用的undo slot分配给当前事务。</li>
<li>找到可用的undo slot后，如果该undo slot是从cached链表中获取的，那么它对应的Undo Log Segment已经分配了，否则的话需要重新分配一个Undo Log Segment，然后从该Undo Log Segment中申请一个页面作为Undo页面链表的first undo page。</li>
<li>然后事务就可以把undo日志写入到上面申请的Undo页面链表了。</li>
</ol>
<p>对临时表的记录做改动的步骤和上述的一样，就不赘述了。不错需要再次强调一次，<mark>如果一个事务在执行过程中既对普通表的记录做了改动，又对临时表的记录做了改动，那么需要为这个记录分配2个回滚段。并发执行的不同事务其实也可以被分配相同的回滚段，只要分配不同的undo slot就可以了</mark>。</p>
<h3 id="238-回滚段相关配置">23.8 回滚段相关配置</h3>
<h4 id="2381-配置回滚段数量">23.8.1 配置回滚段数量</h4>
<p>系统中一共有128个回滚段，其实这只是默认值，我们可以通过启动参数<code>innodb_rollback_segments</code>来配置回滚段的数量，可配置的范围是1~128。但是这个参数并不会影响针对临时表的回滚段数量，针对临时表的回滚段数量一直是32，也就是说：</p>
<ul>
<li>如果我们把innodb_rollback_segments的值设置为1，那么只会有1个针对普通表的可用回滚段，但是仍然有32个针对临时表的可用回滚段。</li>
<li>如果我们把innodb_rollback_segments的值设置为2～33之间的数，效果和将其设置为1是一样的。</li>
<li>如果我们把innodb_rollback_segments设置为大于33的数，那么针对普通表的可用回滚段数量就是该值减去32。</li>
</ul>
<h4 id="2382-配置undo表空间">23.8.2 配置undo表空间</h4>
<p>默认情况下，针对普通表设立的回滚段（第0号以及第33<sub>127号回滚段）都是被分配到系统表空间的。其中的第0号回滚段是一直在系统表空间的，但是第33</sub>127号回滚段可以通过配置放到自定义的undo表空间中。但是这种配置只能在系统初始化（创建数据目录时）的时候使用，一旦初始化完成，之后就不能再次更改了。我们看一下相关启动参数：</p>
<ul>
<li>通过innodb_undo_directory指定undo表空间所在的目录，如果没有指定该参数，则默认undo表空间所在的目录就是数据目录。</li>
<li>通过innodb_undo_tablespaces定义undo表空间的数量。该参数的默认值为0，表明不创建任何undo表空间。</li>
</ul>
<h2 id="第24章-事务的隔离级别与mvcc">第24章 事务的隔离级别与MVCC</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[RocketMq技术内幕笔记（四）]]></title>
        <id>https://q456qq520.github.io/post/rocketmq-ji-zhu-nei-mu-bi-ji-si/</id>
        <link href="https://q456qq520.github.io/post/rocketmq-ji-zhu-nei-mu-bi-ji-si/">
        </link>
        <updated>2023-02-26T09:58:46.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="5-rocketmq-消息消费">5 RocketMQ 消息消费</h2>
<h3 id="51-rocketmq-消息消费概述">5.1 RocketMQ 消息消费概述</h3>
<p>消息消费以组的模式开展， 一个消费组内可以包含多个消费者，每一个消费组可订阅 多个主题，消费组之间有集群模式与广播模式两种消费模式 。</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="5-rocketmq-消息消费">5 RocketMQ 消息消费</h2>
<h3 id="51-rocketmq-消息消费概述">5.1 RocketMQ 消息消费概述</h3>
<p>消息消费以组的模式开展， 一个消费组内可以包含多个消费者，每一个消费组可订阅 多个主题，消费组之间有集群模式与广播模式两种消费模式 。</p>
<!-- more -->
<p>集群模式，主题下的同一条 消息只允许被其中一个消费者消费 。<br>
广播模式，主题下的同一条消息将被集群内的所有消 费者消费一次。</p>
<p>消息服务器与消费者之间的消息传送也有两种方式:推模式、拉模式。 所谓的拉模式，是消费端主动发起拉消息请求，而推模式是消息到达消息服务器后，推送给消息消费者。 RocketMQ 消息推模式的实现基于拉模式，在拉模式上包装一层，一个拉取任务完成后开始下一个拉取任务。</p>
<p>集群模式下，多个消费者如何对消息队列进行负载呢?消息队列负载机制遵循一个通用的思想 : <mark>一个消息队列同一时间只允许被一个消费者消费，一个消费者可以消费多个消息队列</mark> 。</p>
<p>RocketMQ 支持局部顺序消息消费，也就是保证同一个消息队列上的消息顺序消费。 不 支持消息全局顺序消费， 如果要实现某一主题的全局顺序消息消费， 可以将该主题的队列数设置为1，牺牲高可用性。</p>
<p>RocketMQ 支持两种消息过滤模式:表达式(TAG、 SQL92)与类过滤模式。</p>
<p>消息拉模式，主要是由客户端手动调用消息拉取API，而消息推模式是消息服务器主 动将消息推送到消息消费端</p>
<h3 id="52-消息消费者初探">5.2 消息消费者初探</h3>
<p>下面分析推模式的消费者 MQPushConsume的主要API， 如下图所示。<br>
<img src="https://q456qq520.github.io/post-images/1677406267122.png" alt="" loading="lazy"></p>
<blockquote>
<p>MQConsume</p>
</blockquote>
<pre><code class="language-java">/**
* 发送消息 ACK确认
* @param msg 消息
* @param delayLevel 消息延迟级别
* @param brokerName 消息服务器名称
*/
void sendMessageBack(final MessageExt msg, final int delayLevel, final String brokerName)
throws RemotingException, MQBrokerException, InterruptedException, MQClientException;

/**
* 获取消费者对主题 topic分配了哪些消息队列
* @param topic 主题名称
*/
Set&lt;MessageQueue&gt; fetchSubscribeMessageQueues(final String topic) throws MQClientException;
</code></pre>
<blockquote>
<p>MQPushConsumer</p>
</blockquote>
<pre><code class="language-java">/**
* 注册并发消息事件监昕器
* @param messageListener
*/
void registerMessageListener(final MessageListenerConcurrently messageListener);

/**
* 注册顺序消费事件监听器
* @param messageListener
*/
void registerMessageListener(final MessageListenerOrderly messageListener);

/**
* 基于主题订阅消息
* @param topic 消息主题
* @param subExpression 消息过滤表达式，TAG或SQL92表达式
*/
void subscribe(final String topic, final String subExpression) ;
/**

* 基于主题订阅消息，消息过滤方式使用类模式
* @param topic 消息主题
* @param fullClassName 过滤类全路径名
* @param filterClassSource 过滤类代码
*/
void subscribe(final String topic, final String fullClassName,final String filterClassSource);

/**
* 取消消息订阅 
* @param topic
*/
void unsubscribe(final String topic);
</code></pre>
<p>DefaultMQPushConsumer (推模式消息消费者)主要属性:</p>
<blockquote>
<p>DefaultMQPushConsumer</p>
</blockquote>
<pre><code class="language-java">//消费者所属组
private String consumerGroup;

//消息消费模式，分为集群模式、广播模式，默认为集群模式
private MessageModel messageModel = MessageModel.CLUSTERING;

//根据消息进度从消息服务器拉取不到消息时重新计算消费策略
//CONSUME_FROM_MIN_OFFSET,从队列当前最小偏移量开始消费
//CONSUME_FROM_MAX_OFFSET,从队列当前最大偏移量开始消费
//CONSUME_FROM_TIMESTAMP,从消费者启动时间戳开始消费
private ConsumeFromWhere consumeFromWhere = ConsumeFromWhere.CONSUME_FROM_LAST_OFFSET;
//集群模式下消息队列负载策略 
private AllocateMessageQueueStrategy allocateMessageQueueStrategy;
 //集群模式下消息队列负载策略
private AllocateMessageQueueStrategy allocateMessageQueueStrategy;

//订阅信息
private Map&lt;String /* topic */, String /* sub expression */&gt; subscription = new HashMap&lt;String, String&gt;();

/**
* 消息业务监听器
*/
private MessageListener messageListener;

/**
* 消息消费进度存储器
*/
private OffsetStore offsetStore;

/**
* 消息者最新线程数
*/
private int consumeThreadMin = 20;

/**
* 消费者最大线程数，由于消费者线程池使用无界队列，
* 故消费者线程个数其实最多只有 consumeThreadMin 个
*/
private int consumeThreadMax = 20;
/**
* 消费者最大线程数，由于消费者线程池使用无界队列，
* 故消费者线程个数其实最多只有 consumeThreadMin 个
*/
private int consumeThreadMax = 20;

/**
* Threshold for dynamic adjustment of the number of thread pool
*/
private long adjustThreadPoolNumsThreshold = 100000;

/**
* 并发消息消费时处理队列最大跨度，默认 2000,
* 表示如果消息处理队列中偏移量最大的消息与偏移量最小的消息的跨度超过 2000则延迟到毫秒后再拉取消息
*/
private int consumeConcurrentlyMaxSpan = 2000;
//默认值1000， 每1000次流控后打印流控日志
private int pullThresholdForQueue = 1000;

/**
* 推模式下拉取任务间隔时间，默认一次拉取任务完成继续拉取。
*/
private long pullInterval = 0;

/**
* 消息并发消费时一次消费消息条数，通俗点说 就是每次传入MessageListtener#consumeMessage中的消息条数
*/
private int consumeMessageBatchMaxSize = 1;

/**
* 每次消息拉取所拉取的条数，默认32条
*/
private int pullBatchSize = 32;
//是否每次拉取消息都更新订阅信息，默认为 false
private boolean postSubscriptionWhenPull = false;
//最大消费重试次数。如果消息消费次数超过 maxReconsumeTimes还未成功，则将该消息转移到一个失败队列,等待被删除
private int maxReconsumeTimes = -1;
/**
* 延迟将该队列的消息提交到消费者线程的等待时间，默认延迟ls
*/
private long suspendCurrentQueueTimeMillis = 1000;
/**
* 消息消费超时时间，默认为15，单位为分钟 
*/
private long consumeTimeout = 15;
</code></pre>
<h3 id="53-消费者启动流程">5.3 消费者启动流程</h3>
<p>消息消费者是如何启动的，分析 DefaultMQPushConsumerlmpl 的start方法，具体代码如下。</p>
<blockquote>
<p>DefaultMQPushConsumelmpl#copySubscription</p>
</blockquote>
<pre><code class="language-java">private void copySubscription() throws MQClientException {
    try {
        Map&lt;String, String&gt; sub = this.defaultMQPushConsumer.getSubscription();
        if (sub != null) {
            for (final Map.Entry&lt;String, String&gt; entry : sub.entrySet()) {
                final String topic = entry.getKey();
                final String subString = entry.getValue();
                SubscriptionData subscriptionData = FilterAPI.buildSubscriptionData(topic, subString);
                this.rebalanceImpl.getSubscriptionInner().put(topic, subscriptionData);
            }
        }

        if (null == this.messageListenerInner) {
            this.messageListenerInner = this.defaultMQPushConsumer.getMessageListener();
        }

        switch (this.defaultMQPushConsumer.getMessageModel()) {
            case BROADCASTING:
                break;
            case CLUSTERING:
                final String retryTopic = MixAll.getRetryTopic(this.defaultMQPushConsumer.getConsumerGroup());
                SubscriptionData subscriptionData = FilterAPI.buildSubscriptionData(retryTopic, SubscriptionData.SUB_ALL);
                this.rebalanceImpl.getSubscriptionInner().put(retryTopic, subscriptionData);
                break;
            default:
                break;
        }
    } catch (Exception e) {
        throw new MQClientException(&quot;subscription exception&quot;, e);
    }
}
</code></pre>
<p>Step1 :构建主题订阅信息 SubscriptionData 并加入到 Rebalancelmpl 的订阅消息中。 订阅关系来源主要有两个。<br>
1)通过调用 DefaultMQPushConsumerlmpl#subscrib巳( String topic, String subExpression) 方法。<br>
2)订阅重试主题消息。从这里可以看出，RocketMQ消息重试是以消费组为单位，而不是主题，消息重试主题名为 %RETRY%+消费组名。消费者在启动的时候会自动订阅该主题，参与该主题的消息队列负载。</p>
<blockquote>
<p>DefaultMQPushConsumelmpl#start</p>
</blockquote>
<pre><code class="language-java">if (this.defaultMQPushConsumer.getMessageModel() == MessageModel.CLUSTERING) {
    this.defaultMQPushConsumer.changeInstanceNameToPID();
}

this.mQClientFactory = MQClientManager.getInstance().getOrCreateMQClientInstance(this.defaultMQPushConsumer, this.rpcHook);

this.rebalanceImpl.setConsumerGroup(this.defaultMQPushConsumer.getConsumerGroup());
this.rebalanceImpl.setMessageModel(this.defaultMQPushConsumer.getMessageModel());
this.rebalanceImpl.setAllocateMessageQueueStrategy(this.defaultMQPushConsumer.getAllocateMessageQueueStrategy());
this.rebalanceImpl.setmQClientFactory(this.mQClientFactory);

if (this.pullAPIWrapper == null) {
    this.pullAPIWrapper = new PullAPIWrapper(
        mQClientFactory,
        this.defaultMQPushConsumer.getConsumerGroup(), isUnitMode());
}
this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookList);
</code></pre>
<p>Step2:初始化 MQClientlnstance、 Rebalancelmple (消息重新负载实现类)等。</p>
<blockquote>
<p>DefaultMQPushConsumerlmpl#start</p>
</blockquote>
<pre><code class="language-java">if (this.defaultMQPushConsumer.getOffsetStore() != null) {
    this.offsetStore = this.defaultMQPushConsumer.getOffsetStore();
} else {
    switch (this.defaultMQPushConsumer.getMessageModel()) {
        case BROADCASTING:
            this.offsetStore = new LocalFileOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
            break;
        case CLUSTERING:
            this.offsetStore = new RemoteBrokerOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
            break;
        default:
            break;
    }
    this.defaultMQPushConsumer.setOffsetStore(this.offsetStore);
}
this.offsetStore.load();
</code></pre>
<p>Step3 : 初始化消息进度。如果消息消费是集群模式，那么消息进度保存在 Broker上; 如果是广播模式，那么消息消费进度存储在消费端。</p>
<blockquote>
<p>DefaultMQPushConsumerlmpl#start</p>
</blockquote>
<pre><code class="language-java">if (this.getMessageListenerInner() instanceof MessageListenerOrderly) {
this.consumeOrderly = true;
this.consumeMessageService =
    new ConsumeMessageOrderlyService(this, (MessageListenerOrderly) this.getMessageListenerInner());
//POPTODO reuse Executor ?
this.consumeMessagePopService = new ConsumeMessagePopOrderlyService(this, (MessageListenerOrderly) this.getMessageListenerInner());
} else if (this.getMessageListenerInner() instanceof MessageListenerConcurrently) {
this.consumeOrderly = false;
this.consumeMessageService =
    new ConsumeMessageConcurrentlyService(this, (MessageListenerConcurrently) this.getMessageListenerInner());
//POPTODO reuse Executor ?
this.consumeMessagePopService =
    new ConsumeMessagePopConcurrentlyService(this, (MessageListenerConcurrently) this.getMessageListenerInner());
}

this.consumeMessageService.start();
</code></pre>
<p>Step4 :根据是否是顺序消费，创建消费端消费线程服务。 ConsumeMessageService 主要负责消息消费，内部维护一个线程池。</p>
<blockquote>
<p>DefaultMQPushConsumerlmpl#start</p>
</blockquote>
<pre><code class="language-java">boolean registerOK = mQClientFactory.registerConsumer(this.defaultMQPushConsumer.getConsumerGroup(), this);
if (!registerOK) {
    this.serviceState = ServiceState.CREATE_JUST;
    this.consumeMessageService.shutdown(defaultMQPushConsumer.getAwaitTerminationMillisWhenShutdown());
    throw new MQClientException(&quot;The consumer group[&quot; + this.defaultMQPushConsumer.getConsumerGroup()
        + &quot;] has been created before, specify another name please.&quot; + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL),
        null);
}

mQClientFactory.start();
</code></pre>
<p>Step5 :向 MQClientlnstance注册消费者，并启动MQClientlnstance，在一个JVM中的所有消费者、生产者持有同一个 MQClientlnstance, MQClientlnstance 只会启动一次。</p>
<h3 id="54-消息拉取">5.4 消息拉取</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[《从根儿上理解MySQL》读书笔记(四)]]></title>
        <id>https://q456qq520.github.io/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-si/</id>
        <link href="https://q456qq520.github.io/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-si/">
        </link>
        <updated>2023-02-20T10:05:28.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="第15章-查询优化的百科全书-explain详解上">第15章 查询优化的百科全书-Explain详解（上）</h2>
]]></summary>
        <content type="html"><![CDATA[<h2 id="第15章-查询优化的百科全书-explain详解上">第15章 查询优化的百科全书-Explain详解（上）</h2>
<!-- more -->
<p>一条查询语句在经过MySQL查询优化器的各种基于成本和规则的优化会后生成一个所谓的<code>执行计划</code>，这个执行计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访问方法来具体执行查询等等。</p>
<p>mysql为我们提供了<code>EXPLAIN</code>语句来帮助我们查看某个查询语句的具体执行计划</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT 1;
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra          |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+
|  1 | SIMPLE      | NULL  | NULL       | NULL | NULL          | NULL | NULL    | NULL | NULL |     NULL | No tables used |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+
1 row in set, 1 warning (0.01 sec)
</code></pre>
<p>EXPLAIN语句输出的各个列的作用先大致如下：</p>
<table>
<thead>
<tr>
<th>列名</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>id</td>
<td>在一个大的查询语句中每个SELECT关键字都对应一个唯一的id</td>
</tr>
<tr>
<td>select_type</td>
<td>SELECT关键字对应的那个查询的类型</td>
</tr>
<tr>
<td>table</td>
<td>表名</td>
</tr>
<tr>
<td>partitions</td>
<td>匹配的分区信息</td>
</tr>
<tr>
<td>type</td>
<td>针对单表的访问方法</td>
</tr>
<tr>
<td>possible_keys</td>
<td>可能用到的索引</td>
</tr>
<tr>
<td>key</td>
<td>实际上使用的索引</td>
</tr>
<tr>
<td>key_len</td>
<td>实际使用到的索引长度</td>
</tr>
<tr>
<td>ref</td>
<td>当使用索引列等值查询时，与索引列进行等值匹配的对象信息</td>
</tr>
<tr>
<td>rows</td>
<td>预估的需要读取的记录条数</td>
</tr>
<tr>
<td>filtered</td>
<td>某个表经过搜索条件过滤后剩余记录条数的百分比</td>
</tr>
<tr>
<td>Extra</td>
<td>一些额外的信息</td>
</tr>
</tbody>
</table>
<p>下面我们来构建两个和single_table表构造一模一样的s1、s2表，而且这两个表里边儿有10000条记录，除id列外其余的列都插入随机值。表：</p>
<pre><code class="language-mysql">CREATE TABLE single_table (
    id INT NOT NULL AUTO_INCREMENT,
    key1 VARCHAR(100),
    key2 INT,
    key3 VARCHAR(100),
    key_part1 VARCHAR(100),
    key_part2 VARCHAR(100),
    key_part3 VARCHAR(100),
    common_field VARCHAR(100),
    PRIMARY KEY (id),
    KEY idx_key1 (key1),
    UNIQUE KEY idx_key2 (key2),
    KEY idx_key3 (key3),
    KEY idx_key_part(key_part1, key_part2, key_part3)
) Engine=InnoDB CHARSET=utf8;
</code></pre>
<h3 id="151-执行计划输出中各列详解">15.1 执行计划输出中各列详解</h3>
<h4 id="1511-table">15.1.1 table</h4>
<p>MySQL规定<code>EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名</code>。所以我们看一条比较简单的查询语句：</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1;
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
|  1 | SIMPLE      | s1    | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 9688 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
1 row in set, 1 warning (0.00 sec)
</code></pre>
<p>这个查询语句只涉及对s1表的单表查询，所以EXPLAIN输出中只有一条记录，其中的table列的值是s1，表明这条记录是用来说明对s1表的单表访问方法的。</p>
<p>下面我们看一下一个连接查询的执行计划：</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2;
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra                                 |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+
|  1 | SIMPLE      | s1    | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 9688 |   100.00 | NULL                                  |
|  1 | SIMPLE      | s2    | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 9954 |   100.00 | Using join buffer (Block Nested Loop) |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+
2 rows in set, 1 warning (0.01 sec)
</code></pre>
<h4 id="1512-id">15.1.2 id</h4>
<p>我们知道我们写的查询语句一般都以SELECT关键字开头，比较简单的查询语句里只有一个SELECT关键字，比如下面这个查询语句：</p>
<pre><code class="language-mysql">SELECT * FROM s1 WHERE key1 = 'a';
</code></pre>
<p>稍微复杂一点的连接查询中也只有一个SELECT关键字，比如：</p>
<pre><code class="language-mysql">SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = 'a';
</code></pre>
<p>但是下面两种情况下在一条查询语句中会出现多个SELECT关键字：</p>
<pre><code class="language-mysql">SELECT * FROM s1  WHERE key1 IN (SELECT * FROM s2);

SELECT * FROM s1  UNION SELECT * FROM s2;
</code></pre>
<p>查询语句中每出现一个SELECT关键字，MySQL就会为它分配一个唯一的id值。这个id值就是EXPLAIN语句的第一个列。</p>
<p>对于连接查询来说，一个SELECT关键字后边的FROM子句中可以跟随多个表，所以在连接查询的执行计划中，<code>每个表都会对应一条记录，但是这些记录的id值都是相同的</code>，比如：</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2;
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra                                 |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+
|  1 | SIMPLE      | s1    | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 9688 |   100.00 | NULL                                  |
|  1 | SIMPLE      | s2    | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 9954 |   100.00 | Using join buffer (Block Nested Loop) |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+
2 rows in set, 1 warning (0.01 sec)
</code></pre>
<p>上述连接查询中参与连接的s1和s2表分别对应一条记录，但是这两条记录对应的id值都是1。<code>在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前面的表表示驱动表，出现在后边的表表示被驱动表</code>。所以从上面的EXPLAIN输出中我们可以看出，查询优化器准备让s1表作为驱动表，让s2表作为被驱动表来执行查询。</p>
<p>对于包含子查询的查询语句来说，就可能涉及多个SELECT关键字，所以在包含子查询的查询语句的执行计划中，每个SELECT关键字都会对应一个唯一的id值，比如这样：</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a';
+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+
| id | select_type | table | partitions | type  | possible_keys | key      | key_len | ref  | rows | filtered | Extra       |
+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+
|  1 | PRIMARY     | s1    | NULL       | ALL   | idx_key3      | NULL     | NULL    | NULL | 9688 |   100.00 | Using where |
|  2 | SUBQUERY    | s2    | NULL       | index | idx_key1      | idx_key1 | 303     | NULL | 9954 |   100.00 | Using index |
+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+
2 rows in set, 1 warning (0.02 sec)
</code></pre>
<p>从输出结果中我们可以看到，s1表在外层查询中，外层查询有一个独立的SELECT关键字，所以第一条记录的id值就是1，s2表在子查询中，子查询有一个独立的SELECT关键字，所以第二条记录的id值就是2。特别注意的是<code>查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询</code>。所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了，比如说：</p>
<pre><code class="language-java">mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key3 FROM s2 WHERE common_field = 'a');
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+
| id | select_type | table | partitions | type | possible_keys | key      | key_len | ref               | rows | filtered | Extra                        |
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+
|  1 | SIMPLE      | s2    | NULL       | ALL  | idx_key3      | NULL     | NULL    | NULL              | 9954 |    10.00 | Using where; Start temporary |
|  1 | SIMPLE      | s1    | NULL       | ref  | idx_key1      | idx_key1 | 303     | xiaohaizi.s2.key3 |    1 |   100.00 | End temporary                |
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+------------------------------+
2 rows in set, 1 warning (0.00 sec)
</code></pre>
<p>虽然我们的查询语句是一个子查询，但是执行计划中s1和s2表对应的记录的id值全部是1，这就表明了查询优化器将子查询转换为了连接查询。</p>
<p>对于包含UNION子句的查询语句来说，每个SELECT关键字对应一个id值也是没错的，不过还是有点儿特别的东西，比方说下面这个查询：</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1  UNION SELECT * FROM s2;
+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+
| id | select_type  | table      | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra           |
+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+
|  1 | PRIMARY      | s1         | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 9688 |   100.00 | NULL            |
|  2 | UNION        | s2         | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 9954 |   100.00 | NULL            |
| NULL | UNION RESULT | &lt;union1,2&gt; | NULL       | ALL  | NULL          | NULL | NULL    | NULL | NULL |     NULL | Using temporary |
+----+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+
3 rows in set, 1 warning (0.00 sec)
</code></pre>
<p>UNION子句会把多个查询的结果集合并起来并对结果集中的记录进行去重，怎么去重呢？MySQL使用的是内部的临时表。正如上面的查询计划中所示，UNION子句是为了把id为1的查询和id为2的查询的结果集合并起来并去重，所以在内部创建了一个名为<code>&lt;union1, 2&gt;</code>的临时表（就是执行计划第三条记录的table列的名称），id为NULL表明这个临时表是为了合并两个查询的结果集而创建的。</p>
<p>跟UNION对比起来，UNION ALL就不需要为最终的结果集进行去重，它只是单纯的把多个查询的结果集中的记录合并成一个并返回给用户，所以也就不需要使用临时表。</p>
<h4 id="1513-select_type">15.1.3 select_type</h4>
<p>MySQL的为每一个SELECT关键字代表的小查询都定义了一个称之为select_type的属性，意思是我们只要知道了某个小查询的select_type属性，就知道了这个小查询在整个大查询中扮演了一个什么角色。</p>
<p><code>SIMPLE</code><br>
查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型，连接查询也算是SIMPLE类型。</p>
<p><code>PRIMARY</code><br>
对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY。</p>
<p><code>UNION</code><br>
对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION。</p>
<p><code>UNION RESULT</code><br>
MySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT，例子上面有，就不赘述了。</p>
<p><code>SUBQUERY</code><br>
如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY。</p>
<blockquote>
<p>由于select_type为SUBQUERY的子查询由于会被物化，所以只需要执行一遍。</p>
</blockquote>
<p><code>DEPENDENT SUBQUERY</code><br>
如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY。</p>
<blockquote>
<p>select_type为DEPENDENT SUBQUERY的查询可能会被执行多次。</p>
</blockquote>
<p><code>DEPENDENT UNION</code><br>
在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION。</p>
<p><code>DERIVED</code><br>
对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED。</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM (SELECT key1, count(*) as c FROM s1 GROUP BY key1) AS derived_s1 where c &gt; 1;
+----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+
| id | select_type | table      | partitions | type  | possible_keys | key      | key_len | ref  | rows | filtered | Extra       |
+----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+
|  1 | PRIMARY     | &lt;derived2&gt; | NULL       | ALL   | NULL          | NULL     | NULL    | NULL | 9688 |    33.33 | Using where |
|  2 | DERIVED     | s1         | NULL       | index | idx_key1      | idx_key1 | 303     | NULL | 9688 |   100.00 | Using index |
+----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+
2 rows in set, 1 warning (0.00 sec)
</code></pre>
<p><code>MATERIALIZED</code><br>
当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type属性就是MATERIALIZED。</p>
<p><code>UNCACHEABLE SUBQUERY</code><br>
不常用，就不多介绍了。</p>
<p><code>UNCACHEABLE UNION</code><br>
不常用，就不多介绍了。</p>
<h4 id="1514-partitions">15.1.4 partitions</h4>
<p>一般情况下我们的查询语句的执行计划的partitions列的值都是NULL。</p>
<h4 id="1515-type">15.1.5 type</h4>
<p>执行计划的一条记录就代表着MySQL对某个表的执行查询时的访问方法，其中的type列就表明了这个访问方法是什么。</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a';
+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key      | key_len | ref   | rows | filtered | Extra |
+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | s1    | NULL       | ref  | idx_key1      | idx_key1 | 303     | const |    8 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+
1 row in set, 1 warning (0.04 sec)
</code></pre>
<p>可以看到type列的值是ref，表明MySQL即将使用ref访问方法来执行对s1表的查询。使用InnoDB存储引擎的表进行单表访问的一些访问方法如下：system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL。</p>
<p><code>system</code><br>
当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如MyISAM、Memory，那么对该表的访问方法就是system。</p>
<p><code>const</code><br>
当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const。</p>
<p><code>eq_ref</code><br>
在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref。</p>
<pre><code class="language-mysql">EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id;
</code></pre>
<p><code>ref</code><br>
当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref。</p>
<p><code>fulltext</code><br>
全文索引</p>
<p><code>ref_or_null</code><br>
当对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL值时，那么对该表的访问方法就可能是ref_or_null。</p>
<p><code>index_merge</code><br>
一般情况下对于某个表的查询只能使用到一个索引，但在某些场景下可以使用Intersection、Union、Sort-Union这三种索引合并的方式来执行查询，那么对该表的访问方法就可能是index_merge。</p>
<p><code>unique_subquery</code><br>
类似于两表连接中被驱动表的eq_ref访问方法，unique_subquery是针对在一些包含IN子查询的查询语句中，如果查询优化器决定将IN子查询转换为EXISTS子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的type列的值就是unique_subquery。</p>
<p><code>index_subquery</code><br>
index_subquery与unique_subquery类似，只不过访问子查询中的表时使用的是普通的索引。</p>
<p><code>range</code><br>
如果使用索引获取某些范围区间的记录，那么就可能使用到range访问方法。</p>
<p><code>index</code><br>
当我们可以使用索引覆盖，但需要扫描全部的索引记录时，该表的访问方法就是index。</p>
<p><code>ALL</code><br>
全表扫描</p>
<h4 id="1516-possible_keys和key">15.1.6 possible_keys和key</h4>
<p>在EXPLAIN语句输出的执行计划中，possible_keys列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些，key列表示实际用到的索引有哪些。</p>
<p>不过有一点比较特别，就是在使用<code>index</code>访问方法来查询某个表时，possible_keys列是空的，而key列展示的是实际使用到的索引。</p>
<p>另外需要注意的一点是，p<code>ossible_keys列中的值并不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，所以如果可以的话，尽量删除那些用不到的索引</code>。</p>
<h4 id="1517-key_len">15.1.7 key_len</h4>
<p>key_len列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的：</p>
<ol>
<li>对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是VARCHAR(100)，使用的字符集是utf8，那么该列实际占用的最大存储空间就是100 × 3 = 300个字节。</li>
<li>如果该索引列可以存储NULL值，则key_len比不可以存储NULL值时多1个字节。</li>
<li>对于变长字段来说，都会有2个字节的空间来存储该变长列的实际长度。</li>
</ol>
<h4 id="1518-ref">15.1.8 ref</h4>
<p>当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery其中之一时，ref列展示的就是与索引列作等值匹配的东东是什么，比如只是一个常数或者是某个列。</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 = 'a';
+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key      | key_len | ref   | rows | filtered | Extra |
+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | s1    | NULL       | ref  | idx_key1      | idx_key1 | 303     | const |    8 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+----------+---------+-------+------+----------+-------+
1 row in set, 1 warning (0.01 sec)
</code></pre>
<h4 id="1519-rows">15.1.9 rows</h4>
<p>如果查询优化器决定使用全表扫描的方式对某个表执行查询时，执行计划的rows列就代表预计需要扫描的行数，如果使用索引来执行查询时，执行计划的rows列就代表预计扫描的索引记录行数。</p>
<h4 id="15110-filtered">15.1.10 filtered</h4>
<p>分析连接查询的成本时提出过一个<code>condition filtering</code>的概念，就是MySQL在计算驱动表扇出时采用的一个策略：</p>
<ul>
<li>如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。</li>
<li>如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。</li>
</ul>
<pre><code>mysql&gt; EXPLAIN SELECT * FROM s1 WHERE key1 &gt; 'z' AND common_field = 'a';
+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+
| id | select_type | table | partitions | type  | possible_keys | key      | key_len | ref  | rows | filtered | Extra                              |
+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+
|  1 | SIMPLE      | s1    | NULL       | range | idx_key1      | idx_key1 | 303     | NULL |  266 |    10.00 | Using index condition; Using where |
+----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+------------------------------------+
1 row in set, 1 warning (0.00 sec)
</code></pre>
<p>从执行计划的key列中可以看出来，该查询使用idx_key1索引来执行查询，从rows列可以看出满足key1 &gt; 'z'的记录有266条。执行计划的filtered列就代表查询优化器预测在这266条记录中，有多少条记录满足其余的搜索条件，也就是common_field = 'a'这个条件的百分比。此处filtered列的值是10.00，说明查询优化器预测在266条记录中有10.00%的记录满足common_field = 'a'这个条件。</p>
<p>对于单表查询来说，这个filtered列的值没什么意义，我们更关注在连接查询中驱动表对应的执行计划记录的filtered值，比方说下面这个查询：</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = 'a';
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+
| id | select_type | table | partitions | type | possible_keys | key      | key_len | ref               | rows | filtered | Extra       |
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+
|  1 | SIMPLE      | s1    | NULL       | ALL  | idx_key1      | NULL     | NULL    | NULL              | 9688 |    10.00 | Using where |
|  1 | SIMPLE      | s2    | NULL       | ref  | idx_key1      | idx_key1 | 303     | xiaohaizi.s1.key1 |    1 |   100.00 | NULL        |
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+
2 rows in set, 1 warning (0.00 sec)
</code></pre>
<p>从执行计划中可以看出来，查询优化器打算把s1当作驱动表，s2当作被驱动表。我们可以看到驱动表s1表的执行计划的rows列为9688， filtered列为10.00，这意味着驱动表s1的扇出值就是9688 × 10.00% = 968.8，这说明还要对被驱动表执行大约968次查询。</p>
<h2 id="第16章-查询优化的百科全书-explain详解下">第16章 查询优化的百科全书-Explain详解（下）</h2>
<h3 id="161-extra">16.1 Extra</h3>
<p>Extra列是用来说明一些额外信息的，我们可以通过这些额外信息来更准确的理解MySQL到底将如何执行给定的查询语句。</p>
<p><code>No tables used</code><br>
当查询语句的没有FROM子句时将会提示该额外信息。</p>
<p><code>Impossible WHERE</code><br>
查询语句的WHERE子句永远为FALSE时将会提示该额外信息。</p>
<blockquote>
<p>EXPLAIN SELECT * FROM s1 WHERE 1 != 1;</p>
</blockquote>
<p><code>No matching min/max row</code><br>
当查询列表处有MIN或者MAX聚集函数，但是并没有符合WHERE子句中的搜索条件的记录时，将会提示该额外信息。</p>
<p><code>Using index</code><br>
当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息。</p>
<p><code>Using index condition</code><br>
有些搜索条件中虽然出现了索引列，但却不能使用到索引。</p>
<p><code>Using where</code><br>
当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息。</p>
<p>当使用索引访问来执行对某个表的查询，并且该语句的WHERE子句中有除了该索引包含的列之外的其他搜索条件时，在Extra列中也会提示上述额外信息。</p>
<p><code>Using join buffer (Block Nested Loop)</code><br>
在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的<code>基于块的嵌套循环算法</code>。</p>
<p><code>Not exists</code><br>
当我们使用左（外）连接时，如果WHERE子句中包含要求被驱动表的某个列等于NULL值的搜索条件，而且那个列又是不允许存储NULL值的，那么在该表的执行计划的Extra列就会提示Not exists额外信息。</p>
<p><code>Using intersect(...)、Using union(...)和Using sort_union(...)</code><br>
如果执行计划的Extra列出现了Using intersect(...)提示，说明准备使用Intersect索引合并的方式执行查询，括号中的...表示需要进行索引合并的索引名称；如果出现了Using union(...)提示，说明准备使用Union索引合并的方式执行查询；出现了Using sort_union(...)提示，说明准备使用Sort-Union索引合并的方式执行查询。</p>
<p><code>Zero limit</code><br>
当我们的LIMIT子句的参数为0时，表示压根儿不打算从表中读出任何记录，将会提示该额外信息。</p>
<p><code>Using filesort</code><br>
有一些情况下对结果集中的记录进行排序是可以使用到索引的。但是很多情况下排序操作无法使用到索引，只能在内存中（记录较少的时候）或者磁盘中（记录较多的时候）进行排序，MySQL把这种在内存中或者磁盘上进行排序的方式统称为<code>文件排序（英文名：filesort）</code>。如果某个查询需要使用文件排序的方式执行查询，就会在执行计划的Extra列中显示Using filesort提示。</p>
<p>如果查询中需要使用filesort的方式进行排序的记录非常多，那么这个过程是很耗费性能的，我们最好想办法将使用文件排序的执行方式改为使用索引进行排序。</p>
<p><code>Using temporary</code><br>
在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含DISTINCT、GROUP BY、UNION等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行计划的Extra列将会显示Using temporary提示。</p>
<p>执行计划中出现Using temporary并不是一个好的征兆，因为建立与维护临时表要付出很大成本的，所以我们最好能使用索引来替代掉使用临时表。</p>
<p><code>Start temporary, End temporary</code><br>
查询优化器会优先尝试将IN子查询转换成semi-join，而semi-join又有好多种执行策略，当执行策略为DuplicateWeedout时，也就是通过建立临时表来实现为外层查询中的记录进行去重操作时，驱动表查询执行计划的Extra列将显示Start temporary提示，被驱动表查询执行计划的Extra列将显示End temporary提示。</p>
<p><code>LooseScan</code><br>
在将In子查询转为semi-join时，如果采用的是LooseScan执行策略，则在驱动表执行计划的Extra列就是显示LooseScan提示。</p>
<h3 id="162-json格式的执行计划">16.2 Json格式的执行计划</h3>
<p>EXPLAIN语句输出中缺少了一个衡量执行计划好坏的重要属性 —— 成本。不过MySQL提供了一种查看某个执行计划花费的成本的方式：</p>
<p>在EXPLAIN单词和真正的查询语句中间加上<mark>FORMAT=JSON</mark>。</p>
<h3 id="163-extented-explain">16.3 Extented EXPLAIN</h3>
<p>在我们使用EXPLAIN语句查看了某个查询的执行计划后，紧接着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息。</p>
<pre><code class="language-mysql">mysql&gt; EXPLAIN SELECT s1.key1, s2.key1 FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.common_field IS NOT NULL;
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+
| id | select_type | table | partitions | type | possible_keys | key      | key_len | ref               | rows | filtered | Extra       |
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+
|  1 | SIMPLE      | s2    | NULL       | ALL  | idx_key1      | NULL     | NULL    | NULL              | 9954 |    90.00 | Using where |
|  1 | SIMPLE      | s1    | NULL       | ref  | idx_key1      | idx_key1 | 303     | xiaohaizi.s2.key1 |    1 |   100.00 | Using index |
+----+-------------+-------+------------+------+---------------+----------+---------+-------------------+------+----------+-------------+
2 rows in set, 1 warning (0.00 sec)

mysql&gt; SHOW WARNINGS\G
*************************** 1. row ***************************
  Level: Note
   Code: 1003
Message: /* select#1 */ select `xiaohaizi`.`s1`.`key1` AS `key1`,`xiaohaizi`.`s2`.`key1` AS `key1` from `xiaohaizi`.`s1` join `xiaohaizi`.`s2` where ((`xiaohaizi`.`s1`.`key1` = `xiaohaizi`.`s2`.`key1`) and (`xiaohaizi`.`s2`.`common_field` is not null))
1 row in set (0.00 sec)
</code></pre>
<p>可以看到SHOW WARNINGS展示出来的信息有三个字段，分别是Level、Code、Message。我们最常见的就是Code为1003的信息，当Code值为1003时，Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。比如我们上面的查询本来是一个左（外）连接查询，但是有一个s2.common_field IS NOT NULL的条件，着就会导致查询优化器把左（外）连接查询优化为内连接查询，从SHOW WARNINGS的Message字段也可以看出来，原本的LEFT JOIN已经变成了JOIN。</p>
<h2 id="第17章-神兵利器-optimizer-trace表的神器功效">第17章 神兵利器-optimizer trace表的神器功效</h2>
<p>在MySQL 5.6以及之后的版本中，MySQL提出了一个optimizer trace的功能，这个功能可以让我们方便的查看优化器生成执行计划的整个过程，这个功能的开启与关闭由系统变量optimizer_trace决定，我们看一下：</p>
<pre><code class="language-mysql">mysql&gt; SHOW VARIABLES LIKE 'optimizer_trace';
+-----------------+--------------------------+
| Variable_name   | Value                    |
+-----------------+--------------------------+
| optimizer_trace | enabled=off,one_line=off |
+-----------------+--------------------------+
1 row in set (0.02 sec)
</code></pre>
<p>可以看到enabled值为off，表明这个功能默认是关闭的。</p>
<blockquote>
<p>one_line的值是控制输出格式的，如果为on那么所有输出都将在一行中展示，不适合人阅读，所以我们就保持其默认值为off吧。</p>
</blockquote>
<pre><code class="language-mysql">mysql&gt; SET optimizer_trace=&quot;enabled=on&quot;;
Query OK, 0 rows affected (0.00 sec)
</code></pre>
<p>当该查询语句执行完成后，就可以到information_schema数据库下的OPTIMIZER_TRACE表中查看完整的优化过程。这个OPTIMIZER_TRACE表有4个列，分别是：</p>
<ol>
<li>QUERY：表示我们的查询语句。</li>
<li>TRACE：表示优化过程的JSON格式文本。</li>
<li>MISSING_BYTES_BEYOND_MAX_MEM_SIZE：由于优化过程可能会输出很多，如果超过某个限制时，多余的文本将不会被显示，这个字段展示了被忽略的文本字节数。</li>
<li>INSUFFICIENT_PRIVILEGES：表示是否没有权限查看优化过程，默认值是0，只有某些特殊情况下才会是1，我们暂时不关心这个字段的值。</li>
</ol>
<p>优化器执行过程大致分为了三个阶段：</p>
<ul>
<li>prepare阶段</li>
<li>optimize阶段</li>
<li>execute阶段</li>
</ul>
<p>我们所说的基于成本的优化主要集中在optimize阶段，对于单表查询来说，我们主要关注optimize阶段的&quot;rows_estimation&quot;这个过程，这个过程深入分析了对单表查询的各种执行方案的成本；对于多表连接查询来说，我们更多需要关注&quot;considered_execution_plans&quot;这个过程，这个过程里会写明各种不同的连接方式所对应的成本。反正优化器最终会选择成本最低的那种方案来作为最终的执行计划，也就是我们使用EXPLAIN语句所展现出的那种方案。</p>
<h2 id="第18章-调节磁盘和cpu的矛盾-innodb的buffer-pool">第18章 调节磁盘和CPU的矛盾-InnoDB的Buffer Pool</h2>
<p>InnoDB存储引擎在处理客户端的请求时，当需要访问某个页的数据时，就会把完整的页的数据全部加载到内存中，也就是说即使我们<code>只需要访问一个页的一条记录，那也需要先把整个页的数据加载到内存中</code>。将整个页加载到内存中后就可以进行读写访问了，在进行完读写访问之后并不着急把该页对应的内存空间释放掉，而是将其缓存起来，这样将来有请求再次访问该页面时，就可以省去磁盘IO的开销了。</p>
<h3 id="181-innodb的buffer-pool">18.1 InnoDB的Buffer Pool</h3>
<h4 id="1811-buffer-pool">18.1.1 Buffer Pool</h4>
<p>在MySQL服务器启动的时候就向操作系统申请了一片连续的内存，他们给这片内存起了个名，叫做<mark>Buffer Pool（中文名是缓冲池）</mark>。默认情况下Buffer Pool只有128M。可以在启动服务器的时候配置innodb_buffer_pool_size参数的值。</p>
<h4 id="1812-buffer-pool内部组成">18.1.2 Buffer Pool内部组成</h4>
<p>Buffer Pool中默认的缓存页大小和在磁盘上默认的页大小是一样的，都是16KB。InnoDB为每一个缓存页都创建了一些所谓的控制信息，这些控制信息包括该页所属的表空间编号、页号、缓存页在Buffer Pool中的地址、链表节点信息、一些锁信息以及LSN信息等。</p>
<p>每个缓存页对应的控制信息占用的内存大小是相同的，我们就把每个页对应的控制信息占用的一块内存称为一个控制块吧，<mark>控制块和缓存页是一一对应的，它们都被存放到 Buffer Pool 中，其中控制块被存放到 Buffer Pool 的前面，缓存页被存放到 Buffer Pool 后边</mark>。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1677054865529.png" alt="" loading="lazy"></figure>
<h4 id="1813-free链表的管理">18.1.3 free链表的管理</h4>
<p>随着程序的运行，会不断的有磁盘上的页被缓存到Buffer Pool中。那么问题来了，从磁盘上读取一个页到Buffer Pool中的时候该放到哪个缓存页的位置呢？或者说怎么区分Buffer Pool中哪些缓存页是空闲的，哪些已经被使用了呢？</p>
<p>我们最好在某个地方记录一下Buffer Pool中哪些缓存页是可用的，这个时候缓存页对应的控制块就派上大用场了，我们可以把所有空闲的缓存页对应的控制块作为一个节点放到一个链表中，这个链表也可以被称作<mark>free链表（或者说空闲链表）</mark>。刚刚完成初始化的Buffer Pool中所有的缓存页都是空闲的，所以每一个缓存页对应的控制块都会被加入到free链表中，假设该Buffer Pool中可容纳的缓存页数量为n，那增加了free链表的效果图就是这样的：</p>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1677054988757.png" alt="" loading="lazy"></figure>
<p>链表的基节点占用的内存空间并不包含在为Buffer Pool申请的一大片连续内存空间之内，而是单独申请的一块内存空间。</p>
<h4 id="1814-缓存页的哈希处理">18.1.4 缓存页的哈希处理</h4>
<p>当我们需要访问某个页中的数据时，就会把该页从磁盘加载到Buffer Pool中，如果该页已经在Buffer Pool中的话直接使用就可以了。那么问题也就来了，我们怎么知道该页在不在Buffer Pool中呢？</p>
<p>所以我们可以用表空间号 + 页号作为key，缓存页作为value创建一个哈希表，在需要访问某个页的数据时，先从哈希表中根据表空间号 + 页号看看有没有对应的缓存页，如果有，直接使用该缓存页就好，如果没有，那就从free链表中选一个空闲的缓存页，然后把磁盘中对应的页加载到该缓存页的位置。</p>
<h4 id="1815-flush链表的管理">18.1.5 flush链表的管理</h4>
<p>如果我们修改了Buffer Pool中某个缓存页的数据，那它就和磁盘上的页不一致了，这样的缓存页也被称为<mark>脏页（英文名：dirty page）</mark>。</p>
<p>所以每次修改缓存页后，我们并不着急立即把修改同步到磁盘上，而是在未来的某个时间点进行同步，但是如果不立即同步到磁盘的话，那之后再同步的时候我们怎么知道Buffer Pool中哪些页是脏页，哪些页从来没被修改过呢？</p>
<p>所以，我们不得不再创建一个存储脏页的链表，凡是修改过的缓存页对应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的，所以也叫flush链表。链表的构造和free链表差不多，假设某个时间点Buffer Pool中的脏页数量为n，那么对应的flush链表就长这样：<br>
<img src="https://q456qq520.github.io/post-images/1677058320210.png" alt="" loading="lazy"></p>
<h4 id="1816-lru链表的管理">18.1.6 LRU链表的管理</h4>
<h5 id="缓存不够的窘境">缓存不够的窘境</h5>
<p>Buffer Pool对应的内存大小毕竟是有限的，如果需要缓存的页占用的内存大小超过了Buffer Pool大小，也就是free链表中已经没有多余的空闲缓存页的时候就需要把某些旧的缓存页从Buffer Pool中移除，然后再把新的页放进来， 那么问题来了，移除哪些缓存页呢？</p>
<h5 id="简单的lru链表">简单的LRU链表</h5>
<p>当Buffer Pool中不再有空闲的缓存页时，就需要淘汰掉部分最近很少使用的缓存页。</p>
<p>不过，我们怎么知道哪些缓存页最近频繁使用，我们可以再创建一个链表，由于这个链表是为了按照<mark>最近最少使用</mark>的原则去淘汰缓存页的，所以这个链表可以被称为<mark>LRU链表（LRU的英文全称：Least Recently Used）</mark>。当我们需要访问某个页时，可以这样处理LRU链表：</p>
<ol>
<li>如果该页不在Buffer Pool中，在把该页从磁盘加载到Buffer Pool中的缓存页时，就把该缓存页对应的控制块作为节点塞到链表的头部。</li>
<li>如果该页已经缓存在Buffer Pool中，则直接把该页对应的控制块移动到LRU链表的头部。</li>
</ol>
<p>也就是说：只要我们使用到某个缓存页，就把该缓存页调整到LRU链表的头部，这样LRU链表尾部就是最近最少使用的缓存页喽～ 所以当Buffer Pool中的空闲缓存页使用完时，到LRU链表的尾部找些缓存页淘汰。</p>
<h5 id="划分区域的lru链表">划分区域的LRU链表</h5>
<p>上面的这个简单的LRU链表用了没多长时间就发现问题了，因为存在这两种比较尴尬的情况：</p>
<p>情况一：InnoDB提供了一个看起来比较贴心的服务——<code>预读（英文名：read ahead）</code>。所谓预读，就是InnoDB认为执行当前的请求可能之后会读取某些页面，就预先把它们加载到Buffer Pool中。根据触发方式的不同，预读又可以细分为下面两种：</p>
<ul>
<li>
<p>线性预读<br>
InnoDB提供了一个系统变量innodb_read_ahead_threshold，如果顺序访问了某个区（extent）的页面超过这个系统变量的值，就会触发一次异步读取下一个区中全部的页面到Buffer Pool的请求，注意异步读取意味着从磁盘中加载这些被预读的页面并不会影响到当前工作线程的正常执行。这个innodb_read_ahead_threshold系统变量的值默认是56，我们可以在服务器启动时通过启动参数或者服务器运行过程中直接调整该系统变量的值，不过它是一个全局变量，需要使用SET GLOBAL命令来修改。</p>
</li>
<li>
<p>随机预读<br>
如果Buffer Pool中已经缓存了某个区的13个连续的页面，不论这些页面是不是顺序读取的，都会触发一次异步读取本区中所有其的页面到Buffer Pool的请求。InnoDB同时提供了innodb_random_read_ahead系统变量，它的默认值为OFF，也就意味着InnoDB并不会默认开启随机预读的功能，如果我们想开启该功能，可以通过修改启动参数或者直接使用SET GLOBAL命令把该变量的值设置为ON。</p>
</li>
</ul>
<p>预读本来是个好事儿，如果预读到Buffer Pool中的页成功的被使用到，那就可以极大的提高语句执行的效率。可是如果用不到呢？这些预读的页都会放到LRU链表的头部，但是如果此时Buffer Pool的容量不太大而且很多预读的页面都没有用到的话，这就会导致处在LRU链表尾部的一些缓存页会很快的被淘汰掉，也就是所谓的劣币驱逐良币，会大大降低缓存命中率。</p>
<p>情况二：可能会写一些需要扫描全表的查询语句（比如没有建立合适的索引或者压根儿没有WHERE子句的查询）。</p>
<p>扫描全表意味着什么？意味着将访问到该表所在的所有页！假设这个表中记录非常多的话，那该表会占用特别多的页，当需要访问这些页时，会把它们统统都加载到Buffer Pool中，这也就意味Buffer Pool中的所有页都被换了一次血，其他查询语句在执行时又得执行一次从磁盘加载到Buffer Pool的操作。而这种全表扫描的语句执行的频率也不高，每次执行都要把Buffer Pool中的缓存页换一次血，这严重的影响到其他查询对 Buffer Pool的使用，从而大大降低了缓存命中率。</p>
<p>总结就是加载到Buffer Pool中的页不一定被用到或者如果非常多的使用频率偏低的页被同时加载到Buffer Pool时，可能会把那些使用频率非常高的页从Buffer Pool中淘汰掉。</p>
<p>因为有这两种情况的存在，所以InnoDB把这个LRU链表按照一定比例分成两截，分别是：</p>
<ul>
<li>一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做热数据，或者称young区域。</li>
<li>另一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做冷数据，或者称old区域。</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1677059398007.png" alt="" loading="lazy"></figure>
<p>我们是按照某个比例将LRU链表分成两半的，不是某些节点固定是young区域的，某些节点固定是old区域的，随着程序的运行，某个节点所属的区域也可能发生变化。那这个划分成两截的比例怎么确定呢？对于InnoDB存储引擎来说，我们可以通过查看系统变量innodb_old_blocks_pct的值来确定old区域在LRU链表中所占的比例。</p>
<blockquote>
<p>mysql&gt; SHOW VARIABLES LIKE 'innodb_old_blocks_pct';</p>
</blockquote>
<p>有了这个被划分成young和old区域的LRU链表之后，InnoDB就可以针对我们上面提到的两种可能降低缓存命中率的情况进行优化了：</p>
<ol>
<li>当磁盘上的某个页面在初次加载到Buffer Pool中的某个缓存页时，该缓存页对应的控制块会被放到old区域的头部。这样针对预读到Buffer Pool却不进行后续访问的页面就会被逐渐从old区域逐出，而不会影响young区域中被使用比较频繁的缓存页。</li>
<li>在进行全表扫描时，虽然首次被加载到Buffer Pool的页被放到了old区域的头部，但是后续会被马上访问到，每次进行访问的时候又会把该页放到young区域的头部，这样仍然会把那些使用频率比较高的页面给顶下去。所以我们只需要规定，在对某个处在old区域的缓存页进行第一次访问时就在它对应的控制块中记录下来这个访问时间，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从old区域移动到young区域的头部，否则将它移动到young区域的头部。上述的这个间隔时间是由系统变量innodb_old_blocks_time控制的。默认值是1000，它的单位是毫秒。</li>
</ol>
<blockquote>
<p>mysql&gt; SHOW VARIABLES LIKE 'innodb_old_blocks_time';</p>
</blockquote>
<h5 id="更进一步优化lru链表">更进一步优化LRU链表</h5>
<p>对于young区域的缓存页来说，我们每次访问一个缓存页就要把它移动到LRU链表的头部，这样开销是不是太大啦，毕竟在young区域的缓存页都是热点数据，也就是可能被经常访问的，这样频繁的对LRU链表进行节点移动操作是不太好的，为了解决这个问题其实我们还可以提出一些优化策略，比如只有被访问的缓存页位于young区域的1/4的后边，才会被移动到LRU链表头部，这样就可以降低调整LRU链表的频率，从而提升性能。</p>
<h4 id="1817-其他的一些链表">18.1.7 其他的一些链表</h4>
<p>为了更好的管理Buffer Pool中的缓存页，除了我们上面提到的一些措施，设计InnoDB的大佬们还引进了其他的一些链表，比如unzip LRU链表用于管理解压页，zip clean链表用于管理没有被解压的压缩页，zip free数组中每一个元素都代表一个链表，它们组成所谓的伙伴系统来为压缩页提供内存空间等等。</p>
<h3 id="182-刷新脏页到磁盘">18.2 刷新脏页到磁盘</h3>
<p>后台有专门的线程每隔一段时间负责把脏页刷新到磁盘，这样可以不影响用户线程处理正常的请求。主要有两种刷新路径：</p>
<ol>
<li>从LRU链表的冷数据中刷新一部分页面到磁盘。<br>
后台线程会定时从LRU链表尾部开始扫描一些页面，扫描的页面数量可以通过系统变量innodb_lru_scan_depth来指定，如果从里边儿发现脏页，会把它们刷新到磁盘。这种刷新页面的方式被称之为BUF_FLUSH_LRU。</li>
<li>从flush链表中刷新一部分页面到磁盘。<br>
后台线程也会定时从flush链表中刷新一部分页面到磁盘，刷新的速率取决于当时系统是不是很繁忙。这种刷新页面的方式被称之为BUF_FLUSH_LIST。<br>
有时候后台线程刷新脏页的进度比较慢，导致用户线程在准备加载一个磁盘页到Buffer Pool时没有可用的缓存页，这时就会尝试看看LRU链表尾部有没有可以直接释放掉的未修改页面，如果没有的话会不得不将LRU链表尾部的一个脏页同步刷新到磁盘（和磁盘交互是很慢的，这会降低处理用户请求的速度）。这种刷新单个页面到磁盘中的刷新方式被称之为BUF_FLUSH_SINGLE_PAGE。</li>
</ol>
<h3 id="183-多个buffer-pool实例">18.3 多个Buffer Pool实例</h3>
<p>Buffer Pool本质是InnoDB向操作系统申请的一块连续的内存空间，在多线程环境下，访问Buffer Pool中的各种链表都需要加锁处理什么的，在Buffer Pool特别大而且多线程并发访问特别高的情况下，单一的Buffer Pool可能会影响请求的处理速度。</p>
<p>所以在Buffer Pool特别大的时候，我们可以把它们拆分成若干个小的Buffer Pool，每个Buffer Pool都称为一个实例，它们都是独立的，独立的去申请内存空间，独立的管理各种链表，独立的等等，所以在多线程并发访问时并不会相互影响，从而提高并发处理能力。我们可以在服务器启动的时候通过设置innodb_buffer_pool_instances的值来修改Buffer Pool实例的个数。</p>
<h4 id="1831-innodb_buffer_pool_chunk_size">18.3.1 innodb_buffer_pool_chunk_size</h4>
<p>每次当我们要重新调整Buffer Pool大小时，都需要重新向操作系统申请一块连续的内存空间，然后将旧的Buffer Pool中的内容复制到这一块新空间，这是极其耗时的。所以MySQL不再一次性为某个Buffer Pool实例向操作系统申请一大片连续的内存空间，而是以一个所谓的chunk为单位向操作系统申请空间。也就是说一个Buffer Pool实例其实是由若干个chunk组成的，一个chunk就代表一片连续的内存空间，里边儿包含了若干缓存页与其对应的控制块。</p>
<h4 id="1832-配置buffer-pool时的注意事项">18.3.2 配置Buffer Pool时的注意事项</h4>
<ol>
<li>innodb_buffer_pool_size必须是innodb_buffer_pool_chunk_size × innodb_buffer_pool_instances的倍数（这主要是想保证每一个Buffer Pool实例中包含的chunk数量相同）。</li>
<li>如果在服务器启动时，innodb_buffer_pool_chunk_size × innodb_buffer_pool_instances的值已经大于innodb_buffer_pool_size的值，那么innodb_buffer_pool_chunk_size的值会被服务器自动设置为innodb_buffer_pool_size/innodb_buffer_pool_instances的值。</li>
</ol>
<h3 id="184-查看buffer-pool的状态信息">18.4 查看Buffer Pool的状态信息</h3>
<blockquote>
<p>mysql&gt; SHOW ENGINE INNODB STATUS\G</p>
</blockquote>
<h2 id="第19章-事务简介">第19章 事务简介</h2>
<p>在对某个页面进行读写访问时，都会先把这个页面加载到Buffer Pool中，之后如果修改了某个页面，也不会立即把修改同步到磁盘，而只是把这个修改了的页面加到Buffer Pool的flush链表中，在之后的某个时间点才会刷新到磁盘。</p>
<h3 id="191-事务的概念">19.1 事务的概念</h3>
<p>数据库把需要保证原子性、隔离性、一致性和持久性的一个或多个数据库操作称之为一个事务（英文名是：transaction）。</p>
<p>事务是一个抽象的概念，它其实对应着一个或多个数据库操作，数据库根据这些操作所执行的不同阶段把事务大致上划分成了这么几个状态：</p>
<ol>
<li>活动的（active）<br>
事务对应的数据库操作正在执行过程中时，我们就说该事务处在活动的状态。</li>
<li>部分提交的（partially committed）<br>
当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并没有刷新到磁盘时，我们就说该事务处在部分提交的状态。</li>
<li>失败的（failed）<br>
当事务处在活动的或者部分提交的状态时，可能遇到了某些错误（数据库自身的错误、操作系统错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在失败的状态。</li>
<li>中止的（aborted）<br>
从而当前事务处在了失败的状态，要撤销失败事务对当前数据库造成的影响。我们把这个撤销的过程称之为回滚。当回滚操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了中止的状态。</li>
<li>提交的（committed）<br>
当一个处在部分提交的状态的事务将修改过的数据都同步到磁盘上之后，我们就可以说该事务处在了提交的状态。</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://q456qq520.github.io/post-images/1677061485111.png" alt="" loading="lazy"></figure>
<p><mark><strong>只有当事务处于提交的或者中止的状态时，一个事务的生命周期才算是结束了。</strong></mark></p>
<p>对于已经提交的事务来说，该事务对数据库所做的修改将永久生效，对于处于中止状态的事务，该事务对数据库所做的所有修改都会被回滚到没执行该事务之前的状态。</p>
<h3 id="192-mysql中事务的语法">19.2 MySQL中事务的语法</h3>
<h4 id="1921-开启事务">19.2.1 开启事务</h4>
<p>我们可以使用下面两种语句之一来开启一个事务：</p>
<ol>
<li>BEGIN [WORK];<br>
BEGIN语句代表开启一个事务，后边的单词WORK可有可无。开启事务后，就可以继续写若干条语句，这些语句都属于刚刚开启的这个事务。</li>
<li>START TRANSACTION;<br>
START TRANSACTION语句和BEGIN语句有着相同的功效，都标志着开启一个事务，不过可以在START TRANSACTION语句后边跟随几个修饰符。
<ul>
<li>READ ONLY：标识当前事务是一个只读事务</li>
<li>READ WRITE：标识当前事务是一个读写事务，默认方式</li>
<li>WITH CONSISTENT SNAPSHOT：启动一致性读</li>
</ul>
</li>
</ol>
<h4 id="1922-提交事务">19.2.2 提交事务</h4>
<p>提交事务的语法为：COMMIT [WORK]，COMMIT语句就代表提交一个事务，后边的WORK可有可无。</p>
<h4 id="1923-手动中止事务">19.2.3 手动中止事务</h4>
<p>手动中止事务的语法为：ROLLBACK [WORK]，ROLLBACK语句就代表中止并回滚一个事务，后边的WORK可有可无类似的。</p>
<h3 id="193-支持事务的存储引擎">19.3 支持事务的存储引擎</h3>
<p>MySQL中并不是所有存储引擎都支持事务的功能，目前只有InnoDB和NDB存储引擎支持（NDB存储引擎不是我们的重点），如果某个事务中包含了修改使用不支持事务的存储引擎的表，那么对该使用不支持事务的存储引擎的表所做的修改将无法进行回滚。</p>
<h3 id="194-自动提交">19.4 自动提交</h3>
<p>MySQL中有一个系统变量autocommit：</p>
<pre><code class="language-mysql">mysql&gt; SHOW VARIABLES LIKE 'autocommit';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| autocommit    | ON    |
+---------------+-------+
1 row in set (0.01 sec)
</code></pre>
<p>可以看到它的默认值为ON，也就是说默认情况下，如果我们不显式的使用START TRANSACTION或者BEGIN语句开启一个事务，那么每一条语句都算是一个独立的事务，这种特性称之为事务的自动提交。</p>
<h3 id="195-隐式提交">19.5 隐式提交</h3>
<p>当我们使用START TRANSACTION或者BEGIN语句开启了一个事务，或者把系统变量autocommit的值设置为OFF时，事务就不会进行自动提交，但是如果我们输入了某些语句之后就会悄悄的提交掉，就像我们输入了COMMIT语句了一样，这种因为某些特殊的语句而导致事务提交的情况称为隐式提交，这些会导致事务隐式提交的语句包括：</p>
<ol>
<li>定义或修改数据库对象的数据定义语言（Data definition language，缩写为：DDL）。</li>
<li>隐式使用或修改mysql数据库中的表</li>
<li>事务控制或关于锁定的语句<br>
当我们在一个事务还没提交或者回滚时就又使用START TRANSACTION或者BEGIN语句开启了另一个事务时，会隐式的提交上一个事。</li>
<li>加载数据的语句<br>
比如我们使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前面语句所属的事务。</li>
<li>关于MySQL复制的一些语句<br>
使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句时也会隐式的提交前面语句所属的事务。</li>
<li>其它的一些语句<br>
使用ANALYZE TABLE、CACHE INDEX、CHECK TABLE、FLUSH、 LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前面语句所属的事务。</li>
</ol>
<h3 id="196-保存点">19.6 保存点</h3>
<p>如果你开启了一个事务，并且已经敲了很多语句，忽然发现上一条语句有点问题，你只好使用ROLLBACK语句来让数据库状态恢复到事务执行之前的样子，然后一切从头再来，总有一种一夜回到解放前的感觉。数据库提出了一个保存点（英文：savepoint）的概念，就是在事务对应的数据库语句中打几个点，我们在调用ROLLBACK语句时可以指定会滚到哪个点，而不是回到最初的原点。定义保存点的语法如下：</p>
<blockquote>
<p>SAVEPOINT 保存点名称;</p>
</blockquote>
<p>当我们想回滚到某个保存点时，可以使用下面这个语句（下面语句中的单词WORK和SAVEPOINT是可有可无的）：</p>
<blockquote>
<p>ROLLBACK [WORK] TO [SAVEPOINT] 保存点名称;</p>
</blockquote>
<p>如果我们想删除某个保存点，可以使用这个语句：</p>
<blockquote>
<p>RELEASE SAVEPOINT 保存点名称;</p>
</blockquote>
<h2 id="第20章-说过的话就一定要办到-redo日志上">第20章 说过的话就一定要办到-redo日志（上）</h2>
<h3 id="201-redo日志是什么">20.1 redo日志是什么</h3>
<p>InnoDB存储引擎是以页为单位来管理存储空间的，进行的增删改查操作其实本质上都是在访问页面（包括读页面、写页面、创建新页面等操作）。在真正访问页面之前，需要把在磁盘上的页缓存到内存中的Buffer Pool之后才可以访问。但是事务的时候又有一个称之为持久性的特性，就是说对于一个已经提交的事务，在事务提交后即使系统发生了崩溃，这个事务对数据库中所做的更改也不能丢失。假设在事务提交后突然发生了某个故障，导致内存中的数据都失效了，那么这个已经提交了的事务对数据库中所做的更改也就跟着丢失了，这是我们所不能忍受的。那么如何保证这个持久性呢？</p>
<p>我们只是想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。所以我们其实没有必要在每次事务提交时就把该事务在内存中修改过的全部页面刷新到磁盘，只需要把修改了哪些东西记录一下就好，这样我们在事务提交时，把上述内容刷新到磁盘中，即使之后系统崩溃了，重启之后只要按照上述内容所记录的步骤重新更新一下数据页，那么该事务对数据库中所做的修改又可以被恢复出来，也就意味着满足持久性的要求。因为在系统奔溃重启时需要按照上述内容所记录的步骤重新更新数据页，所以上述内容也被称之为<mark>重做日志</mark>，英文名为<mark>redo log</mark>。</p>
<p>redo日志刷新到磁盘的好处如下：</p>
<ul>
<li>redo日志占用的空间非常小<br>
  存储表空间ID、页号、偏移量以及需要更新的值所需的存储空间是很小的。</li>
<li>redo日志是顺序写入磁盘的<br>
  在执行事务的过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO。</li>
</ul>
<h3 id="202-redo日志格式">20.2 redo日志格式</h3>
<p>redo日志本质上只是记录了一下事务对数据库做了哪些修改。 InnoDB针对事务对数据库的不同修改场景定义了多种类型的redo日志，但是绝大部分类型的redo日志都有下面这种通用的结构：<br>
<img src="https://q456qq520.github.io/post-images/1677146469313.png" alt="" loading="lazy"></p>
<ol>
<li>type：该条redo日志的类型。<br>
  在MySQL 5.7.21这个版本中，InnoDB一共为redo日志设计了53种不同的类型。</li>
<li>space ID：表空间ID。</li>
<li>page number：页号。</li>
<li>data：该条redo日志的具体内容。</li>
</ol>
<h3 id="203-简单的redo日志类型">20.3 简单的redo日志类型</h3>
<p>如果我们没有为某个表显式的定义主键，并且表中也没有定义Unique键，那么InnoDB会自动的为表添加一个称之为row_id的隐藏列作为主键。为这个row_id隐藏列赋值的方式如下：</p>
<ol>
<li>服务器会在内存中维护一个全局变量，每当向某个包含隐藏的row_id列的表中插入一条记录时，就会把该变量的值当作新记录的row_id列的值，并且把该变量自增1。</li>
<li>每当这个变量的值为256的倍数时，就会将该变量的值刷新到系统表空间的页号为7的页面中一个称之为Max Row ID的属性处</li>
<li>当系统启动时，会将上面提到的Max Row ID属性加载到内存中，将该值加上256之后赋值给我们前面提到的全局变量</li>
</ol>
<p>这个Max Row ID属性占用的存储空间是8个字节，当某个事务向某个包含row_id隐藏列的表插入一条记录，并且为该记录分配的row_id值为256的倍数时，就会向系统表空间页号为7的页面的相应偏移量处写入8个字节的值。但是我们要知道，这个写入实际上是在Buffer Pool中完成的，我们需要为这个页面的修改记录一条redo日志，以便在系统奔溃后能将已经提交的该事务对该页面所做的修改恢复出来。这种情况下<mark>redo日志中只需要记录一下在某个页面的某个偏移量处修改了几个字节的值</mark>，具体被修改的内容是什么就好了，InnoDB把这种极其简单的redo日志称之为物理日志，并且根据在页面中写入数据的多少划分了几种不同的redo日志类型：</p>
<ul>
<li>MLOG_1BYTE（type字段对应的十进制数字为1）：表示在页面的某个偏移量处写入1个字节的redo日志类型。</li>
<li>MLOG_2BYTE（type字段对应的十进制数字为2）：表示在页面的某个偏移量处写入2个字节的redo日志类型。</li>
<li>MLOG_4BYTE（type字段对应的十进制数字为4）：表示在页面的某个偏移量处写入4个字节的redo日志类型。</li>
<li>MLOG_8BYTE（type字段对应的十进制数字为8）：表示在页面的某个偏移量处写入8个字节的redo日志类型。</li>
<li>MLOG_WRITE_STRING（type字段对应的十进制数字为30）：表示在页面的某个偏移量处写入一串数据。</li>
</ul>
<p>上面提到的Max Row ID属性实际占用8个字节的存储空间，MLOG_8BYTE的redo日志结构如下所示：<br>
<img src="https://q456qq520.github.io/post-images/1677146763313.png" alt="" loading="lazy"></p>
<p>其余MLOG_1BYTE、MLOG_2BYTE、MLOG_4BYTE类型的redo日志结构和MLOG_8BYTE的类似，只不过具体数据中包含对应个字节的数据罢了。MLOG_WRITE_STRING类型的redo日志表示写入一串数据，但是因为不能确定写入的具体数据占用多少字节，所以需要在日志结构中添加一个<mark>len</mark>字段。</p>
<h3 id="204-复杂一些的redo日志类型">20.4 复杂一些的redo日志类型</h3>
<p>有时候执行一条语句会修改非常多的页面，包括系统数据页面和用户数据页面（用户数据指的就是聚簇索引和二级索引对应的B+树）。以一条INSERT语句为例，它除了要向B+树的页面中插入数据，也可能更新系统数据Max Row ID的值，不过对于我们用户来说，平时更关心的是语句对B+树所做更新：</p>
<ul>
<li>表中包含多少个索引，一条INSERT语句就可能更新多少棵B+树。</li>
<li>针对某一棵B+树来说，既可能更新叶子节点页面，也可能更新内节点页面，也可能创建新的页面（在该记录插入的叶子节点的剩余空间比较少，不足以存放该记录时，会进行页面的分裂，在内节点页面中添加目录项记录）。</li>
</ul>
<p>在语句执行过程中，INSERT语句对所有页面的修改都得保存到redo日志中去。这句话说的比较轻巧，做起来可就比较麻烦了，一个数据页中除了存储实际的记录之后，还有什么File Header、Page Header、Page Directory等等部分（在介绍数据页的章节有详细讲解），所以每往叶子节点代表的数据页里插入一条记录时，还有其他很多地方会跟着更新，比如说：</p>
<ol>
<li>可能更新Page Directory中的槽信息。</li>
<li>Page Header中的各种页面统计信息，比如PAGE_N_DIR_SLOTS表示的槽数量可能会更改，PAGE_HEAP_TOP代表的还未使用的空间最小地址可能会更改，PAGE_N_HEAP代表的本页面中的记录数量可能会更改，等等，各种信息都可能会被修改。</li>
<li>我们知道在数据页里的记录是按照索引列从小到大的顺序组成一个单向链表的，每插入一条记录，还需要更新上一条记录的记录头信息中的next_record属性来维护这个单向链表。<br>
<img src="https://q456qq520.github.io/post-images/1677207440794.png" alt="" loading="lazy"><br>
把一条记录插入到一个页面时需要更改的地方非常多。这时我们如果使用上面介绍的简单的物理redo日志来记录这些修改时，可以有两种解决方案：</li>
</ol>
<p>方案一：在每个修改的地方都记录一条redo日志。<br>
  有多少数据修改，就写多少条物理redo日志。这样子记录redo日志的缺点是显而易见的，因为被修改的地方是在太多了，可能记录的redo日志占用的空间都比整个页面占用的空间都多了。<br>
方案二：将整个页面的第一个被修改的字节到最后一个修改的字节之间所有的数据当成是一条物理redo日志中的具体数据。从图中也可以看出来，第一个被修改的字节到最后一个修改的字节之间仍然有许多没有修改过的数据，我们把这些没有修改的数据也加入到redo日志中去太浪费了。</p>
<p>正因为上述两种使用物理redo日志的方式来记录某个页面中做了哪些修改比较浪费，InnoDB提出了一些新的redo日志类型，比如：</p>
<ul>
<li>MLOG_REC_INSERT（对应的十进制数字为9）：表示插入一条使用非紧凑行格式的记录时的redo日志类型。</li>
<li>MLOG_COMP_REC_INSERT（对应的十进制数字为38）：表示插入一条使用紧凑行格式的记录时的redo日志类型。</li>
<li>MLOG_COMP_PAGE_CREATE（type字段对应的十进制数字为58）：表示创建一个存储紧凑行格式记录的页面的redo日志类型。</li>
<li>MLOG_COMP_REC_DELETE（type字段对应的十进制数字为42）：表示删除一条使用紧凑行格式记录的redo日志类型。</li>
<li>MLOG_COMP_LIST_START_DELETE（type字段对应的十进制数字为44）：表示从某条给定记录开始删除页面中的一系列使用紧凑行格式记录的redo日志类型。</li>
<li>MLOG_COMP_LIST_END_DELETE（type字段对应的十进制数字为43）：与MLOG_COMP_LIST_START_DELETE类型的redo日志呼应，表示删除一系列记录直到MLOG_COMP_LIST_END_DELETE类型的redo日志对应的记录为止。</li>
<li>MLOG_ZIP_PAGE_COMPRESS（type字段对应的十进制数字为51）：表示压缩一个数据页的redo日志类型。</li>
</ul>
<p>这些类型的redo日志既包含物理层面的意思，也包含逻辑层面的意思，具体指：<br>
物理层面看，这些日志都指明了对哪个表空间的哪个页进行了修改。<br>
逻辑层面看，在系统奔溃重启时，并不能直接根据这些日志里的记载，将页面内的某个偏移量处恢复成某个数据，而是需要调用一些事先准备好的函数，执行完这些函数后才可以将页面恢复成系统奔溃前的样子。</p>
<p>我们以类型为MLOG_COMP_REC_INSERT这个代表插入一条使用紧凑行格式的记录时的redo日志为例来理解一下我们上面所说的物理层面和逻辑层面到底是什么意思。<br>
<img src="https://q456qq520.github.io/post-images/1677208360208.png" alt="" loading="lazy"></p>
<blockquote>
<p>n_uniques的值为主键的列数，对于其他二级索引来说，该值为索引列数+主键列数。</p>
</blockquote>
<h3 id="205-mini-transaction">20.5 Mini-Transaction</h3>
<h4 id="2051-以组的形式写入redo日志">20.5.1 以组的形式写入redo日志</h4>
<p>语句在执行过程中可能修改若干个页面。比如我们前面说的一条INSERT语句可能修改系统表空间页号为7的页面的Max Row ID属性，还会更新聚簇索引和二级索引对应B+树中的页面。由于对这些页面的更改都发生在Buffer Pool中，所以在修改完页面之后，需要记录一下相应的redo日志。在执行语句的过程中产生的redo日志被InnoDB划分成了若干个不可分割的组，比如：</p>
<ol>
<li>更新Max Row ID属性时产生的redo日志是不可分割的。</li>
<li>向聚簇索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。</li>
<li>向某个二级索引对应B+树的页面中插入一条记录时产生的redo日志是不可分割的。</li>
<li>还有其他的一些对页面的访问操作时产生的redo日志是不可分割的。</li>
</ol>
<p>我们以向某个索引对应的B+树插入一条记录为例，在向B+树中插入这条记录之前，需要先定位到这条记录应该被插入到哪个叶子节点代表的数据页中，定位到具体的数据页之后，有两种可能的情况：</p>
<p>情况一：该数据页的剩余的空闲空间充足，足够容纳这一条待插入记录，那么事情很简单，直接把记录插入到这个数据页中，记录一条类型为MLOG_COMP_REC_INSERT的redo日志就好了，我们把这种情况称之为乐观插入。<br>
情况二：该数据页剩余的空闲空间不足，那么事情就悲剧了，我们前面说过，遇到这种情况要进行所谓的页分裂操作，也就是新建一个叶子节点，然后把原先数据页中的一部分记录复制到这个新的数据页中，然后再把记录插入进去，把这个叶子节点插入到叶子节点链表中，最后还要在内节点中添加一条目录项记录指向这个新创建的页面。很显然，这个过程要对多个页面进行修改，也就意味着会产生多条redo日志，我们把这种情况称之为悲观插入。</p>
<p>InnoDB认为向某个索引对应的B+树中插入一条记录的这个过程必须是原子的，不能说插了一半之后就停止了。比方说在悲观插入过程中，新的页面已经分配好了，数据也复制过去了，新的记录也插入到页面中了，可是没有向内节点中插入一条目录项记录，这个插入过程就是不完整的，这样会形成一棵不正确的B+树。我们知道redo日志是为了在系统奔溃重启时恢复崩溃前的状态，如果在悲观插入的过程中只记录了一部分redo日志，那么在系统奔溃重启时会将索引对应的B+树恢复成一种不正确的状态，这是InnoDB所不能忍受的。所以他们规定在执行这些需要保证原子性的操作时必须以组的形式来记录的redo日志，在进行系统奔溃重启恢复时，针对某个组中的redo日志，要么把全部的日志都恢复掉，要么一条也不恢复。怎么做到的呢？这得分情况讨论：</p>
<ol>
<li>
<p>有的需要保证原子性的操作会生成多条redo日志，比如向某个索引对应的B+树中进行一次悲观插入就需要生成许多条redo日志。<br>
就是在该组中的最后一条redo日志后边加上一条特殊类型的redo日志，该类型名称为MLOG_MULTI_REC_END，type字段对应的十进制数字为31，该类型的redo日志结构很简单，只有一个type字段。只有当解析到类型为MLOG_MULTI_REC_END的redo日志，才认为解析到了一组完整的redo日志，才会进行恢复。否则的话直接放弃前面解析到的redo日志。type字段的第一个比特位为1，代表该需要保证原子性的操作只产生了单一的一条redo日志，否则表示该需要保证原子性的操作产生了一系列的redo日志。<br>
<img src="https://q456qq520.github.io/post-images/1677209909187.png" alt="" loading="lazy"></p>
</li>
<li>
<p>有的需要保证原子性的操作只生成一条redo日志，比如更新Max Row ID属性的操作就只会生成一条redo日志。</p>
</li>
</ol>
<h4 id="2052-mini-transaction的概念">20.5.2 Mini-Transaction的概念</h4>
<p>MySQL把对底层页面中的一次原子访问的过程称之为一个<mark>Mini-Transaction</mark>，简称mtr，比如上面所说的修改一次Max Row ID的值算是一个Mini-Transaction，向某个索引对应的B+树中插入一条记录的过程也算是一个Mini-Transaction。通过上面的叙述我们也知道，一个所谓的mtr可以包含一组redo日志，在进行奔溃恢复时这一组redo日志作为一个不可分割的整体。</p>
<p><mark>一个事务可以包含若干条语句，每一条语句其实是由若干个mtr组成，每一个mtr又可以包含若干条redo日志。</mark></p>
<h3 id="206-redo日志的写入过程">20.6 redo日志的写入过程</h3>
<h4 id="2061-redo-log-block">20.6.1 redo log block</h4>
<p>通过mtr生成的redo日志都放在了大小为512字节的页中。为了和我们前面提到的表空间中的页做区别，我们这里把用来存储redo日志的页称为block。一个redo log block的示意图如下：<br>
<img src="https://q456qq520.github.io/post-images/1677210454228.png" alt="" loading="lazy"></p>
<p>真正的redo日志都是存储到占用496字节大小的<code>log block body</code>中，图中的<code>log block header</code>和<code>log block trailer</code>存储的是一些管理信息。</p>
<figure data-type="image" tabindex="5"><img src="https://q456qq520.github.io/post-images/1677210723157.png" alt="" loading="lazy"></figure>
<ul>
<li>LOG_BLOCK_HDR_NO：每一个block都有一个大于0的唯一标号，本属性就表示该标号值。</li>
<li>LOG_BLOCK_HDR_DATA_LEN：表示block中已经使用了多少字节，初始值为12（因为log block body从第12个字节处开始）。随着往block中写入的redo日志越来也多，本属性值也跟着增长。如果log block body已经被全部写满，那么本属性的值被设置为512。</li>
<li>LOG_BLOCK_FIRST_REC_GROUP：一条redo日志也可以称之为一条redo日志记录（redo log record），一个mtr会生产多条redo日志记录，这些redo日志记录被称之为一个redo日志记录组（redo log record group）。LOG_BLOCK_FIRST_REC_GROUP就代表该block中第一个mtr生成的redo日志记录组的偏移量（其实也就是这个block里第一个mtr生成的第一条redo日志的偏移量）。</li>
<li>LOG_BLOCK_CHECKPOINT_NO：表示所谓的checkpoint的序号</li>
<li>LOG_BLOCK_CHECKSUM：表示block的校验值，用于正确性校验。</li>
</ul>
<h4 id="2062-redo日志缓冲区">20.6.2 redo日志缓冲区</h4>
<p>写入redo日志时也不能直接直接写到磁盘上，实际上在服务器启动时就向操作系统申请了一大片称之为redo log buffer的连续内存空间，翻译成中文就是<mark>redo日志缓冲区</mark>，我们也可以简称为<mark>log buffer</mark>。这片内存空间被划分成若干个连续的redo log block，就像这样：</p>
<figure data-type="image" tabindex="6"><img src="https://q456qq520.github.io/post-images/1677489015933.png" alt="" loading="lazy"></figure>
<p>可以通过启动参数innodb_log_buffer_size来指定log buffer的大小，在MySQL 5.7.21这个版本中，该启动参数的默认值为16MB。</p>
<h4 id="2063-redo日志写入log-buffer">20.6.3 redo日志写入log buffer</h4>
<p>向log buffer中写入redo日志的过程是顺序的，也就是先往前面的block中写，当该block的空闲空间用完之后再往下一个block中写。当我们想往log buffer中写入redo日志时，第一个遇到的问题就是应该写在哪个block的哪个偏移量处，InnoDB提供了一个称之为buf_free的全局变量，该变量指明后续写入的redo日志应该写入到log buffer中的哪个位置，如图所示：<br>
<img src="https://q456qq520.github.io/post-images/1677489217583.png" alt="" loading="lazy"></p>
<p>一个mtr执行过程中可能产生若干条redo日志，这些redo日志是一个不可分割的组，所以其实并不是每生成一条redo日志，就将其插入到log buffer中，而是每个mtr运行过程中产生的日志先暂时存到一个地方，当该mtr结束的时候，将过程中产生的一组redo日志再全部复制到log buffer中。</p>
<h2 id="第21章-redo日志下">第21章 redo日志（下）</h2>
<h3 id="211-redo日志文件">21.1 redo日志文件</h3>
<h4 id="2111-redo日志刷盘时机">21.1.1 redo日志刷盘时机</h4>
<p>mtr运行过程中产生的一组redo日志在mtr结束时会被复制到log buffer中，在一些情况下它们会被刷新到磁盘里，比如：</p>
<ul>
<li>log buffer空间不足时<br>
  log buffer的大小是有限的（通过系统变量innodb_log_buffer_size指定），如果不停的往这个有限大小的log buffer里塞入日志，很快它就会被填满。设计InnoDB的大佬认为如果当前写入log buffer的redo日志量已经占满了log buffer总容量的大约一半左右，就需要把这些日志刷新到磁盘上。</li>
<li>事务提交时<br>
  我们前面说过之所以使用redo日志主要是因为它占用的空间少，还是顺序写，在事务提交时可以不把修改过的Buffer Pool页面刷新到磁盘，但是为了保证持久性，必须要把修改这些页面对应的redo日志刷新到磁盘。</li>
<li>后台线程不停的刷刷刷<br>
  后台有一个线程，大约每秒都会刷新一次log buffer中的redo日志到磁盘。</li>
<li>正常关闭服务器时</li>
<li>做checkpoint时</li>
</ul>
<h4 id="2112-redo日志文件组">21.1.2 redo日志文件组</h4>
<p>MySQL的数据目录（使用==SHOW VARIABLES LIKE 'datadir'==查看）下默认有两个名为<code>ib_logfile0</code>和<code>ib_logfile1</code>的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。如果我们对默认的redo日志文件不满意，可以通过下面几个启动参数来调节：</p>
<ul>
<li>innodb_log_group_home_dir<br>
  该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。</li>
<li>innodb_log_file_size<br>
  该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB，</li>
<li>innodb_log_files_in_group<br>
  该参数指定redo日志文件的个数，默认值为2，最大值为100。</li>
</ul>
<p>磁盘上的redo日志文件不只一个，而是以一个日志文件组的形式出现的。这些文件以ib_logfile[数字]（数字可以是0、1、2...）的形式进行命名。在将redo日志写入日志文件组时，是从ib_logfile0开始写，如果ib_logfile0写满了，就接着ib_logfile1写，同理，ib_logfile1写满了就去写ib_logfile2，依此类推。如果写到最后一个文件该咋办？那就重新转到ib_logfile0继续写，所以整个过程如下图所示：</p>
<figure data-type="image" tabindex="7"><img src="https://q456qq520.github.io/post-images/1677489786566.png" alt="" loading="lazy"></figure>
<h4 id="2113-redo日志文件格式">21.1.3 redo日志文件格式</h4>
<p>log buffer本质上是一片连续的内存空间，被划分成了若干个512字节大小的block。将log buffer中的redo日志刷新到磁盘的本质就是把block的镜像写入日志文件中，所以redo日志文件其实也是由若干个512字节大小的block组成。都是由两部分组成：</p>
<ul>
<li>前2048个字节，也就是前4个block是用来存储一些管理信息的。</li>
<li>从第2048字节往后是用来存储log buffer中的block镜像的。</li>
</ul>
<figure data-type="image" tabindex="8"><img src="https://q456qq520.github.io/post-images/1677489963509.png" alt="" loading="lazy"></figure>
<p>而其中前4个特殊block如下<br>
<img src="https://q456qq520.github.io/post-images/1677490599945.png" alt="" loading="lazy"></p>
<ol>
<li>log file header：描述该redo日志文件的一些整体属性</li>
</ol>
<table>
<thead>
<tr>
<th>属性名</th>
<th>长度（单位：字节）</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>LOG_HEADER_FORMAT</td>
<td>4</td>
<td>redo日志的版本，在MySQL 5.7.21中该值永远为1</td>
</tr>
<tr>
<td>LOG_HEADER_PAD1</td>
<td>4</td>
<td>做字节填充用的，没什么实际意义</td>
</tr>
<tr>
<td>LOG_HEADER_START_LSN</td>
<td>8</td>
<td>标记本redo日志文件开始的LSN值，也就是文件偏移量为2048字节初对应的LSN值</td>
</tr>
<tr>
<td>LOG_HEADER_CREATOR</td>
<td>32</td>
<td>一个字符串，标记本redo日志文件的创建者是谁。正常运行时该值为MySQL的版本号，比如：&quot;MySQL 5.7.21&quot;，使用mysqlbackup命令创建的redo日志文件的该值为&quot;ibbackup&quot;和创建时间。</td>
</tr>
<tr>
<td>LOG_BLOCK_CHECKSUM</td>
<td>4</td>
<td>本block的校验值，所有block都有</td>
</tr>
</tbody>
</table>
<ol start="2">
<li>checkpoint1：记录关于checkpoint的一些属性，看一下它的结构：</li>
</ol>
<table>
<thead>
<tr>
<th>属性名</th>
<th>长度（单位：字节）</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>LOG_CHECKPOINT_NO</td>
<td>8</td>
<td>服务器做checkpoint的编号，每做一次checkpoint，该值就加1。</td>
</tr>
<tr>
<td>LOG_CHECKPOINT_LSN</td>
<td>8</td>
<td>服务器做checkpoint结束时对应的LSN值，系统奔溃恢复时将从该值开始。</td>
</tr>
<tr>
<td>LOG_CHECKPOINT_OFFSET</td>
<td>8</td>
<td>上个属性中的LSN值在redo日志文件组中的偏移量</td>
</tr>
<tr>
<td>LOG_CHECKPOINT_LOG_BUF_SIZE</td>
<td>8</td>
<td>服务器在做checkpoint操作时对应的log buffer的大小</td>
</tr>
<tr>
<td>LOG_BLOCK_CHECKSUM</td>
<td>4</td>
<td>本block的校验值，所有block都有</td>
</tr>
</tbody>
</table>
<h3 id="212-log-sequeue-number">21.2 Log Sequeue Number</h3>
<p>InnoDB为记录已经写入的redo日志量，设计了一个称之为<mark>Log Sequeue Number</mark>的全局变量，翻译过来就是：日志序列号，简称<mark>lsn</mark>。InnoDB规定初始的lsn值为8704（也就是一条redo日志也没写入时，lsn的值为8704）。</p>
<p>每一组由mtr生成的redo日志都有一个唯一的LSN值与其对应，LSN值越小，说明redo日志产生的越早。</p>
<h4 id="2121-flushed_to_disk_lsn">21.2.1 flushed_to_disk_lsn</h4>
<p>redo日志是首先写到log buffer中，之后才会被刷新到磁盘上的redo日志文件。所以InnoDB提出了一个称之为<mark>buf_next_to_write</mark>的全局变量，标记当前log buffer中已经有哪些日志被刷新到磁盘中了。</p>
<p>InnoDB提出了一个表示刷新到磁盘中的redo日志量的全局变量，称之为<mark>flushed_to_disk_lsn</mark>。系统第一次启动时，该变量的值和初始的lsn值是相同的，都是8704。随着系统的运行，redo日志被不断写入log buffer，但是并不会立即刷新到磁盘，lsn的值就和flushed_to_disk_lsn的值拉开了差距。如果两者的值相同时，说明log buffer中的所有redo日志都已经刷新到磁盘中了。</p>
<h4 id="2122-lsn值和redo日志文件偏移量的对应关系">21.2.2 lsn值和redo日志文件偏移量的对应关系</h4>
<p>因为lsn的值是代表系统写入的redo日志量的一个总和，一个mtr中产生多少日志，lsn的值就增加多少（当然有时候要加上log block header和log block trailer的大小），这样mtr产生的日志写到磁盘中时，很容易计算某一个lsn值在redo日志文件组中的偏移量，如图：</p>
<figure data-type="image" tabindex="9"><img src="https://q456qq520.github.io/post-images/1677491173209.png" alt="" loading="lazy"></figure>
<h4 id="2123-flush链表中的lsn">21.2.3 flush链表中的LSN</h4>
<p>一个mtr代表一次对底层页面的原子访问，在访问过程中可能会产生一组不可分割的redo日志，在mtr结束时，会把这一组redo日志写入到log buffer中。除此之外，在mtr结束时还有一件非常重要的事情要做，就是把在mtr执行过程中可能修改过的页面加入到Buffer Pool的flush链表。</p>
<p>当第一次修改某个缓存在Buffer Pool中的页面时，就会把这个页面对应的控制块插入到flush链表的头部，之后再修改该页面时由于它已经在flush链表中了，就不再次插入了。也就是说flush链表中的脏页是按照页面的第一次修改时间从大到小进行排序的。在这个过程中会在缓存页对应的控制块中记录两个关于页面何时修改的属性：</p>
<ul>
<li>oldest_modification：如果某个页面被加载到Buffer Pool后进行第一次修改，那么就将修改该页面的mtr开始时对应的lsn值写入这个属性。</li>
<li>newest_modification：每修改一次页面，都会将修改该页面的mtr结束时对应的lsn值写入这个属性。也就是说该属性表示页面最近一次修改后对应的系统lsn值。</li>
</ul>
<p>假设mtr_1执行过程中修改了页a，那么在mtr_1执行结束时，就会将页a对应的控制块加入到flush链表的头部。并且将mtr_1开始时对应的lsn写入页a对应的控制块的oldest_modification属性中，把mtr_1结束时对应的lsn写入页a对应的控制块的newest_modification属性中。</p>
<p>总结一下上面说的，就是：flush链表中的脏页按照修改发生的时间顺序进行排序，也就是按照oldest_modification代表的LSN值进行排序，被多次更新的页面不会重复插入到flush链表中，但是会更新newest_modification属性的值。</p>
<h4 id="2124-checkpoint">21.2.4 checkpoint</h4>
<p>我们的redo日志文件组容量是有限的，我们不得不选择循环使用redo日志文件组中的文件，但是这会造成最后写的redo日志与最开始写的redo日志追尾，这时应该想到：<mark>redo日志只是为了系统奔溃后恢复脏页用的，如果对应的脏页已经刷新到了磁盘，也就是说即使现在系统奔溃，那么在重启后也用不着使用redo日志恢复该页面了，所以该redo日志也就没有存在的必要了，那么它占用的磁盘空间就可以被后续的redo日志所重用</mark>。也就是说：<mark>判断某些redo日志占用的磁盘空间是否可以覆盖的依据就是它对应的脏页是否已经刷新到磁盘里</mark>。我们看一下前面一直介绍的那个例子：<br>
<img src="https://q456qq520.github.io/post-images/1677491805432.png" alt="" loading="lazy"></p>
<p>虽然mtr_1和mtr_2生成的redo日志都已经被写到了磁盘上，但是它们修改的脏页仍然留在Buffer Pool中，所以它们生成的redo日志在磁盘上的空间是不可以被覆盖的。之后随着系统的运行，如果页a被刷新到了磁盘，那么它对应的控制块就会从flush链表中移除。</p>
<p>这样mtr_1生成的redo日志就没有用了，它们占用的磁盘空间就可以被覆盖掉了。设计InnoDB的大佬提出了一个全局变量<mark>checkpoint_lsn</mark>来代表当前系统中可以被覆盖的redo日志总量是多少，这个变量初始值也是8704。</p>
<p>比方说现在页a被刷新到了磁盘，mtr_1生成的redo日志就可以被覆盖了，所以我们可以进行一个增加checkpoint_lsn的操作，我们把这个过程称之为做一次checkpoint。做一次checkpoint其实可以分为两个步骤：</p>
<p>步骤一：计算一下当前系统中可以被覆盖的redo日志对应的lsn值最大是多少。<br>
  redo日志可以被覆盖，意味着它对应的脏页被刷到了磁盘，只要我们计算出当前系统中被最早修改的脏页对应的oldest_modification值，那凡是在系统lsn值小于该节点的oldest_modification值时产生的redo日志都是可以被覆盖掉的，我们就把该脏页的oldest_modification赋值给checkpoint_lsn。</p>
<p>比方说当前系统中页a已经被刷新到磁盘，那么flush链表的尾节点就是页c，该节点就是当前系统中最早修改的脏页了，它的oldest_modification值为8916，我们就把8916赋值给checkpoint_lsn（也就是说在redo日志对应的lsn值小于8916时就可以被覆盖掉）。</p>
<p>步骤二：将checkpoint_lsn和对应的redo日志文件组偏移量以及此次checkpint的编号写到日志文件的管理信息（就是checkpoint1或者checkpoint2）中。<br>
  InnoDB维护了一个目前系统做了多少次checkpoint的变量<mark>checkpoint_no</mark>，每做一次checkpoint，该变量的值就加1。我们前面说过计算一个lsn值对应的redo日志文件组偏移量是很容易的，所以可以计算得到该checkpoint_lsn在redo日志文件组中对应的偏移量checkpoint_offset，然后把这三个值都写到redo日志文件组的管理信息中。<br>
  每一个redo日志文件都有2048个字节的管理信息，但是上述关于checkpoint的信息只会被写到日志文件组的第一个日志文件的管理信息中。InnoDB规定，当checkpoint_no的值是偶数时，就写到checkpoint1中，是奇数时，就写到checkpoint2中。</p>
<h4 id="2125-批量从flush链表中刷出脏页">21.2.5 批量从flush链表中刷出脏页</h4>
<p>如果当前系统修改页面的操作十分频繁，这样就导致写日志操作十分频繁，系统lsn值增长过快。如果后台的刷脏操作不能将脏页刷出，那么系统无法及时做checkpoint，可能就需要用户线程同步的从flush链表中把那些最早修改的脏页（oldest_modification最小的脏页）刷新到磁盘，这样这些脏页对应的redo日志就没用了，然后就可以去做checkpoint了。</p>
<h4 id="2126-查看系统中的各种lsn值">21.2.6 查看系统中的各种LSN值</h4>
<p>我们可以使用<mark>SHOW ENGINE INNODB STATUS</mark>命令查看当前InnoDB存储引擎中的各种LSN值的情况，比如：</p>
<pre><code class="language-mysql">mysql&gt; SHOW ENGINE INNODB STATUS\G

(...省略前面的许多状态)
LOG
---
Log sequence number 124476971
Log flushed up to   124099769
Pages flushed up to 124052503
Last checkpoint at  124052494
0 pending log flushes, 0 pending chkp writes
24 log i/o's done, 2.00 log i/o's/second
----------------------
(...省略后边的许多状态)
</code></pre>
<ol>
<li>Log sequence number：代表系统中的lsn值，也就是当前系统已经写入的redo日志量，包括写入log buffer中的日志。</li>
<li>Log flushed up to：代表flushed_to_disk_lsn的值，也就是当前系统已经写入磁盘的redo日志量。</li>
<li>Pages flushed up to：代表flush链表中被最早修改的那个页面对应的oldest_modification属性值。</li>
<li>Last checkpoint at：当前系统的checkpoint_lsn值。</li>
</ol>
<h3 id="213-innodb_flush_log_at_trx_commit的用法">21.3 innodb_flush_log_at_trx_commit的用法</h3>
<p>为了保证事务的持久性，用户线程在事务提交时需要将该事务执行过程中产生的所有redo日志都刷新到磁盘上。如果对事务的持久性要求不是那么强烈的话，可以选择修改一个称为<mark>innodb_flush_log_at_trx_commit</mark>的系统变量的值，该变量有3个可选的值：</p>
<p>0：当该系统变量值为0时，表示在事务提交时不立即向磁盘中同步redo日志，这个任务是交给后台线程做的。<br>
  这样很明显会加快请求处理速度，但是如果事务提交后服务器挂了，后台线程没有及时将redo日志刷新到磁盘，那么该事务对页面的修改会丢失。<br>
1：当该系统变量值为1时，表示在事务提交时需要将redo日志同步到磁盘，可以保证事务的持久性。1也是innodb_flush_log_at_trx_commit的默认值。<br>
2：当该系统变量值为2时，表示在事务提交时需要将redo日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘。<br>
  这种情况下如果数据库挂了，操作系统没挂的话，事务的持久性还是可以保证的，但是操作系统也挂了的话，那就不能保证持久性了。</p>
<h3 id="214-崩溃恢复">21.4 崩溃恢复</h3>
<h4 id="2141-确定恢复的起点">21.4.1 确定恢复的起点</h4>
<p>checkpoint_lsn之前的redo日志都可以被覆盖，也就是说这些redo日志对应的脏页都已经被刷新到磁盘中了，既然它们已经被刷盘，我们就没必要恢复它们了。对于checkpoint_lsn之后的redo日志，它们对应的脏页可能没被刷盘，也可能被刷盘了，我们不能确定，所以需要从checkpoint_lsn开始读取redo日志来恢复页面。</p>
<p>当然，redo日志文件组的第一个文件的管理信息中有两个block都存储了checkpoint_lsn的信息，我们当然是要<mark>选取最近发生的那次checkpoint的信息</mark>。衡量checkpoint发生时间早晚的信息就是所谓的checkpoint_no，我们只要把checkpoint1和checkpoint2这两个block中的checkpoint_no值读出来比一下大小，哪个的checkpoint_no值更大，说明哪个block存储的就是最近的一次checkpoint信息。这样我们就能拿到最近发生的checkpoint对应的checkpoint_lsn值以及它在redo日志文件组中的偏移量checkpoint_offset。</p>
<h4 id="2142-确定恢复的终点">21.4.2 确定恢复的终点</h4>
<p>在写redo日志的时候都是顺序写的，写满了一个block之后会再往下一个block中写：<br>
<img src="https://q456qq520.github.io/post-images/1677492591192.png" alt="" loading="lazy"></p>
<p>普通block的log block header部分有一个称之为<mark>LOG_BLOCK_HDR_DATA_LEN</mark>的属性，该属性值记录了当前block里使用了多少字节的空间。对于被填满的block来说，该值永远为512。如果该属性的值不为512，那么就是它了，它就是此次奔溃恢复中需要扫描的最后一个block。</p>
<h4 id="2143-怎么恢复">21.4.3 怎么恢复</h4>
<p>假设现在的redo日志文件中有5条redo日志，如图：<br>
<img src="https://q456qq520.github.io/post-images/1677492672934.png" alt="" loading="lazy"></p>
<p>由于redo 0在checkpoint_lsn后边，恢复时可以不管它。我们现在可以按照redo日志的顺序依次扫描checkpoint_lsn之后的各条redo日志，按照日志中记载的内容将对应的页面恢复出来。这样没什么问题，不过InnoDB还是想了一些办法加快这个恢复的过程：</p>
<ul>
<li>
<p>使用哈希表<br>
根据redo日志的space ID和page number属性计算出散列值，把space ID和page number相同的redo日志放到哈希表的同一个槽里，如果有多个space ID和page number都相同的redo日志，那么它们之间使用链表连接起来，按照生成的先后顺序链接起来的，如下图<br>
<img src="https://q456qq520.github.io/post-images/1677492773396.png" alt="" loading="lazy"><br>
之后就可以遍历哈希表，因为对同一个页面进行修改的redo日志都放在了一个槽里，所以可以一次性将一个页面修复好（避免了很多读取页面的随机IO），这样可以加快恢复速度。另外需要注意一点的是，同一个页面的redo日志是按照生成时间顺序进行排序的，所以恢复的时候也是按照这个顺序进行恢复。</p>
</li>
<li>
<p>跳过已经刷新到磁盘的页面<br>
checkpoint_lsn之前的redo日志对应的脏页确定都已经刷到磁盘了，但是checkpoint_lsn之后的redo日志我们不能确定是否已经刷到磁盘，主要是因为在最近做的一次checkpoint后，可能后台线程又不断的从LRU链表和flush链表中将一些脏页刷出Buffer Pool。这些在checkpoint_lsn之后的redo日志，如果它们对应的脏页在奔溃发生时已经刷新到磁盘，那在恢复时也就没有必要根据redo日志的内容修改该页面了。<br>
在File Header里有一个称之为FIL_PAGE_LSN的属性，该属性记载了最近一次修改页面时对应的lsn值（其实就是页面控制块中的newest_modification值）。如果在做了某次checkpoint之后有脏页被刷新到磁盘中，那么该页对应的FIL_PAGE_LSN代表的lsn值肯定大于checkpoint_lsn的值，凡是符合这种情况的页面就不需要重复执行lsn值小于FIL_PAGE_LSN的redo日志了，所以更进一步提升了奔溃恢复的速度。</p>
</li>
</ul>
<h3 id="215-log_block_hdr_no是如何计算的">21.5 LOG_BLOCK_HDR_NO是如何计算的</h3>
<p>对于实际存储redo日志的普通的log block来说，在log block header处有一个称之为LOG_BLOCK_HDR_NO的属性，我们说这个属性代表一个唯一的标号。这个属性是初次使用该block时分配的，跟当时的系统lsn值有关。使用下面的公式计算该block的LOG_BLOCK_HDR_NO值：</p>
<blockquote>
<p>((lsn / 512) &amp; 0x3FFFFFFFUL) + 1</p>
</blockquote>
<p>0x3FFFFFFFUL对应的二进制数的前2位为0，后30位的值都为1。</p>
<p>不论lsn多大，((lsn / 512) &amp; 0x3FFFFFFFUL)的值肯定在0<sub>0x3FFFFFFFUL之间，再加1的话肯定在1</sub>0x40000000UL之间。而0x40000000UL这个值大家应该很熟悉，这个值就代表着1GB。也就是说系统最多能产生不重复的LOG_BLOCK_HDR_NO值只有1GB个。InnoDB规定redo日志文件组中包含的所有文件大小总和不得超过512GB，一个block大小是512字节，也就是说redo日志文件组中包含的block块最多为1GB个，所以有1GB个不重复的编号值也就够用了。</p>
<p>另外，LOG_BLOCK_HDR_NO值的第一个比特位比较特殊，称之为<mark>flush bit</mark>，如果该值为1，代表着本block是在某次将log buffer中的block刷新到磁盘的操作中的第一个被刷入的block。</p>
<p>链接:<a href="/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-wu">《从根儿上理解MySQL》读书笔记(五)</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elasticsearch]]></title>
        <id>https://q456qq520.github.io/post/elasticsearch/</id>
        <link href="https://q456qq520.github.io/post/elasticsearch/">
        </link>
        <updated>2023-02-20T08:50:59.000Z</updated>
        <summary type="html"><![CDATA[<p>··</p>
]]></summary>
        <content type="html"><![CDATA[<p>··</p>
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[面试题（二）]]></title>
        <id>https://q456qq520.github.io/post/mian-shi-ti-er/</id>
        <link href="https://q456qq520.github.io/post/mian-shi-ti-er/">
        </link>
        <updated>2023-02-12T04:23:14.000Z</updated>
        <content type="html"><![CDATA[<h2 id="zookeeper">Zookeeper</h2>
<h3 id="1-zk的使用场景">1. zk的使用场景</h3>
<ol>
<li>统一命名服务（Name Service）</li>
<li>配置管理</li>
<li>集群管理</li>
<li>共享锁<br>
实现方式也是需要获得锁的 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，然后调用 getChildren方法获取当前的目录节点列表中最小的目录节点是不是就是自己创建的目录节点，如果正是自己创建的，那么它就获得了这个锁，如果不是那么它就调用 exists(String path, boolean watch) 方法并监控 Zookeeper 上目录节点列表的变化，一直到自己创建的节点是列表中最小编号的目录节点，从而获得锁，释放锁很简单，只要删除前面它自己所创建的目录节点就行了。</li>
<li>队列管理</li>
<li>负载均衡</li>
<li>分布式通知/协调</li>
</ol>
<h3 id="2-zk节点类型有哪些有哪些角色-持久化临时leaderfollowobserver">2. zk节点类型有哪些，有哪些角色。持久化/临时，leader，follow，observer</h3>
<p><code>节点类型</code><br>
PERSISTENT：持久化节点<br>
PERSISTENT_SEQUENTIAL：持久化有序节点<br>
PHEMERAL：临时节点<br>
EPHEMERAL_SEQUENTIAL：临时有序节点<br>
容器节点：当没有子节点时，未来会被服务器删除（命令加 -c）<br>
TTL节点：超过TTL指定时间，被服务器删除</p>
<blockquote>
<p>注：临时节点下不能创建子节点，临时节点的声明周期不是永久，跟随客户端连接，客户端会话失效后，临时节点会自动删除。3.5新增了2种节点</p>
</blockquote>
<p><code>角色</code><br>
leader<br>
Leader服务器是整个zookeeper集群的核心，主要工作任务：<br>
1. 事务请求(增/删/改)的唯一调度和处理者，保证集群事务处理的顺序性<br>
2. 集群内部各服务器的调度者<br>
follower<br>
1. 处理客户端非事务请求、转发事务请求给leader服务器<br>
2. 参与事务请求Proposal的投票（需要半数以上服务器通知才能通知leader commit数据，leader发起的提案，需要follower投票）<br>
3. 参与leader选举的投票<br>
observer<br>
observer 是 zookeeper3.3 开始引入的一个全新的服务器角色，从字面来理解，该角色充当了观察者的角色。观察 zookeeper 集群中的最新状态变化并将这些状态变化同步到 observer 服务器上。Observer 的工作原理与follower 角色基本一致，而它和 follower 角色唯一的不同在于 observer 不参与任何形式的投票，包括事务请求Proposal的投票和leader选举的投票。简单来说，observer服务器只提供非事务请求服务，通常在于不影响集群事务处理能力的前提下提升集群非事务处理的能力。</p>
<h3 id="3zk的恢复模式是如何工作的-按顺序启动三个zk节点他们是如何选出leader的">3.zk的恢复模式，是如何工作的。按顺序启动三个zk节点，他们是如何选出leader的</h3>
<p>崩溃恢复模式，zk 使用单一主进程 leader 来处理客户端所有的事务请求，采用ZAB协议将服务器数状态以事务形式广播到所有Follower上。</p>
<p><code>恢复模式</code>：ZAB协议支持的崩溃恢复可以保证在Leader进程崩溃的时候可以重新选出Leader并且保证数据的完整性;<br>
什么时候会进入恢复模式呢？</p>
<ol>
<li>集群刚刚启动的时候。</li>
<li>leader 因为故障宕机了。</li>
<li>leader 失去了半数的机器支持。</li>
</ol>
<p>特殊情况崩溃的处理：<br>
已经被 leader 提交的 Proposal 确保最终被所有的 Follower 提交。确保只有在 leader 上被提出的 Proposal 会被遗弃。</p>
<p>这里的丢弃事务是如何让进行的呢？我们知道，每一个事务都是有一个 zxid进行标记的，这个zxid 是一个 64位的数字，低32位做为计数器，高32位作为 leadert 的epcho周期、重新选举出来的 leader 会在急集群中找到最大的日志的 zxid，然后提取出来 + 1 作为自己的 epcho 周期数，然后把后面的32位清零，开始计数。</p>
<p><code>选举流程</code><br>
Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。</p>
<ol>
<li>Zookeeper选主流程<br>
第一次启动（5台服务器顺序启动）<br>
（1）服务器1启动，发起选举。服务器1投自己一票，此时服务器1有一票，未超过半数以上票数，选举无法完成，服务器保持状态为LOOKING；<br>
（2）服务器2启动，再次发起选举。服务器1和服务器2分别半先投给自己1票，服务器1和服务器2会通信，此时服务器1发现服务器2的myid比自己大，因此会将自己的票给服务器2。此时服务器1票数为0，服务器票数为2。服务器2的票数没有达到半数以上，选举无法完成，服务器1和2状态为LOOKING；<br>
（3）服务器3启动，发起一次选举。最终服务1会有0票，服务器2会有0票，服务器3会有3票，此时服务器3的票数超过半数，因此服务器3当选为leader。服务器1和服务器2的状态变为FOLLOWING,服务器3的状态变为LEADING;<br>
(4)随后服务器4和服务器5启动，此时已经有leader了，不再进行选举了，服务器4和服务器5更改状态为FOLLOWING。</li>
</ol>
<p>非第一次启动<br>
当集群中的一台服务器无法和leader保持连接时，会进入leader选举，而此时，集群会有以下两种状态：<br>
集群中已经存在leader:针对这种情况，该节点试图去选择leader时，会被其他节点告知当前服务器的leader信息。因此，该服务器仅需要和leader重新建立连接并同步状态即可。<br>
集群中已经不存在leader:假设集群由5个节点组成，SID分别为1,2,3,4,5；ZXID分别为9,9,8,7,6，EPOCH均为1，且此时SID为3的服务器为leader。突然，当3和5服务器发生故障时，会触发leader选举。选举规则为：EPOCH大的直接胜出，如果EPOCH相同，则ZXID大的胜出，如果ZXID相同，则SID大的胜出。</p>
<p><code>Zookeeper集群&quot;脑裂&quot;问题处理</code><br>
脑裂 (Split-Brain) 就是比如当你的 cluster 里面有两个节点，它们都知道在这个 cluster 里需要选举出一个 master。那么当它们两个之间的通信完全没有问题的时候，就会达成共识，选出其中一个作为 master。但是如果它们之间的通信出了问题，那么两个结点都会觉得现在没有 master，所以每个都把自己选举成 master，于是 cluster 里面就会有两个 master。</p>
<p>假死：由于心跳超时（网络原因导致的）认为 leader 死了，但其实 leader 还存活着。<br>
脑裂：由于假死会发起新的 leader 选举，选举出一个新的 leader，但旧的 leader 网络又通了，导致出现了两个 leader ，有的客户端连接到老的 leader，而有的客户端则连接到新的 leader。</p>
<p>解决脑裂问题：</p>
<ol>
<li>Quorums (法定人数) 方式:即只有集群中超过半数节点投票才能选举出 Leader。这样的方式可以确保 leader 的唯一性,要么选出唯一的一个 leader,要么选举失败。</li>
<li>添加冗余的心跳线，例如双线条线，尽量减少“裂脑”发生机会。</li>
<li>启用磁盘锁。正在服务一方锁住共享磁盘，“裂脑&quot;发生时，让对方完全&quot;抢不走&quot;共享磁盘资源。但使用锁磁盘也会有一个不小的问题，如果占用共享盘的一方不主动&quot;解锁”，另一方就永远得不到共享磁盘。</li>
<li>设置仲裁机制。例如设置参考 IP（如网关 IP），当心跳线完全断开时，2个节点都各自 ping 一下 参考 IP，不通则表明断点就出在本端，不仅&quot;心跳&quot;、还兼对外&quot;服务&quot;的本端网络链路断了，即使启动（或继续）应用服务也没有用了，那就主动放弃竞争，让能够 ping 通参考 IP 的一端去起服务。更保险一些，ping 不通参考 IP 的一方干脆就自我重启，以彻底释放有可能还占用着的那些共享资源。</li>
</ol>
<h3 id="4zk的广播模式是如何工作的-一个写入请求是如何工作的">4.zk的广播模式，是如何工作的。一个写入请求是如何工作的</h3>
<p><code>消息广播模式</code>：在ZooKeeper中所有的事务请求都由一个主服务器也就是Leader来处理，其他服务器为Follower，Leader将客户端的事务请求转换为事务Proposal，并且将Proposal分发给集群中其他所有的Follower，然后Leader等待Follwer反馈，当有过半数（&gt;=N/2+1）的Follower反馈信息后，Leader将再次向集群内Follower广播Commit信息，Commit为将之前的Proposal提交。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1676252940259.jpeg" alt="" loading="lazy"></figure>
<p>一个客户端的写请求进来之后，leader 会为每个客户端的写请求包装成事务，并提供一个递增事务ID(zxid),保证每个消息的因果关系顺序。leader 会为该事务生成一个 Proposal,进行广播，leader 会为每一个 Follower服务器分配一个单独的FIFO 队列，然后把 Proposal 放到队列中，每一个 Follower 收到 该 Proposal 之后会把它持久到磁盘上，当完全写入之后，发一个 ACK 给leader，收到超过半数机器的ack之后，他自己把自己机器上 Proposal 提交一下，同时开始广播 commit,每一个 Follower 收到 commit 之后，完成各自的事务提交。</p>
<h2 id="kafka">Kafka</h2>
<h3 id="5mq的使用场景">5.mq的使用场景</h3>
<ol>
<li>异步通信：有些业务不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。</li>
<li>解耦：降低工程间的强依赖程度，针对异构系统进行适配。在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。通过消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口，当应用发生变化时，可以独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束</li>
<li>冗余：有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的&quot;插入-获取-删除&quot;范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。</li>
<li>扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。便于分布式扩容</li>
<li>过载保护：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量无法提取预知；如果以为了能处理这类瞬间峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃</li>
<li>可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。</li>
<li>顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。</li>
<li>缓冲：在任何重要的系统中，都会有需要不同的处理时间的元素。消息队列通过一个缓冲层来帮助任务最高效率的执行，该缓冲有助于控制和优化数据流经过系统的速度。以调节系统响应时间。</li>
<li>数据流处理：分布式系统产生的海量数据流，如：业务日志、监控数据、用户行为等，针对这些数据流进行实时或批量采集汇总，然后进行大数据分析是当前互联网的必备技术，通过消息队列完成此类数据收集是最好的选择</li>
</ol>
<h3 id="6kafka为什么这么快-批量压缩磁盘顺序写零拷贝">6.kafka为什么这么快。批量，压缩，磁盘顺序写，零拷贝</h3>
<p><code>顺序读写</code><br>
实际上不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高。磁盘的顺序读写是磁盘使用模式中最有规律的，并且操作系统也对这种模式做了大量优化，Kafka就是使用了磁盘顺序读写来提升的性能。Kafka的message是不断追加到本地磁盘文件末尾的，而不是随机的写入，这使得Kafka写入吞吐量得到了显著提升。</p>
<p>这种方法有一个缺陷—— 没有办法删除数据 ，所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个Topic都有一个offset用来表示读取到了第几条数据。<br>
如果不删除硬盘肯定会被撑满，所以Kafka提供了两种策略来删除数据。一是基于时间，二是基于partition文件大小。</p>
<p><code>Page Cache</code><br>
为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：</p>
<ul>
<li>避免Object消耗：如果是使用 Java 堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。</li>
<li>避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题</li>
</ul>
<p>相比于使用JVM或in-memory cache等数据结构，利用操作系统的Page Cache更加简单可靠。首先，操作系统层面的缓存利用率会更高，因为存储的都是紧凑的字节结构而不是独立的对象。其次，操作系统本身也对于Page Cache做了大量优化，提供了 write-behind、read-ahead以及flush等多种机制。再者，即使服务进程重启，系统缓存依然不会消失，避免了in-process cache重建缓存的过程。<br>
通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。</p>
<p><code>零拷贝</code><br>
基于传统的IO方式，底层实际上通过调用来实现。通过把数据从硬盘读取到内核缓冲区，再复制到用户缓冲区；然后再通过写入到socket缓冲区，最后写入网卡设备。整个过程发生了4次用户态和内核态的上下文切换和4次拷贝。<br>
<img src="https://q456qq520.github.io/post-images/1676270483249.jpeg" alt="" loading="lazy"><br>
零拷贝技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域，这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。<br>
那么对于零拷贝而言，<mark>并非真的是完全没有数据拷贝的过程，只不过是减少用户态和内核态的切换次数以及CPU拷贝的次数</mark>。</p>
<p>几种常见的零拷贝技术如下：</p>
<ol>
<li>mmap<br>
<img src="https://q456qq520.github.io/post-images/1676270475877.jpeg" alt="" loading="lazy"><br>
mmap() 就是在用户态直接引用文件句柄，也就是用户态和内核态共享内核态的数据缓冲区，此时数据不需要复制到用户态空间。当应用程序往 mmap 输出数据时，此时就直接输出到了内核态数据，如果此时输出设备是磁盘的话，会直接写盘（flush间隔是30秒）。</li>
<li>对于sendfile 而言，数据不需要在应用程序做业务处理，仅仅是从一个 DMA 设备传输到另一个 DMA设备。 此时数据只需要复制到内核态，用户态不需要复制数据，并且也不需要像 mmap 那样对内核态的数据的句柄（文件引用）。如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1676270518662.jpeg" alt="" loading="lazy"></li>
</ol>
<p><code>分区分段 + 索引</code><br>
Kafka的message是按topic分类存储的，topic中的数据又是按照一个一个的partition即分区存储到不同broker节点。每个partition对应了操作系统上的一个文件夹，partition实际上又是按照segment分段存储的。<br>
通过这种分区分段的设计，Kafka的message消息实际上是分布式存储在一个一个小的segment中的，每次文件操作也是直接操作的segment。为了进一步的查询优化，Kafka又默认为分段后的数据文件建立了索引文件，就是文件系统上的.index文件。这种分区分段+索引的设计，不仅提升了数据读取的效率，同时也提高了数据操作的并行度。</p>
<p><code>批量压缩</code><br>
在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO，对于需要在广域网上的数据中心之间发送消息的数据流水线尤其如此。进行数据压缩会消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑。</p>
<ul>
<li>如果每个消息都压缩，但是压缩率相对很低，所以Kafka使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩</li>
<li>Kafka允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩</li>
<li>Kafka支持多种压缩协议，包括Gzip和Snappy压缩协议<br>
Kafka速度的秘诀在于，它把所有的消息都变成一个批量的文件，并且进行合理的批量压缩，减少网络IO损耗，通过mmap提高I/O速度，写入数据的时候由于单个Partition是末尾添加所以速度最优；读取数据的时候配合sendfile直接暴力输出。</li>
</ul>
<h3 id="7kafka会丢消息吗哪些场景下会存在">7.kafka会丢消息吗？哪些场景下会存在？</h3>
<p>会，主要有下面三种场景：</p>
<ol>
<li>
<p><code>生产者丢失消息</code><br>
Kafka Producer 是异步发送消息的，如果你的 Producer 客户端使用了 producer.send(msg) 方法来发送消息，方法会立即返回，但此时并不能代表消息已经发送成功了。如果消息再发送的过程中发生了网络抖动，那么消息可能没有传递到 Broker，那么消息可能会丢失。如果发送的消息本身不符合，如大小超过了 Broker 的承受能力等。</p>
</li>
<li>
<p><code>服务端丢失消息</code><br>
Leader Broker 宕机了，触发选举过程，集群选举了一个落后 Leader 太多的 Broker 作为 Leader，那么落后的那些消息就会丢失了。<br>
Kafka 为了提升性能，使用页缓存机制，将消息写入页缓存而非直接持久化至磁盘，采用了异步批量刷盘机制，也就是说，按照一定的消息量和时间间隔去刷盘，刷盘的动作由操作系统来调度的，如果刷盘之前，Broker 宕机了，重启后在页缓存的这部分消息则会丢失。</p>
</li>
<li>
<p><code>消费者丢失消息</code><br>
消费者拉取了消息，并处理了消息，但处理消息异常了导致失败，并且提交了偏移量，消费者重启后，会从之前已提交的位移的下一个位置重新开始消费，消费失败的那些消息不会再次处理，即相当于消费者丢失了消息。<br>
消费者拉取了消息，并提交了消费位移，但是在消息处理结束之前突然发生了宕机等故障，消费者重启后，会从之前已提交的位移的下一个位置重新开始消费，之前未处理完成的消息不会再次处理，即相当于消费者丢失了消息。</p>
</li>
</ol>
<p><code>解决办法</code></p>
<ol>
<li>生产端不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。带有回调通知的 send 方法可以针对发送失败的消息进行重试处理。</li>
<li>生产端设置 acks = all。代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。</li>
<li>生产端设置 retries = 3，当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries &gt; 0 的 Producer 能够自动重试消息发送，避免消息丢失。消息太大，超过max.request.size参数配置的值则此方法不可行。</li>
<li>生产端设置 retry.backoff.ms = 300，合理估算重试的时间间隔，可以避免无效的频繁重试。</li>
<li>Broker 端设置 unclean.leader.election.enable = false。它控制的是哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false，即不允许这种情况的发生。</li>
<li>Broker 端设置 replication.factor &gt;= 3。其实这里想表述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余。</li>
<li>Broker 端设置 min.insync.replicas &gt; 1。这控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。</li>
<li>Broker 端确保 replication.factor &gt; min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1。</li>
<li>消费端确保消息消费完成再提交。最好把它设置成 enable.auto.commit = false，并采用手动提交位移的方式。</li>
<li>消费端没有重试机制不支持消息重试，也没有死信队列，因此使用 Kafka 做消息队列时，需要自己实现消息重试的功能。
<ul>
<li>创建一个 Topic 作为重试 Topic，用于接收等待重试的消息。</li>
<li>普通 Topic 消费者设置待重试消息的下一个重试 Topic。</li>
<li>从重试 Topic 获取待重试消息存储到 Redis 的 ZSet 中，并以下一次消费时间排序。</li>
<li>定时任务从 Redis 获取到达消费时间的消息，并把消息发送到对应的 Topic。</li>
<li>同一个消息重试次数过多则不再重试。</li>
</ul>
</li>
</ol>
<h3 id="8kafka会重复消费消息吗哪些场景下会存在">8.kafka会重复消费消息吗？哪些场景下会存在？</h3>
<p>会，主要有下面两种场景：<br>
<code>生产者重复消息</code><br>
原因：生产者发出一条消息，Broker 落盘以后因为网络等种种原因，发送端得到一个发送失败的响应或者网络中断，然后生产者收到一个可恢复的 Exception 重试消息导致消息重复。<br>
<code>消费者重复消息</code><br>
原因：数据消费完没有及时提交 offset 到 Broker，消息消费端在消费过程中挂掉没有及时提交 offset 到 Broker，另一个消费端启动拿之前记录的 offset 开始消费，由于 offset 的滞后性可能会导致新启动的客户端有少量重复消费。</p>
<p>Kafka 实际上通过两种机制来确保消息消费的精确一次：</p>
<ol>
<li>幂等性（Idempotence）<br>
保证在消息重发的时候，消费者不会重复处理。即使在消费者收到重复消息的时候，重复处理，也要保证最终结果的一致性。Kafka为了实现幂等性，在 0.11.0 版本之后，它在底层设计架构中引入了ProducerID和SequenceNumber。<br>
ProducerID：在每个新的 Producer 初始化时，会被分配一个唯一的 ProducerID，这个 ProducerID 对客户端使用者是不可见的。<br>
SequenceNumber：对于每个 ProducerID，Producer 发送数据的每个 Topic 和 Partition 都对应一个从 0 开始单调递增的 SequenceNumber 值。</li>
<li>事务（Transaction）<br>
幂等性不能实现多分区以及多会话上的消息无重复，而 Kafka 事务则可以弥补这个缺陷，Kafka 自 0.11 版本开始也提供了对事务的支持，目前主要是在 read committed 隔离级别上做事情。它能保证多条消息原子性地写入到目标分区，同时也能保证 Consumer 只能看到事务成功提交的消息。<br>
事务型 Producer 能够保证将消息原子性地写入到多个分区中。这批消息要么全部写入成功，要么全部失败。另外，事务型 Producer 也不惧进程的重启。Producer 重启回来后，Kafka 依然保证它们发送消息的精确一次处理。</li>
</ol>
<h3 id="9kafka的部署结构">9.kafka的部署结构</h3>
<h3 id="10kafka生产者发送消息模式同步-异步-发后既忘-失败重试">10.kafka生产者，发送消息模式：同步、异步、发后既忘。失败重试</h3>
<ol>
<li>Fire-and-forget(发后既忘)<br>
只发送消息，不关心消息是否发送成功。本质上也是一种异步发送的方式，消息先存储在缓冲区中，达到设定条件后批量发送。当然这是kafka吞吐量最高的一种方式,并配合参数acks=0，这样生产者不需要等待服务器的响应，以网络能支持的最大速度发送消息。但是也是消息最不可靠的一种方式，因为对于发送失败的消息没有做任何处理。</li>
<li>Synchronous send(同步)<br>
同步发送，send()方法会返回Futrue对象，通过调用Futrue对象的get()方法，等待直到结果返回，根据返回的结果可以判断是否发送成功。如果业务要求消息必须是按顺序发送的，那么可以使用同步的方式，并且只能在一个partation上，结合参数设置retries的值让发送失败时重试，设置<code>max_in_flight_requests_per_connection=1</code>，可以控制生产者在收到服务器晌应之前只能发送1个消息，在消息发送成功后立刻flush，从而控制消息顺序发送，但是消息过大则不适用。</li>
<li>Asynchronous send(异步)<br>
异步发送，在调用send()方法的时候指定一个callback函数，当broker接收到返回的时候，该callback函数会被触发执行。如果业务需要知道消息发送是否成功，并且对消息的顺序不关心，那么可以用异步+回调的方式来发送消息，配合参数retries=0，并将发送失败的消息记录到日志文件中；要使用callback函数，先要实现org.apache.kafka.clients.producer.Callback接口，该接口只有一个onCompletion方法。如果发送异常，onCompletion的参数Exception e会为非空。</li>
</ol>
<h3 id="11kafka生产者如何确定分区">11.kafka生产者，如何确定分区</h3>
<p>Kafka 的消息数据的组织方式分一下几个层次：</p>
<ol>
<li>Topic，可以理解为一个容器，用来存放同一主题的消息，这里的主题可以理解为各种不同的业务、部门、甚至是租户等。</li>
<li>Partition，也叫做分区，就是把同一个 Topic 中的数据，分成几部分，保存在不同的 Kafka Broker 里，这样可以提高消息吞吐量。每一个消息只能存在于一个分区中，不会重复保存。</li>
<li>Replica，也叫做副本，每一个分区可以有若干个副本，它们保存着同样的数据，保证分区的可用性。</li>
</ol>
<p>Topic 是在生产消息之前就设定好的，每个消息会固定发送到指定的 Topic；Replica 是分区数据的完整副本，只需要分区的 Leader 副本的数据变化，同步数据即可。而分区是比较灵活的，一个消息被发送到指定的 Topic 后，要进入哪个分区，需要根据一个分区策略来计算。</p>
<p><code>轮询策略</code><br>
默认的策略就是轮询策略。轮询策略就是把生产的消息，按照分区，进行顺序分配。比如一个 Topic 被分成了三个分区，那么，第一条消息进入分区0，第二条消息进入分区1，第三条消息进去分区2，第四条消息再进入到分区0，以此类推。</p>
<p><code>自定义策略</code><br>
在 Kafka 中，如果要自己定义分区策略，需要修改生产者中的 partitioner.class 参数，它的值是一个 Java 类的完整名称（包含包名），这个类需要实现 <code>org.apache.kafka.clients.producer.Partitioner</code>接口，这个接口中有 partition 和 close 方法，其中 partition 方法就是实现具体分区逻辑的地方</p>
<p><code>随机策略</code></p>
<pre><code class="language-java">List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);
return ThreadLocalRandom.current().nextInt(partitions.size());
</code></pre>
<p><code>根据消息 key 分区的策略</code><br>
在 Kafka 中，每一则消息可以定义一个 key，我们可以根据这个 key 进行分区逻辑的计算。<br>
根据 key 来计算分区，有一个好处，就是可以让 key 相同的消息进入到同一个分区。因为在 Kafka 中，同一个分区是可以保证顺序的，而多个分区之间是不能保证顺序的，这样既可以享受分区带来的高吞吐量，也可以保证消息顺序。</p>
<h3 id="12kafka生产者线程模型">12.kafka生产者，线程模型</h3>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1676274380989.png" alt="" loading="lazy"></figure>
<h3 id="13kafka消费者如何确定分区rebalance如何处理">13.kafka消费者，如何确定分区，rebalance如何处理？</h3>
<p>消费者是采用Pull拉取方式从broker的partition获取数据，pull模式可以根据消费者的消费能力来进行自己调整，不同的消费者性能不一样。如果broker没有数据的话，消费者可以配置timeout，进行阻塞等待一段时间后再返回。</p>
<p>我们知道一个topic有多个partition，一个消费者组里面就有多个消费者，那是怎么分配的呢？一个主题topic可以有多个消费者，因为里面有多个partition分区（leader分区），一个partition leader可以由一个消费组里的一个消费者来消费。</p>
<p>那么消费者从哪个分区来进行消费呢？</p>
<ol>
<li>策略一、round-robin （RoundRobinAssignor非默认策略）轮训，按照消费者组来进行轮训分配，同个消费者组监听不同的主题也是一样，是把所有的partition和所有的consumer都列出来，所以的话消费者组里面的订阅主题是一样的才可以，主题不一样的话会出现分配不均匀的问题。</li>
<li>策略二、range（RangeAssignor默认策略）范围，按照主题来进行分配，如果不平均分配的话，则第一个消费者会分配比较多的分区，一个消费者监听不同的主题也不影响</li>
</ol>
<p>什么是Rebalance操作？<br>
Kafka怎么均匀的分配某一个topic下所有的partition到各个消费者的呢，从而使得消息的消费速度达到了最快，这就是平衡。而rebalance（重平衡）其实就是重新进行partition的分配，从而使得partition的分配重新达到了平衡的状态。</p>
<p>什么时候会发生rebalance？</p>
<ul>
<li>订阅 Topic 的分区数发生变化。</li>
<li>订阅的 Topic 个数发生变化。</li>
<li>消费组内成员个数发生变化。例如有新的 consumer 实例加入该消费组或者离开组(主动离开或被认为离开)。</li>
</ul>
<p>Rebalance 发生后的执行过程<br>
1、有新的 Consumer 加入 Consumer Group<br>
2、从 Consumer Group 选出 leader<br>
3、leader 进行分区的分配</p>
<h3 id="14kafka消费者位移提交自动-手动-同步-异步">14.kafka消费者，位移提交，自动、手动、同步、异步</h3>
<p>消费者在消费了消息之后会把消费的offset 更新到以 名称为__consumer_offsets_的内置Topic中; 每个消费组都有维护一个当前消费组的offset。</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>描述</th>
<th>default</th>
</tr>
</thead>
<tbody>
<tr>
<td>enable.auto.commit</td>
<td>如果为true，消费者的offset将在后台周期性的提交</td>
<td>true</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果enable.auto.commit设置为true，则消费者偏移量自动提交给Kafka的频率（以毫秒为单位）</td>
<td>5000</td>
</tr>
</tbody>
</table>
<p><code>自动提交</code><br>
消费者端开启了自动提交之后,每隔auto.commit.interval.ms自动提交一次;</p>
<p><code>手动提交</code><br>
手动提交 offset 的方法有两种:分别是 <code>commitSync(同步提交)</code>和 <code>commitAsync(异步 提交)</code>。两者的相同点是，都会将本次 poll 的一批数据最高的偏移量提交;不同点是， commitSync 阻塞当前线程，一直到提交成功，并且会自动失败重试(由不可控因素导致， 也会出现提交失败);而commitAsync则没有失败重试机制，故有可能提交失败。</p>
<h3 id="15kafka-broker如何保证消息存储持久化">15.kafka broker，如何保证消息存储持久化</h3>
<h3 id="16kafka-broker存储结构logindextimeindex">16.kafka broker，存储结构，log，index，timeIndex</h3>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1676276835685.png" alt="" loading="lazy"></figure>
<ol>
<li>kafka 中消息是以主题 Topic 为基本单位进行归类的，这里的 Topic 是逻辑上的概念，实际上在磁盘存储是根据分区 Partition 存储的, 即每个 Topic 被分成多个 Partition，分区 Partition 的数量可以在主题 Topic 创建的时候进行指定。</li>
<li>Partition 分区主要是为了解决 Kafka 存储的水平扩展问题而设计的， 如果一个 Topic 的所有消息都只存储到一个 Kafka Broker上的话， 对于 Kafka 每秒写入几百万消息的高并发系统来说，这个 Broker 肯定会出现瓶颈， 故障时候不好进行恢复，所以 Kafka 将 Topic 的消息划分成多个 Partition， 然后均衡的分布到整个 Kafka Broker 集群中。</li>
<li>Partition 分区内每条消息都会被分配一个唯一的消息 id,即我们通常所说的 偏移量 Offset, 因此 kafka 只能保证每个分区内部有序性,并不能保证全局有序性。</li>
<li>然后每个 Partition 分区又被划分成了多个 <code>LogSegment</code>，这是为了防止 Log 日志过大，Kafka 又引入了日志分段(LogSegment)的概念，将 Log 切分为多个 LogSegement，相当于一个巨型文件被平均分割为一些相对较小的文件，这样也便于消息的查找、维护和清理。这样在做历史数据清理的时候，直接删除旧的 LogSegement 文件就可以了。</li>
<li>Log 日志在物理上只是以文件夹的形式存储，而每个 LogSegement 对应磁盘上的一个日志文件和两个索引文件，以及可能的其他文件(比如以&quot;.snapshot&quot;为后缀的快照索引文件等)</li>
</ol>
<p>举个例子，假设现在有一个名为“topic-order”的 Topic，该 Topic 中有4个 Partition，那么在实际物理存储上表现为“topic-order-0”、“topic-order-1”、“topic-order-2”、“topic-order-3” 这4个文件夹。首先向 Log 中写入消息是顺序写入的。但是只有最后一个 LogSegement 才能执行写入操作，之前的所有 LogSegement 都不能执行写入操作。我们将最后一个 LogSegement 称为&quot;activeSegement&quot;，即表示当前活跃的日志分段。随着消息的不断写入，当 activeSegement 满足一定的条件时，就需要创建新的 activeSegement，之后再追加的消息会写入新的 activeSegement。</p>
<p>为了更高效的进行消息检索，每个 LogSegment 中的日志文件(以“.log”为文件后缀)都有对应的几个索引文件：偏移量索引文件(以“.index”为文件后缀)、时间戳索引文件(以“.timeindex”为文件后缀)、快照索引文件 (以“.snapshot”为文件后缀)。其中每个 LogSegment 都有一个 Offset 来作为基准偏移量(baseOffset)，用来表示当前 LogSegment 中第一条消息的 Offset。偏移量是一个64位的 Long 长整型数，日志文件和这几个索引文件都是根据基准偏移量(baseOffset)命名的，名称固定为20位数字，没有达到的位数前面用0填充。比如第一个 LogSegment 的基准偏移量为0，对应的日志文件为00000000000000000000.log。</p>
<h3 id="17kafka-broker是如何保证不丢数据的isr副本管理高水位线">17.kafka broker，是如何保证不丢数据的，ISR副本管理，高水位线</h3>
<p><code>什么是ISR</code><br>
ISR，全称 in-sync replicas，是一组动态维护的同步副本集合，每个topic分区都有自己的ISR列表，ISR中的所有副本都与leader保持同步状态(也包括leader本身)，只有ISR中的副本才有资格被选为新的leader，<br>
Producer发送消息时，消息只有被全部写到了ISR中，才会被视为已提交状态，若分区ISR中有N个副本，那么该分区ISR最多可以忍受 N-1 个副本崩溃而不丢失消息。</p>
<p><code>什么是高水位？</code><br>
在kafka中水位用于表示消息在分区中的位移或位置，高水位用于表示已提交的消息的分界线的位置，在高水位这个位置之前的消息都是已提交的，在高水位这个位置之后的消息都是未提交的。所以，高水位可以看作是已提交消息和未提交消息之间的分割线，如果把分区比喻为一个竖起来的水容器的话，这个表示就更明显了，在高水位之下的消息都是已提交的，在高水位之上的消息都是未提交的。<br>
<img src="https://q456qq520.github.io/post-images/1676281692278.png" alt="" loading="lazy"><br>
在 Kafka 中，高水位的作用主要有 2 个。</p>
<ol>
<li>定义消息可见性，即用来标识分区下的哪些消息是可以被消费者消费的。</li>
<li>帮助 Kafka 完成副本同步。</li>
</ol>
<blockquote>
<p>位移值等于高水位的消息也属于未提交消息。也就是说，高水位上的消息是不能被消费者消费的。</p>
</blockquote>
<p>每个副本对象都保存了一组高水位值和 LEO 值，但实际上，在 Leader 副本所在的 Broker 上，还保存了其他 Follower 副本的 LEO 值。</p>
<p><code>副本同步过程</code><br>
假设某Kafka集群中(broker1、2、3)仅有一个Topic，该Topic只有一个分区，该分区有3个副本，ISR中也是这3个副本，该Topic中目前没有任何数据，因此3个副本中的LEO和HW都是0。<br>
此时某Producer(Producer的acks参数设置成了-1)向broker1中的leader副本发送了一条消息，接下的流程如下：</p>
<ol>
<li>broker1上的leader副本接收到消息，将自己的LEO更新为1</li>
<li>broker2和3上的follower副本各自发送请求给broker1</li>
<li>broker1分别将消息推送给broker2、3上的副本</li>
<li>follower副本收到消息后，进行写入然后将自己的LEO也更新为1</li>
<li>leader副本收到其他follower副本的数据请求响应(response)后，更新HW值为1，此时位移为0的消息可以被consumer消费</li>
</ol>
<p><code>副本同步机制的危害</code></p>
<ol>
<li>数据丢失</li>
<li>数据不一致</li>
</ol>
<p>解决办法：<br>
kafka引入了Leader Epoch，Leader Epoch是一对值：（epoch,offset），epoch：代表当前 leader 的版本号，从0开始，当 Leader 变更过一次时，我们的 epoch 就会 +1，offset：该 epoch 版本的 Leader 写入第一条消息的位移</p>
<h3 id="18kafka-broker优先副本有什么作用是如何使用的">18.kafka broker，优先副本有什么作用？是如何使用的</h3>
<p>所谓的优先副本是指在AR集合列表中的第一个副本。理想情况下，优先副本就是该分区的leader 副本，所以也可以称之为 preferred leader。Kafka 要确保所有主题的优先副本在 Kafka 集群中均匀分布，这样就保证了所有分区的 leader 均衡分布。以此来促进集群的负载均衡，这一行为也可以称为“分区平衡”。</p>
<p><code>优先副本选举</code></p>
<h2 id="rocketmq">RocketMq</h2>
<h3 id="19rocketmq为什么这么快-磁盘顺序写零拷贝">19.rocketMq为什么这么快。磁盘顺序写，零拷贝</h3>
<ol>
<li>顺序读写<br>
对磁盘读写时，如果是顺序读写，那么磁头几乎不用换道，或者换道的时间很短。读写效率会提高很多。（rocketmq 写是顺序写，读并不是，但是它提高的读机制使得读类似顺序读）<br>
rocketmq 将消息写入CommitLog 文件夹中的mappedFile文件（这个文件超过1G后会新建一个）时，是按照顺序写入的。不论消息属于哪个 Topic 的哪个 Queue 。都会按照顺序依次存储到CommitLog 文件夹中的mappedFile文件。</li>
<li>零拷贝-mmap技术<br>
mmap将一个文件或者其它对象映射进内存。mmap系统调用使得进程之间通过映射同一个普通文件实现共享内存。普通文件被映射到进程地址空间后，进程可以像访问普通内存一样对文件进行访问，不必再调用read()，write（）等操作。因为已经将文件映射到内存，所以就减少了一次cpu拷贝</li>
<li>预读取机制：<br>
consumequeue中的数据是顺序存放的，还引入了PageCache的预读取机制，使得对consumequeue文件的读取几乎接近于内存读取，即使在有消息堆积情况下也不会影响性能。若用户要读取数据，其首先会从PageCache中读取，若没有命中，则OS在从物理磁盘上加载该数据到PageCache的同时，也会顺序对其相邻数据块中的数据进行预读取。</li>
<li>文件预分配：<br>
CommitLog 的大小默认是1G，当超过大小限制的时候需要准备新的文件，而 RocketMQ 就起了一个后台线程 AllocateMappedFileService，不断的处理 AllocateRequest，AllocateRequest其实就是预分配的请求，会提前准备好下一个文件的分配，防止在消息写入的过程中分配文件，产生抖动。</li>
</ol>
<h3 id="20rocketmq会丢消息吗哪些场景下会存在">20.rocketMq会丢消息吗？哪些场景下会存在？</h3>
<figure data-type="image" tabindex="4"><img src="https://q456qq520.github.io/post-images/1676339415188.png" alt="" loading="lazy"></figure>
<p>如何保证消息不丢失：<br>
<img src="https://q456qq520.github.io/post-images/1676339515168.png" alt="" loading="lazy"></p>
<h3 id="21rocketmq会重复消费消息吗哪些场景下会存在">21.rocketMq会重复消费消息吗？哪些场景下会存在？</h3>
<p>比如生产者发送消息的时候使用了重试机制，发送消息后由于网络原因没有收到MQ的响应信息，报了个超时异常，然后又去重新发送了一次消息。但其实MQ已经接到了消息，并返回了响应，只是因为网络原因超时了。这种情况下，一条消息就会被发送两次。</p>
<p>在消费者处理了一条消息后会返回一个offset给MQ，证明这条消息被处理过了。但是，假如这条消息已经处理过了，在返回offset给MQ的时候服务宕机了，MQ就没有接收到这条offset，那么服务重启后会再次消费这条消息。</p>
<p>如果说MQ解决不了数据重复消费的问题，那么现在可以转化成 At least once + 幂等性 = Exactly once 这样就可以保证重复消费了。主要有下列三种方法</p>
<ol>
<li>数据库的唯一约束实现幂等</li>
<li>为更新的数据设置前置条件</li>
<li>记录并检查操作<br>
在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。</li>
</ol>
<h3 id="22rocketmq的部署结构name-servce之间部通讯">22.rocketMq的部署结构，Name Servce之间部通讯</h3>
<figure data-type="image" tabindex="5"><img src="https://q456qq520.github.io/post-images/1676341349908.png" alt="" loading="lazy"></figure>
<h3 id="23rocketmq生产者发送消息模式同步-异步-单向">23.rocketMq生产者，发送消息模式：同步、异步、单向。</h3>
<ol>
<li>同步发送<br>
Producer 向 broker 发送消息，阻塞当前线程等待 broker 响应 发送结果。</li>
<li>异步发送<br>
Producer 首先构建一个向 broker 发送消息的任务，把该任务提交给线程池，等执行完该任务时，回调用户自定义的回调函数，执行处理结果。</li>
<li>Oneway 发送<br>
Oneway 方式只负责发送请求，不等待应答，Producer 只负责把请求发出去，而不处理响应结果。</li>
<li>延迟发送：指定延迟的时间，在延迟时间到达之后再进行消息的发送。</li>
<li>批量发送：对于同类型、同特征的消息，可以聚合进行批量发送，减少MQ的连接发送次数，能够显著提升性能。</li>
</ol>
<h3 id="24rocketmq消费者如何确定分区rebalance如何处理">24.	rocketMq消费者，如何确定分区，rebalance如何处理？</h3>
<p><code>Rebalance(再均衡)</code>机制指的是：将一个Topic下的多个队列(或称之为分区)，在同一个消费者组(consumer group)下的多个消费者实例(consumer instance)之间进行重新分配。</p>
<p>Rebalance限制：<br>
由于一个队列最多分配给一个消费者，因此当某个消费者组下的消费者实例数量大于队列的数量时，多余的消费者实例将分配不到任何队列。</p>
<p>Rebalance除了以上限制，更加严重的是，在发生Rebalance时，存在着一些危害，如下所述：<br>
消费暂停：考虑在只有Consumer 1的情况下，其负责消费所有5个队列；在新增Consumer 2，触发Rebalance时，需要分配2个队列给其消费。那么Consumer 1就需要停止这2个队列的消费，等到这两个队列分配给Consumer 2后，这两个队列才能继续被消费。<br>
重复消费：Consumer 2 在消费分配给自己的2个队列时，必须接着从Consumer 1之前已经消费到的offset继续开始消费。然而默认情况下，offset是异步提交的，如consumer 1当前消费到offset为10，但是异步提交给broker的offset为8；那么如果consumer 2从8的offset开始消费，那么就会有2条消息重复。也就是说，Consumer 2 并不会等待Consumer1提交完offset后，再进行Rebalance，因此提交间隔越长，可能造成的重复消费就越多。<br>
消费突刺：由于rebalance可能导致重复消费，如果需要重复消费的消息过多；或者因为rebalance暂停时间过长，导致积压了部分消息。那么都有可能导致在rebalance结束之后瞬间可能需要消费很多消息。</p>
<p>具体步骤为：</p>
<ol>
<li>消费端会通过RebalanceService线程，10秒钟做一次基于topic下的所有队列负载</li>
<li>消费端遍历自己的所有topic，依次调rebalanceByTopic</li>
<li>根据topic获取此topic下的所有queue</li>
<li>选择一台broker获取基于group的所有消费端（有心跳向所有broker注册客户端信息）</li>
<li>选择队列分配策略实例AllocateMessageQueueStrategy执行分配算法</li>
</ol>
<p>什么时候触发负载均衡:</p>
<ol>
<li>消费者启动之后</li>
<li>消费者数量发生变更</li>
<li>每10秒会触发检查一次rebalance</li>
</ol>
<p>分配算法,RocketMQ提供了6中分区的分配算法:</p>
<ol>
<li>AllocateMessageQueueAveragely ：平均分配算法（默认）</li>
<li>AllocateMessageQueueAveragelyByCircle：环状分配消息队列</li>
<li>AllocateMessageQueueByConfig：按照配置来分配队列： 根据用户指定的配置来进行负载</li>
<li>AllocateMessageQueueByMachineRoom：按照指定机房来配置队列</li>
<li>AllocateMachineRoomNearby：按照就近机房来配置队列：</li>
<li>AllocateMessageQueueConsistentHash：一致性hash，根据消费者的cid进行</li>
</ol>
<h3 id="25rocketmq消费者tag是如何实现的">25.	rocketMq消费者，tag是如何实现的</h3>
<p>RocketMQ消息中间件相比于其他消息中间件提供了更细粒度的消息过滤,相比于Topic做业务维度的区分,Tag，即消息标签，用于对某个Topic下的消息进行进一步分类。消息队列RocketMQ版的生产者在发送消息时，指定消息的Tag，消费者需根据已经指定的Tag来进行订阅。</p>
<p>tag可以理解为topic的子类型,具有某一类型细分属性的集合,sql过滤模式是使用表达式实现通过消息内容的值进行过滤。</p>
<ol>
<li>消息生产者发送带tag的消息,先存储到commitlog,然后定时分发到topic对应的consumerQueue,消息对应的entry有8位存储tag的hashcode值。</li>
<li>消费端启动时将订阅关系通过心跳方式发送到broker,broker存储到ConsumerFilterManager中。</li>
<li>不论是push还是pull模式,本质上都是consumer去broker拉取消息,只不过对于push模式来说,通过pull将消息拉取到本地队列,并触发本地消费逻辑。</li>
<li>消息过滤逻辑是在broker实现,从consumerQueue拉取消息的时候,触发过滤逻辑,将符合条件的tag消息拉到本地消费。</li>
</ol>
<p><img src="https://q456qq520.github.io/post-images/1676344246694.png" alt="" loading="lazy">、</p>
<h3 id="26rocketmq消费者顺序消息消费失败如何处理">26.rocketMq消费者，顺序消息消费失败如何处理？</h3>
<p>当一条消息消费失败，RocketMQ就会自动进行消息重试。而如果消息超过最大重试次数，RocketMQ就会认为这个消息有问题。但是此时，RocketMQ不会立刻将这个有问题的消息丢弃，而会将其发送到这个消费者组对应的一种特殊队列：死信队列。</p>
<p>顺序消费通过客户端参数DefaultMQPushConsumer.maxReconsumeTimes设置最大重试次数，超过最大重试次数，消息将被转移到死信队列，范围是-1 – 16之间。<br>
maxReconsumeTimes默认值为-1，对于顺序消费模式来说 -1就代表着Integer.MAX_VALUE，表示无限次本地立即重试消费。这里的重试不再会将消息发往broker重试队列，只在在本地重试。<br>
顺序消费的重试由于不再需要broker控制，那么重试的间隔时间也是通过本地参数控制的，可通过MessageListenerOrderly#consumeMessage方法的ConsumeOrderlyContext参数指定重试策略，通过配置ConsumeOrderlyContext.suspendCurrentQueueTimeMillis属性指定间隔时间，参数取值范围10～30000ms，默认值-1，表示1000ms，即1秒重试一次。</p>
<p>业务方可以进行下面的操作处理失败消息：</p>
<ol>
<li>增加重试次数：如果重试次数设置较少，则增加重试次数以保证消息被成功消费。</li>
<li>检查消息内容：确保消息内容符合消费者的要求，避免消息因格式不正确导致消费失败。</li>
<li>检查消费者代码：检查消费者代码，确保代码实现了正确的消息处理流程。</li>
<li>消息补偿：如果多次重试仍然失败，考虑使用消息补偿机制以确保消息的最终一致性。</li>
</ol>
<h3 id="27rocketmq消费者非顺序消息消费失败如何处理重试队列retryconsumergroupconsumergroup">27.rocketMq消费者，非顺序消息消费失败如何处理？重试队列，%RETRY%consumerGroup@consumerGroup</h3>
<p>由于Consumer端逻辑出现了异常，导致没有返回SUCCESS状态，那么Broker就会在一段时间后尝试重试。</p>
<p>RocketMQ会为每个消费组都设置一个Topic名称为“%RETRY%+consumerGroup”的重试队列（这里需要注意的是，这个Topic的重试队列是针对消费组，而不是针对每个Topic设置的），用于暂时保存因为各种异常而导致Consumer端无法消费的消息，每个Consumer实例在启动的时候就默认订阅了该消费组的重试队列Topic。</p>
<p>考虑到异常恢复起来需要一些时间，会为重试队列设置多个重试级别，每个重试级别都有与之对应的重新投递延时，重试次数越多投递延时就越大（实际上就是配置的延时队列的级别level）。RocketMQ对于重试消息的处理是先保存至Topic名称为“SCHEDULE_TOPIC_XXXX”的延迟队列中，后台定时任务按照对应的时间进行Delay后重新保存至“%RETRY%+consumerGroup”的重试队列中。</p>
<p>对于非顺序消息，消费失败默认重试16次，延迟等级为3~18。(messageDelayLevel = &quot;1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h&quot;)</p>
<h3 id="28rocketmq-broker延迟队列如何实现schedule_topic_xxxx">28.rocketMq broker，延迟队列如何实现，SCHEDULE_TOPIC_XXXX</h3>
<p>Broker收到延时消息了，会先发送到主题（SCHEDULE_TOPIC_XXXX）的相应时间段的Message Queue中，然后通过一个定时任务轮询这些队列，到期后，把消息投递到目标Topic的队列中，然后消费者就可以正常消费这些消息。</p>
<h3 id="29rocketmq-broker事务消息是如何处理">29.	rocketMq broker，事务消息是如何处理？</h3>
<p>RocketMQ针对事务消息扩展了两个相关的概念：</p>
<ol>
<li>半消息<br>
半消息（Half Message）是一种特殊的消息类型，处于这个状态的消息暂时不能被Consumer消费。<br>
当一条事务消息被成功投递到Broker上，但Broker没有收到Producer的二次确认时，该事务消息就处于暂时不可消费的状态，这种消息就是半消息。</li>
<li>消息状态回查<br>
由于网络抖动、系统宕机等等原因，可能导致Producer向Broker发送的二次确认信息没有送达。如果Broker检测到某条事务消息长时间处于半消息状态，则会主动向Producer端发起回查操作，查询该事务消息在Producer端的事务状态。这个机制主要是用来解决分布式事务中的超时问题。</li>
</ol>
<p>RocketMQ事务消息流程图及执行步骤如下：<br>
<img src="https://q456qq520.github.io/post-images/1676345749327.png" alt="" loading="lazy"></p>
<ol>
<li>Producer向Broker端发送半消息</li>
<li>Broker发送ACK确认，表示半消息发送成功</li>
<li>Producer执行本地事务</li>
<li>本地事务完毕，根据事务的状态，Producer向Broker发送二次确认消息，确认该半消息的Commit或Rollback状态。Broker收到二次确认消息之后：如果是Commit状态，则直接将消息发送到Consumer端执行消费逻辑；如果是Rollback状态，则会直接将其标记为失败，不会发送给Consumer</li>
<li>针对超时情况，Broker主动向Producer发起消息回查</li>
<li>Producer处理回查消息，返回对应的本地事务执行结果</li>
<li>Broker针对消息回查的结果，执行【步骤4】的操作</li>
</ol>
<h3 id="30rocketmq-broker存储结构commitlogconsumequeueindex">30.rocketMq broker，存储结构，commitLog，consumequeue，index</h3>
<figure data-type="image" tabindex="6"><img src="https://q456qq520.github.io/post-images/1676345918124.png" alt="" loading="lazy"></figure>
<ol>
<li>Commitlog文件<br>
commitLog文件的最大的一个特点就是消息的顺序写入，随机读写，关于commitLog的文件的落盘有两种，一种是同步刷盘，一种是异步刷盘，可通过 flushDiskType 进行配置。在写入commitLog的时候内部会有一个mappedFile内存映射文件，消息是先写入到这个内存映射文件中，然后根据刷盘策略写到硬盘中。</li>
<li>consumerQueue文件<br>
每一个topic有多个queue，每个queue放着不同的消息。而每个topic下的queue队列都会对应一个Consumerqueue文件。消费者可以通过Consumerqueue来确定自己的消费进度，获取消息在commitLog文件中的具体的offset和大小。consumequeue存放在store文件里面，里面的consumequeue文件里面按照topic排放，然后每个topic默认4个队列，里面存放的consumequeue文件。ConsumeQueue中并不需要存储消息的内容，而存储的是消息在CommitLog中的offset。也就是说ConsumeQueue其实是CommitLog的一个索引文件。</li>
<li>indexFile文件<br>
RocketMQ还支持通过MessageID或者MessageKey来查询消息，使用ID查询时，因为ID就是用broker+offset生成的(这里msgId指的是服务端的)，所以很容易就找到对应的commitLog文件来读取消息。对于用MessageKey来查询消息，MessageStore通过构建一个index来提高读取速度。indexfile文件存储在store目录下的index文件里面，里面存放的是消息的hashcode和index内容，文件由一个文件头组成：长40字节。500w个hashslot，每个4字节。2000w个index条目，每个20字节。</li>
</ol>
<h3 id="31rocketmq-broker如何保证消息存储持久化集群模式同步双写异步刷盘">31.	rocketMq broker，如何保证消息存储持久化，集群模式，同步双写（异步刷盘）</h3>
<p>当消息投递到broker之后，会先存到page cache，然后根据broker设置的刷盘策略是否立即刷盘，也就是如果刷盘策略为异步，broker并不会等待消息落盘就会返回producer成功，也就是说当broker所在的服务器突然宕机，则会丢失部分页的消息。即使broker设置了同步刷盘，如果主broker磁盘损坏，也是会导致消息丢失。 因此可以给broker指定slave，同时设置master为SYNC_MASTER，然后将slave设置为同步刷盘策略。</p>
<p>此模式下，producer每发送一条消息，都会等消息投递到master和slave都落盘成功了，broker才会当作消息投递成功，保证休息不丢失。</p>
<h3 id="32kafka和rocketmq有啥区别">32.kafka和rocketmq有啥区别？</h3>
<p><code>相同之处</code><br>
两者底层原理有很多相似之处，RocketMQ借鉴了Kafka的设计。<br>
两者均利用了操作系统Page Cache的机制，同时尽可能通过顺序io降低读写的随机性，将读写集中在很小的范围内，减少缺页中断，进而减少了对磁盘的访问，提高了性能。</p>
<p><code>不同之处</code></p>
<ol>
<li>存储形式<br>
Kafka采用partition，每个topic的每个partition对应一个文件。顺序写入，定时刷盘。但一旦单个broker的partition过多，则顺序写将退化为随机写，Page Cache脏页过多，频繁触发缺页中断，性能大幅下降。<br>
RocketMQ采用CommitLog+ConsumeQueue，单个broker所有topic在CommitLog中顺序写，Page Cache只需保持最新的页面即可。同时每个topic下的每个queue都有一个对应的ConsumeQueue文件作为索引。ConsumeQueue占用Page Cache极少，刷盘影响较小。</li>
<li>存储可靠性<br>
RocketMQ支持异步刷盘，同步刷盘，同步Replication，异步Replication。<br>
Kafka使用异步刷盘，异步Replication。</li>
<li>顺序消息<br>
Kafka和RocketMQ都仅支持单topic分区有序。RocketMQ官方虽宣称支持严格有序，但方式为使用单个分区。</li>
<li>延时消息<br>
RocketMQ支持固定延时等级的延时消息，等级可配置。<br>
kfaka不支持延时消息。</li>
<li>消息重复<br>
RocketMQ仅支持At Least Once。<br>
Kafka支持At Least Once、Exactly Once。</li>
<li>消息过滤<br>
RocketMQ执行过滤是在Broker端，支持tag过滤及自定义过滤逻辑。<br>
Kafka不支持Broker端的消息过滤，需要在消费端自定义实现。</li>
<li>消息失败重试<br>
RocketMQ支持定时重试，每次重试间隔逐渐增加。<br>
Kafka不支持重试。</li>
<li>DLQ（dead letter queue）<br>
RocketMQ通过DLQ来记录所有消费失败的消息。<br>
Kafka无DLQ。Spring等第三方工具有实现，方式为将失败消息写入一个专门的topic。</li>
<li>回溯消费<br>
RocketMQ支持按照时间回溯消费，实现原理与Kafka相同。<br>
Kafka需要先根据时间戳找到offset，然后从offset开始消费。</li>
<li>事务<br>
RocketMQ支持事务消息，采用二阶段提交+broker定时回查。但也只能保证生产者与broker的一致性，broker与消费者之间只能单向重试。即保证的是最终一致性。<br>
Kafka从0.11版本开始支持事务消息，除支持最终一致性外，还实现了消息Exactly Once语义（单个partition）。</li>
<li>服务发现<br>
RocketMQ自己实现了namesrv。<br>
Kafka使用ZooKeeper。</li>
<li>高可用<br>
RocketMQ在高可用设计上粒度只控制在Broker。其保证高可用是通过master-slave主从复制来解决的。<br>
Kafka控制高可用的粒度是放在分区上。每个topic的leader分区和replica分区都可以在所有broker上负载均衡的存储。</li>
</ol>
<h3 id="33rocketmq发现丢消息了怎么办">33.rocketmq发现丢消息了怎么办？</h3>
<ol>
<li>生产者（Producer） 通过网络发送消息给 Broker，当 Broker 收到之后，将会返回确认响应信息给 Producer。所以生产者只要接收到返回的确认响应，就代表消息在生产阶段未丢失。</li>
<li>Broker 端不丢消息，保证消息的可靠性，我们需要将消息保存机制修改为同步刷盘方式，即消息存储磁盘成功，才会返回响应。</li>
<li>，Broker 通常采用一主（master）多从（slave）部署方式。为了保证消息不丢失，消息还需要复制到 slave 节点。采用同步的复制方式，master 节点将会同步等待 slave 节点复制完成，才会返回确认响应。</li>
<li>Broker 未收到消费确认响应或收到其他状态，消费者下次还会再次拉取到该条消息，进行重试。这样的方式有效避免了消费者消费过程发生异常，或者消息在网络传输中丢失的情况。</li>
</ol>
<h2 id="redis">Redis</h2>
<h3 id="34redisredis的使用场景redis为什么这么快">34.[redis]Redis的使用场景，redis为什么这么快？</h3>
<ol>
<li>基于内存实现<br>
Redis是基于内存存储实现的数据库，相对于数据存在磁盘的数据库，就省去磁盘磁盘I/O的消耗。</li>
<li>高效的数据结构</li>
<li>合理的线程模型<br>
Redis是单线程的，其实是指Redis的网络IO和键值对读写是由一个线程来完成的。但Redis的其他功能，比如持久化、异步删除、集群数据同步等等，实际是由额外的线程执行的。</li>
<li>I/O 多路复用<br>
I/O ：网络 I/O；多路 ：多个网络连接；复用：复用同一个线程。IO多路复用其实就是一种同步IO模型，它实现了一个线程可以监视多个文件句柄；一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；而没有文件句柄就绪时,就会阻塞应用程序，交出cpu。<br>
多路I/O复用技术可以让单个线程高效的处理多个连接请求，而Redis使用用epoll作为I/O多路复用技术的实现。并且Redis自身的事件处理模型将epoll中的连接、读写、关闭都转换为事件，不在网络I/O上浪费过多的时间。</li>
<li>虚拟内存机制</li>
</ol>
<h3 id="35rediskeys-是如何工作的scan是如何工作的">35.[redis]keys * 是如何工作的，scan是如何工作的</h3>
<p><code>keys *</code><br>
keys命令的原理就是扫描整个redis里面所有的db的key数据，然后根据我们的通配的字符串进行模糊查找出来。更为致命的是，这个命令会阻塞redis多路复用的io主线程，如果这个线程阻塞，在此执行之间其他的发送向redis服务端的命令，都会阻塞，从而引发一系列级联反应，导致瞬间响应卡顿，从而引发超时等问题，所以应该在生产环境禁止用使用keys和类似的命令smembers，这种时间复杂度为O（N），且会阻塞主线程的命令，是非常危险的。</p>
<p><code>scan</code><br>
Redis使用了Hash表作为底层实现，原因不外乎高效且实现简单。Redis底层key的存储结构就是类似于HashMap那样数组+链表的结构。其中第一维的数组大小为2n(n&gt;=0)。每次扩容数组长度扩大一倍。<br>
scan命令就是对这个一维数组进行遍历。每次返回的游标值也都是这个数组的索引。limit参数表示遍历多少个数组的元素，将这些元素下挂接的符合条件的结果都返回。因为每个元素下挂接的链表大小不同，所以每次返回的结果数量也就不同。</p>
<p>scan命令的时间复杂度虽然也是O(N)，但它是分次进行的，不会阻塞线程。<br>
scan命令提供了limit参数，可以控制每次返回结果的最大条数。</p>
<h3 id="37redisredis46-线程模型文件事件处理器">37.[redis]Redis4/6 线程模型（文件事件处理器）</h3>
<p>redis6.0之前线程模型：<br>
<img src="https://q456qq520.github.io/post-images/1676541394703.png" alt="" loading="lazy"></p>
<p>由于 Redis 是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个 Socket 队列中，当 socket 可读则交给单线程事件分发器逐个被执行。</p>
<p>redis支持多线程主要就是两个原因：</p>
<ol>
<li>可以充分利用服务器 CPU 资源，目前主线程只能利用一个核</li>
<li>多线程任务可以分摊 Redis 同步 IO 读写负荷</li>
</ol>
<p>Redis6.0的多线程默认是禁用的，只使用主线程。如需开启需要修改redis.conf配置文件：<code>io-threads-do-reads yes</code><br>
开启多线程后，还需要设置线程数，否则是不生效的。同样修改redis.conf配置文件。关于线程数的设置，官方有一个建议：4 核的机器建议设置为 2 或 3 个线程，8核的建议设置为 6 个线程，线程数一定要小于机器核数。线程数并不是越大越好，官方认为超过了 8 个基本就没什么意义了。</p>
<p>Redis6.0多线程的实现机制？流程如下：<br>
主线程获取 socket 放入等待列表<br>
将 socket 分配给各个 IO 线程（并不会等列表满）<br>
主线程阻塞等待 IO 线程（多线程）读取 socket 完毕<br>
主线程执行命令 - 单线程（如果命令没有接收完毕，会等 IO 下次继续）<br>
主线程阻塞等待 IO 线程（多线程）将数据回写 socket 完毕（一次没写完，会等下次再写）<br>
解除绑定，清空等待队列</p>
<p><strong>需要注意的是，Redis 多 IO 线程模型只用来处理网络读写请求，对于 Redis 的读写命令，依然是单线程处理。</strong></p>
<figure data-type="image" tabindex="7"><img src="https://q456qq520.github.io/post-images/1676541918506.png" alt="" loading="lazy"></figure>
<h3 id="38redis数据结构string-list-hash-set-zset-位图-hyperlogloguv-布隆过滤器">38.[redis]数据结构：String、List、Hash、Set、ZSet、位图、HyperLogLog(uv)、布隆过滤器</h3>
<p>5 种基础数据结构 ：String（字符串）、List（列表）、Set（集合）、Hash（散列）、Zset（有序集合）。<br>
3 种特殊数据结构 ：HyperLogLogs（基数统计）、Bitmap （位存储）、Geospatial (地理位置)。<br>
<code>String类型</code><br>
String 是最基本的 key-value 结构，key 是唯一标识，value 是具体的值，value其实不仅是字符串， 也可以是数字（整数或浮点数），value 最多可以容纳的数据长度是 512M。</p>
<p><code>Hash</code><br>
Hash 是一个键值对（key-value）集合，其中 value 的形式如： <code>value=[{field1，value1}，...{fieldN，valueN}]</code>。Hash 特别适合用于存储对象。Hash 类型的底层数据结构是由压缩列表或哈希表实现的。</p>
<p><code>List</code><br>
List 列表是简单的字符串列表，按照插入顺序排序，可以从头部或尾部向 List 列表添加元素。<br>
列表的最大长度为 2^32 - 1，也即每个列表支持超过 40 亿个元素。List 类型的底层数据结构是由双端链表或压缩列表实现的。<br>
<img src="https://q456qq520.github.io/post-images/1676600145675.png" alt="" loading="lazy"></p>
<p>如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用压缩列表作为 List 类型的底层数据结构；<br>
如果列表的元素不满足上面的条件，Redis 会使用双端链表作为 List 类型的底层数据结构；</p>
<p><code>set</code><br>
Set 类型是一个无序并唯一的键值集合，它的存储顺序不会按照插入的先后顺序进行存储。<br>
一个集合最多可以存储 2^32-1 个元素。概念和数学中个的集合基本类似，可以交集，并集，差集等等，所以 Set 类型除了支持集合内的增删改查，同时还支持多个集合取交集、并集、差集。<br>
Set 类型的底层数据结构是由哈希表或整数集合实现的。</p>
<p><code>zset</code><br>
Zset 类型（有序集合类型）相比于 Set 类型多了一个排序属性 score（分值），对于有序集合 ZSet 来说，每个存储元素相当于有两个值组成的，一个是有序结合的元素值，一个是排序值。<br>
有序集合保留了集合不能有重复成员的特性（分值可以重复），但不同的是，有序集合中的元素可以排序。<br>
<img src="https://q456qq520.github.io/post-images/1676600392146.png" alt="" loading="lazy"></p>
<p>Zset 类型的底层数据结构是由压缩列表或跳表实现的。</p>
<p>如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构；<br>
如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构；</p>
<p><code>HyperLogLog</code><br>
HyperLogLog 是用来做基数统计的算法，即对集合去重元素的计数</p>
<p>在输入元素的数量不超过2^64个，计算基数所需的内存最多12KB，该结构使用一种近似值算法，标准误差0.81%。</p>
<p><code>位图</code><br>
位图，即大量bit组成的一个数据结构(每个bit只能是0和1)。Redis 的位图（bitmap）是由多个二进制位组成的数组，数组中的每个二进制位都有与之对应的偏移量（从 0 开始），通过这些偏移量可以对位图中指定的一个或多个二进制位进行操作。<br>
<img src="https://q456qq520.github.io/post-images/1676601290789.png" alt="" loading="lazy"></p>
<p>BitMap 的基本原理就是用一个 bit 来标记某个元素对应的 Value，而 Key 即是该元素。由于采用一 个bit 来存储一个数据，因此可以大大的节省空间。</p>
<p><code>布隆过滤器</code><br>
布隆过滤器：一种数据结构，是由一串很长的二进制向量组成，可以将其看成一个二进制数组。既然是二进制，那么里面存放的不是0，就是1，但是初始默认值都是0。</p>
<p>当一个元素加入布隆过滤器中的时候，会进行如下操作：</p>
<ol>
<li>使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。</li>
<li>根据得到的哈希值，在位数组中把对应下标的值置为 1。</li>
</ol>
<p>当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作：</p>
<ol>
<li>对给定元素再次进行相同的哈希计算；</li>
<li>得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，<br>
如果存在一个值不为 1，说明该元素不在布隆过滤器中。</li>
</ol>
<h3 id="39redis内部编码sdsintlinkedlistziplistquicklistinsetskiplistdict">39.[redis]内部编码：sds，int，linkedList，ziplist，quickList，inset，skiplist，dict</h3>
<p>在Redis中有一个「核心的对象」叫做redisObject ，是用来表示所有的key和value的，用redisObject结构体来表示String、Hash、List、Set、ZSet五种数据类型。在redisObject中「type表示属于哪种数据类型，encoding表示该数据的存储方式」，也就是底层的实现的该数据类型的数据结构。<br>
<img src="https://q456qq520.github.io/post-images/1676599283431.png" alt="" loading="lazy"><br>
<code>SDS</code><br>
String 类型的底层的数据结构实现主要是 int 和 SDS（简单动态字符串）。字符串对象的内部编码有三种int、raw、embst。</p>
<ol>
<li>如果一个字符串对象保存的是整数值，并且这个整数值可以用 long 类型来表示，那么字符串对象会将整数值保存在字符串对象结构的 ptr 属性里面（将 void* 转换成 long），并将字符串对象的编码设置为 int。</li>
<li>如果字符串对象保存的是一个字符串，并且这个字符串的长度小于等于 32 字节（redis 2.+ 版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为 embstr， embstr 编码是专门用于保存短字符串的一种优化编码方式：</li>
<li>如果字符串对象保存的是一个字符串，并且这个字符串的长度大于 32 字节（redis 2.+ 版本），那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串，并将对象的编码设置为 raw：</li>
</ol>
<p>SDS称为「简单动态字符串」，对于SDS中的定义在Redis的源码中有的三个属性int len、int free、char buf[]。len保存了字符串的长度，free表示buf数组中未使用的字节数量，buf数组则是保存字符串的每一个字符元素。<br>
因此当你在Redsi中存储一个字符串Hello时，根据Redis的源代码的描述可以画出SDS的形式的redisObject结构图如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1676603471646.png" alt="" loading="lazy"></p>
<ol>
<li>SDS提供「空间预分配」和「惰性空间释放」两种策略。在为字符串分配空间时，分配的空间比实际要多，这样就能「减少连续的执行字符串增长带来内存重新分配的次数」。<br>
当字符串被缩短的时候，SDS也不会立即回收不适用的空间，而是通过free属性将不使用的空间记录下来，等后面使用的时候再释放。</li>
<li>SDS是二进制安全的，除了可以储存字符串以外还可以储存二进制文件（如图片、音频，视频等文件的二进制数据）</li>
<li>SDS会先根据len属性判断空间是否满足要求，若是空间不够，就会进行相应的空间扩展，所以不会出现缓冲区溢出的情况。</li>
<li>Redis中获取字符串长度只要读取len的值就可，时间复杂度变为O(1)</li>
</ol>
<p><code>int</code><br>
Redis中规定假如存储的是「整数型值」，比如set num 123这样的类型，就会使用 int的存储方式进行存储，在redisObject的ptr属性中就会保存该值。</p>
<p><code>linkedlist</code><br>
linkedlist即经典的双链表，双端链表是 Redis 的列表键的底层实现之一。Redis在实现链表的时候，定义其为双端无环链表。<br>
list 结构为链表提供了表头指针 head, 表尾指针 tail, 以及链表长度的计数器 len 来方便的对链表进行一个双端的遍历，或者查看链表长度。<br>
<img src="https://q456qq520.github.io/post-images/1676604379662.png" alt="" loading="lazy"><br>
<code>ziplist</code><br>
压缩列表（ziplist）是一组连续内存块组成的顺序的数据结构，压缩列表能够节省空间，压缩列表中使用多个节点来存储数据。<br>
压缩列表是列表键和哈希键底层实现的原理之一，压缩列表并不是以某种压缩算法进行压缩存储数据，而是它表示一组连续的内存空间的使用，节省空间，压缩列表的内存结构图如下：<br>
<img src="https://q456qq520.github.io/post-images/1676603715501.png" alt="" loading="lazy"><br>
压缩列表中每一个节点表示的含义如下所示：</p>
<p>zlbytes：4个字节的大小，记录压缩列表占用内存的字节数。zltail：4个字节大小，记录表尾节点距离起始地址的偏移量，用于快速定位到尾节点的地址。zllen：2个字节的大小，记录压缩列表中的节点数。entry：表示列表中的每一个节点。zlend：表示压缩列表的特殊结束符号'0xFF'。</p>
<p>再压缩列表中每一个entry节点又有三部分组成，包括previous_entry_ength、encoding、content。<br>
previous_entry_ength表示前一个节点entry的长度，可用于计算前一个节点的其实地址，因为他们的地址是连续的。encoding：这里保存的是content的内容类型和长度。content：content保存的是每一个节点的内容。</p>
<p><code>quickList</code><br>
QuickList是一个节点为 ZipList 的双端链表，节点采用 ZipList ，解决了传统链表的内存占用问题，控制了 ZipList 大小，解决连续内存空间申请效率问题，中间节点可以压缩，进一步节省了内存<br>
<img src="https://q456qq520.github.io/post-images/1676604517170.png" alt="" loading="lazy"><br>
<code>inset</code><br>
intset 是 set 集合的一种实现方式，基于整数数组来实现，并且具备长度可变、有序等特征，底层采用二分查找方式来查询。<br>
<code>skiplist</code><br>
skiplist也叫做「跳跃表」，跳跃表是一种有序的数据结构，它通过每一个节点维持多个指向其它节点的指针，从而达到快速访问的目的。<br>
skiplist由如下几个特点：</p>
<ol>
<li>有很多层组成，由上到下节点数逐渐密集，最上层的节点最稀疏，跨度也最大。</li>
<li>每一层都是一个有序链表，只扫包含两个节点，头节点和尾节点。</li>
<li>每一层的每一个每一个节点都含有指向同一层下一个节点和下一层同一个位置节点的指针。</li>
<li>如果一个节点在某一层出现，那么该以下的所有链表同一个位置都会出现该节点。<br>
<img src="https://q456qq520.github.io/post-images/1676604124705.png" alt="" loading="lazy"><br>
在跳跃表的结构中有head和tail表示指向头节点和尾节点的指针，能后快速的实现定位。level表示层数，len表示跳跃表的长度，BW表示后退指针，在从尾向前遍历的时候使用。BW下面还有两个值分别表示分值（score）和成员对象（各个节点保存的成员对象）。<br>
跳跃表的实现中，除了最底层的一层保存的是原始链表的完整数据，上层的节点数会越来越少，并且跨度会越来越大。跳跃表的上面层就相当于索引层，都是为了找到最后的数据而服务的，数据量越大，条表所体现的查询的效率就越高，和平衡树的查询效率相差无几。</li>
</ol>
<p><code>dict</code><br>
键与值的映射关系正是通过 Dict 来实现的。是 set 和 hash 的实现方式之一。Dict 由三部分组成，分别是：哈希表（DictHashTable）、哈希节点（DictEntry）、字典（Dict）</p>
<p>当我们向 Dict 添加键值对时，Redis 首先根据 key 计算出 hash 值（h），然后利用 h &amp; sizemask 来计算元素应该存储到数组中的哪个索引位置。</p>
<h3 id="40redis持久化方式aof-rdb-混合">40.[redis]持久化方式：AOF、RDB、混合</h3>
<p><code>RDB</code><br>
RDB是一种快照存储持久化方式，具体就是将Redis某一时刻的内存数据保存到硬盘的文件当中，默认保存的文件名为dump.rdb，而在Redis服务器启动时，会重新加载dump.rdb文件的数据到内存当中恢复数据。<br>
当客户端向服务器发送save命令请求进行持久化时，服务器会阻塞save命令之后的其他客户端的请求，直到数据同步完成。与save命令不同，bgsave命令是一个异步操作。当客户端发服务发出bgsave命令时，Redis服务器主进程会forks一个子进程来数据同步问题，在将数据保存到rdb文件之后，子进程会退出。</p>
<p><code>AOF</code><br>
AOF持久化方式会记录客户端对服务器的每一次写操作命令，并将这些写操作以Redis协议追加保存到以后缀为aof文件末尾，在Redis服务器重启时，会加载并运行aof文件的命令，以达到恢复数据的目的。</p>
<p>三种写入策略：</p>
<ol>
<li>always，客户端的每一个写操作都保存到aof文件当，这种策略很安全，但是每个写请注都有IO操作，所以也很慢。</li>
<li>everysec，appendfsync的默认写入策略，每秒写入一次aof文件，因此，最多可能会丢失1s的数据。</li>
<li>no，Redis服务器不负责写入aof，而是交由操作系统来处理什么时候写入aof文件。更快，但也是最不安全的选择，不推荐使用。</li>
</ol>
<p><code>混合</code><br>
混合持久化方式，Redis 4.0 之后新增的方式，混合持久化是结合了 RDB 和 AOF 的优点，开启了混合持久化模式后，AOF在重写的时候，不再是单纯的将AOF缓冲区的命令写入AOF文件中，而是将重写这一刻之前的内存做RDB的快照处理，并将将RDB的快照内容和增量的AOF修改内存数据的命令放在一起，都写入AOF，新的文件一开始不叫appendonly.aof，等到重写完新的AOF文件才会进行改名，覆盖原有的AOF文件，完成新旧AOF文件的交替。恢复的时候可以先加载AOF文件中RDB的部分，再根据命令还原剩余部分。这样对于数据恢复的效率和安全性都能够得到保障。</p>
<blockquote>
<p>aof-use-rdb-preamble yes</p>
</blockquote>
<h3 id="41redis持久化方式aof-rdb-优缺点原理">41.[redis]持久化方式：AOF、RDB 优缺点，原理</h3>
<p><code>RDB 方式的优点</code></p>
<ol>
<li>RDB 是一个非常紧凑的文件,它保存了某个时间点的数据集,非常适用于数据集的备份</li>
<li>RDB 是一个紧凑的单一文件,很方便传送到另一个远端数据中心，非常适用于灾难恢复。</li>
<li>RDB 在保存 RDB 文件时父进程唯一需要做的就是 fork 出一个子进程,接下来的工作全部由子进程来做，父进程不需要再做其他 IO 操作，这种工作方式使得 Redis 可以从写时复制（copy-on-write）机制中获益，所以 RDB 持久化方式可以最大化 Redis 的性能。</li>
<li>与AOF相比,在恢复大的数据集的时候，RDB 方式会更快一些。<br>
<code>RDB 方式的缺点</code></li>
<li>如果你希望在 Redis 意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么 RDB 不适合你.虽然你可以配置不同的save时间点(例如每隔 5 分钟并且对数据集有 100 个写的操作),是 Redis 要完整的保存整个数据集是一个比较繁重的工作,你通常会每隔5分钟或者更久做一次完整的保存,万一在 Redis 意外宕机,你可能会丢失几分钟的数据。</li>
<li>RDB 需要经常 fork 子进程来保存数据集到硬盘上,当数据集比较大的时候, fork 的过程是非常耗时的,可能会导致 Redis 在一些毫秒级内不能响应客户端的请求。<br>
<code>AOF 方式的优点</code></li>
<li>AOF文件是一个只进行追加的日志文件,所以不需要写入seek,即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令,你也也可使用redis-check-aof工具修复这些问题。</li>
<li>Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。</li>
<li>AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。</li>
<li>你可以使用不同的 fsync 策略：无 fsync、每秒 fsync 、每次写的时候 fsync .使用默认的每秒 fsync 策略, Redis 的性能依然很好( fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求),一旦出现故障，你最多丢失1秒的数据。</li>
</ol>
<p><code>AOF 方式的缺点</code></p>
<ol>
<li>对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。</li>
<li>根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。<br>
<img src="https://q456qq520.github.io/post-images/1676605399216.png" alt="" loading="lazy"></li>
</ol>
<h3 id="42redis过期策略定期删除惰性删除从库的过期策略">42.[redis]过期策略：定期删除，惰性删除，从库的过期策略</h3>
<p>常见的删除策略：</p>
<ol>
<li>定时删除<br>
在设置键的过期时间的同时，创建一个定时器，让定时器在键的过期时间来临时，立即执行对键的删除操作。<br>
定时删除策略可以保证过期键尽可能快地被删除，并释放过期键占用的内存。</li>
</ol>
<p>因此，定时删除策略的优缺点如下所示：<br>
优点：对内存非常友好<br>
缺点：对CPU时间非常不友好，如果服务器创建大量的定时器，服务器处理命令请求的性能就会降低<br>
Redis不支持定时策略。</p>
<ol start="2">
<li>惰性删除<br>
放任过期键不管，每次从键空间中获取键时，检查该键是否过期，如果过期，就删除该键，如果没有过期，就返回该键。惰性删除策略只会在获取键时才对键进行过期检查，不会在删除其它无关的过期键花费过多的CPU时间。<br>
惰性删除策略的优缺点如下所示：<br>
优点：对CPU时间非常友好<br>
缺点：对内存非常不友好</li>
</ol>
<p>举个例子，如果数据库有很多的过期键，而这些过期键又恰好一直没有被访问到，那这些过期键就会一直占用着宝贵的内存资源，造成资源浪费。</p>
<p>过期键的惰性删除策略由<code>expireIfNeeded</code>函数实现，所有读写数据库的Redis命令在执行之前都会调用expireIfNeeded函数对输入键进行检查：</p>
<ul>
<li>如果输入键已经过期，那么将输入键从数据库中删除</li>
<li>如果输入键未过期，那么不做任何处理</li>
</ul>
<ol start="3">
<li>定期删除<br>
每隔一段时间，程序对数据库进行一次检查，删除里面的过期键，至于要删除哪些数据库的哪些过期键，则由算法决定。<br>
定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响，同时，通过定期删除过期键，也有效地减少了因为过期键而带来的内存浪费。</li>
</ol>
<p>过期键的定期删除策略由<code>activeExpireCycle</code>函数实现，每当Redis服务器的周期性操作<code>serverCron</code>函数执行时，activeExpireCycle函数就会被调用，它在规定的时间内，分多次遍历服务器中的各个数据库，从数据库的expires字典中随机检查一部分键的过期时间，并删除其中的过期键。</p>
<p><code>从库过期策略</code><br>
在主从复制模式下，从服务器的过期键删除动作由主服务器控制：</p>
<ol>
<li>主服务器在删除一个过期键后，会显式地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键。</li>
<li>从服务器在执行客户端发送的读命令时，即使发现该键已过期也不会删除该键，照常返回该键的值。</li>
<li>从服务器只有接收到主服务器发送的DEL命令后，才会删除过期键。</li>
</ol>
<p><code>RDB对过期键的处理</code><br>
在执行SAVE命令或者BGSAVE命令创建一个新的RDB文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的RDB文件中。如果服务器以主服务器模式运行，在载入RDB文件时，程序会对文件中保存的键进行检查，未过期的键会被载入到数据库中，过期键会被忽略。如果服务器以从服务器模式运行，在载入RDB文件时，文件中保存的所有键，不论是否过期，都会被载入到数据库中。<br>
<code>AOF对过期键的处理</code><br>
如果数据库中的某个键已经过期，并且服务器开启了AOF持久化功能，当过期键被惰性删除或者定期删除后，程序会向AOF文件追加一条DEL命令，显式记录该键已被删除。<br>
举个例子，如果客户端执行命令GET message访问已经过期的message键，那么服务器将执行以下3个动作：</p>
<ul>
<li>从数据库中删除message键</li>
<li>追加一条DEL message命令到AOF文件</li>
<li>向执行GET message命令的客户端返回空回复<br>
在执行AOF文件重写时，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中。</li>
</ul>
<h3 id="43redis内存淘汰策略拒绝有过期时间无过期时间随机-快过期的">43.[redis]内存淘汰策略：拒绝，有过期时间/无过期时间；随机、快过期的</h3>
<ol>
<li>noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错。默认策略。</li>
<li>allkeys-lru：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，移除最近最少使用的 key（这个是最常用的）。</li>
<li>allkeys-lfu：在所有的数据中淘汰使用使用频率最低的数据。</li>
<li>allkeys-random：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，随机移除某个 key。</li>
<li>volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，移除最近最少使用的 key。</li>
<li>volatile-lfu：在设置过期时间的数据中淘汰使用频率最低的数据。</li>
<li>volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，随机移除某个 key。</li>
<li>volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，有更早过期时间的 key 优先移除。</li>
</ol>
<h3 id="44redis部署模式-主从哨兵集群模式">44.[redis]部署模式。主从，哨兵，集群模式</h3>
<p><code>主从模式</code><br>
redis 多机器部署时，这些机器节点会被分成两类，一类是主节点（master 节点），一类是从节点（slave 节点）。一般主节点可以进行读、写操作，而从节点只能进行读操作。同时由于主节点可以写，数据会发生变化，当主节点的数据发生变化时，会将变化的数据同步给从节点，这样从节点的数据就可以和主节点的数据保持一致了。一个主节点可以有多个从节点，但是一个从节点会只会有一个主节点，也就是所谓的一主多从结构。<br>
<img src="https://q456qq520.github.io/post-images/1676618817839.png" alt="" loading="lazy"><br>
优点：</p>
<ol>
<li>支持主从复制，主机会自动将数据同步到从机，可以进行读写分离;</li>
<li>为了分载 Master 的读操作压力，Slave 服务器可以为客户端提供只读操作的服务，写服务依然必须由 Master 来完成;</li>
<li>Slave 同样可以接受其他 Slaves 的连接和同步请求，这样可以有效地分载 Master 的同步压力;</li>
<li>Master 是以非阻塞的方式为 Slaves 提供服务。所以在 Master-Slave 同步期间，客户端仍然可以提交查询或修改请求;</li>
<li>Slave 同样是以阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis 则返回同步之前的数据。</li>
</ol>
<p>缺点：</p>
<ol>
<li>Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的 IP 才能恢复;</li>
<li>主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了系统的可用性;</li>
<li>如果多个 Slave 断线了，需要重启的时候，尽量不要在同一时间段进行重启。因为只要 Slave 启动，就会发送 sync 请求和主机全量同步，当多个 Slave 重启的时候，可能会导致 Master IO 剧增从而宕机。</li>
<li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂;</li>
<li>redis 的主节点和从节点中的数据是一样的，降低的内存的可用性</li>
</ol>
<p><code>哨兵模式</code><br>
在主从模式下，redis 同时提供了哨兵命令redis-sentinel，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵进程向所有的 redis 机器发送命令，等待 Redis 服务器响应，从而监控运行的多个 Redis 实例。<br>
哨兵可以有多个，一般为了便于决策选举，使用奇数个哨兵。哨兵可以和 redis 机器部署在一起，也可以部署在其他的机器上。多个哨兵构成一个哨兵集群，哨兵直接也会相互通信，检查哨兵是否正常运行，同时发现 master 宕机哨兵之间会进行决策选举新的 master<br>
<img src="https://q456qq520.github.io/post-images/1676618828608.png" alt="" loading="lazy"><br>
哨兵模式的作用:</p>
<ol>
<li>通过发送命令，让 Redis 服务器返回监控其运行状态，包括主服务器和从服务器;</li>
<li>当哨兵监测到 master 宕机，会自动将 slave 切换到 master，然后通过发布订阅模式通过其他的从服务器，修改配置文件，让它们切换主机;</li>
<li>然而一个哨兵进程对 Redis 服务器进行监控，也可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。</li>
</ol>
<p>优点</p>
<ol>
<li>哨兵模式是基于主从模式的，所有主从的优点，哨兵模式都具有。</li>
<li>主从可以自动切换，系统更健壮，可用性更高。</li>
</ol>
<p>缺点</p>
<ol>
<li>具有主从模式的缺点，每台机器上的数据是一样的，内存的可用性较低。</li>
<li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。</li>
</ol>
<p><code>集群模式</code><br>
redis3.0 上加入了 Cluster 集群模式，实现了 Redis 的分布式存储，对数据进行分片，也就是说每台 Redis 节点上存储不同的内容，Redis 的集群模式本身没有使用一致性 hash 算法，而是使用 slots 插槽。<br>
<img src="https://q456qq520.github.io/post-images/1676618835508.png" alt="" loading="lazy"><br>
优点</p>
<ol>
<li>采用去中心化思想，数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布;</li>
<li>可扩展性：可线性扩展到 1000 多个节点，节点可动态添加或删除;</li>
<li>高可用性：部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副本，能够实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave 到 Master 的角色提升;</li>
<li>降低运维成本，提高系统的扩展性和可用性。</li>
</ol>
<p>缺点</p>
<ol>
<li>Redis Cluster 是无中心节点的集群架构，依靠 Goss 协议(谣言传播)协同自动化修复集群的状态<br>
但 GosSIp 有消息延时和消息冗余的问题，在集群节点数量过多的时候，节点之间需要不断进行 PING/PANG 通讯，不必须要的流量占用了大量的网络资源。</li>
<li>数据迁移问题<br>
Redis Cluster 可以进行节点的动态扩容缩容，这一过程，在目前实现中，还处于半自动状态，需要人工介入。在扩缩容的时候，需要进行数据迁移。<br>
而 Redis 为了保证迁移的一致性，迁移所有操作都是同步操作，执行迁移时，两端的 Redis 均会进入时长不等的阻塞状态，对于小 Key，该时间可以忽略不计，但如果一旦 Key 的内存使用过大，严重的时候会接触发集群内的故障转移，造成不必要的切换。</li>
</ol>
<h3 id="45redis部署模式-主从的同步策略增量同步全量同步">45.[redis]部署模式。主从的同步策略，增量同步\全量同步</h3>
<p>当Slave需要和Master进行数据同步时：</p>
<ol>
<li>
<pre><code>Salve会发送sync命令到Master
</code></pre>
</li>
<li>
<pre><code>Master启动一个后台进程，将Redis中的数据快照保存到文件中
</code></pre>
</li>
<li>
<pre><code>启动后台进程的同时，Master会将保存数据快照期间接收到的写命令缓存起来
</code></pre>
</li>
<li>
<pre><code>Master完成写文件操作后，将该文件发送给Salve
</code></pre>
</li>
<li>
<pre><code>Salve将文件保存到磁盘上，然后加载文件到内存恢复数据快照到Salve的Redis上
</code></pre>
</li>
<li>
<pre><code>当Salve完成数据快照的恢复后，Master将这期间收集的写命令发送给Salve端
</code></pre>
</li>
<li>
<pre><code>后续Master收集到的写命令都会通过之前建立的连接，增量发送给salve端
</code></pre>
</li>
</ol>
<p><mark>主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。</mark></p>
<h3 id="46redis部署模式-哨兵模式的优点是如何工作的">46.[redis]部署模式。哨兵模式的优点，是如何工作的？</h3>
<p>Redis 哨兵模式：<br>
优点：<br>
可以监控主节点的状态，如果主节点出现故障，哨兵会自动将从节点升级为主节点，从而保证了数据的可用性。<br>
可以在不停止 Redis 服务的情况下进行节点的替换或者维护。<br>
是一种简单、高效的高可用方案。<br>
缺点：<br>
当主节点出现故障时，涉及到数据重新同步和更新操作，这样会对系统性能造成一定影响。<br>
如果所有哨兵都失效，就无法实现高可用。</p>
<p><code>哨兵的原理</code><br>
1 从库发现<br>
哨兵在连接主库之后，会调用 INFO 命令获取主库的信息，再从中解析出连接主库的从库信息，再以此和其他从库建立连接进行监控。<br>
哨兵对所有节点都会每隔 10s 发送一次 INFO 命令，从各节点获取 Redis 集群实时的拓扑图信息。如果新节点加入，哨兵就会去监控新的节点。</p>
<p>2 发布/订阅机制<br>
哨兵们在连接同一个主库之后，是通过发布/订阅（pub/sub）模式来发现彼此的存在的。哨兵们会在每个 Redis 服务上创建并订阅一个名为 <code>__sentinel__:hello</code> 的频道，哨兵们就是通过它来相互发现，实现相互通信的。<br>
订阅后，每个哨兵每隔 2 秒都会向 hello 频道发布一条携带自身信息的 hello 信息，这样哨兵就能知道其他哨兵的状态、监控的主节点和是否有新的哨兵加入：</p>
<p>3 监控<br>
哨兵在对 Redis 节点建立 TCP 连接之后，会周期性地发送 <code>PING</code> 命令给节点（默认是 1s），以此判断节点是否正常。如果在<code>down-after-millisenconds</code> 时间内没有收到节点的响应，它就认为这个节点掉线了。</p>
<p>4 主观下线<br>
当哨兵发现与自己连接的其他节点断开连接，它就会将该节点标记为主观下线（+sdown），包括主节点、从节点或者其他哨兵都可以标记为 sdown 状态。当该节点重新连接之后，哨兵会取消对它的主观下线标记，操作是 <code>-sdown</code>。如果哨兵判断从节点或者其他哨兵节点主观下线，哨兵并不会执行其他操作。如果是主节点主观下线，哨兵就要采取措施，确定主节点是否真的宕机，并执行故障转移。</p>
<p>5 客观下线<br>
哨兵确认主节点是否真的宕机这一步成为客观下线确认，如果主节点真的宕机了，哨兵就会将主节点标记为客观下线（+odown）状态。<br>
要判断主节点是否客观下线，需要与其他哨兵达成共识，如果大多数哨兵认为主节点主观下线了，哨兵才能确认主节点客观下线。达成共识的方式就是发起一轮投票，如果票数超过哨兵节点数的一半，并且大于等于 <code>quorum</code> 设置的数量，就是投票成功。否则哨兵就不能说主节点客观下线了。</p>
<p>6 客观下线投票过程</p>
<ul>
<li>当哨兵发现主节点下线，标记主节点为 <code>sdown</code> 状态。</li>
<li>哨兵向其他哨兵发送 <code>SENTINEL is-master-down-by-addr</code> 命令，询问其他哨兵该主节点是否已下线。</li>
<li>其他哨兵在收到投票请求之后，会检查本地主缓存中主节点的状态并进行回复（1 表示下线，0 表示正常）。</li>
<li>发起的询问的哨兵在接收到回复之后，会累加“下线”的得票数。</li>
<li>当下线的票数大于一半哨兵数量并且不小于 <code>quorum</code>时，就会将主节点标记为 <code>odown</code> 状态。并开始准备故障转移。</li>
<li>发起投票的哨兵有一个投票倒计时，倒计时结束如果票数仍然不够的话，则放弃本次客观线下投票。并尝试继续与主节点建立连接。</li>
</ul>
<p>7 故障转移<br>
哨兵在将主节点标记为 odown 状态之后，就会马上开始尝试故障转移了。<br>
故障转移主要由 <code>sentinelFailoverStateMachineZ(sentinelRedisInstance)</code>函数负责2。该函数由一个状态机组成，共有五个状态，标志着故障转移共分为五个大步骤：</p>
<table>
<thead>
<tr>
<th>状态</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>WAIT_START</code></td>
<td>Leader 选举</td>
</tr>
<tr>
<td><code>SELECT_SLAVE</code></td>
<td>Master 选取</td>
</tr>
<tr>
<td><code>SEND_SLAVEOF_NOONE</code></td>
<td>Slave 身份去除</td>
</tr>
<tr>
<td><code>WAIT_PROMOTION</code></td>
<td>提升 Master</td>
</tr>
<tr>
<td><code>RECONF_SLAVES</code></td>
<td>配置从节点</td>
</tr>
</tbody>
</table>
<p>哨兵首先进入 WATI_START 状态进行准备，等待哨兵成为哨兵集群的 Leader 才有资格进行故障转移。如果在超时时间之内哨兵都没有成为 Leader，则哨兵会调用 sentinelAbortFailover() 函数并结束本次故障转移。当选 Leader 后哨兵会进入 SELECT_SLAVE 状态，选取新的主节点。当确定新的主节点后，哨兵会进入 SEND_SLAVEOF_NOONE 状态，撤销该节点的 Slave 状态。在发送指令之后，哨兵会进入 WAIT_PROMOTION 状态，等待该节点将自己提升为主节点。当节点提升为 Master 之后，哨兵会进入 RECONF_SLAVES 状态，更新所有从节点的配置，让他们去复制新的 Master。</p>
<p>当哨兵进行故障转移之后，哨兵会通知客户端主节点发生更换，让客户端去连接新的主节点。</p>
<p>哨兵同样是通过发布/订阅机制实现的客户端通知，每个连接哨兵的客户端，会去订阅哨兵的 <code>+switch-master</code> 频道，当 Leader 进行故障转移后，会向其他哨兵发送新主节点配置，然后所有哨兵都会在 <code>+switch-master</code> 频道发布主节点切换信息，此时客户端监听到变化，就会去连接新的主节点。客户端后台线程订阅 <code>+switch-master</code>频道，接收到消息之后解析并重新初始化全局主节点 <code>initMaster()</code>。</p>
<h3 id="47redis部署模式-集群模式的优点是如何分片的">47.[redis]部署模式。集群模式的优点，是如何分片的？</h3>
<p>Redis 集群模式：<br>
优点：<br>
可以提供高可用性，如果一个节点出现故障，集群自动转移到其他节点。<br>
可以大大提高 Redis 的读写性能，因为可以在不同的节点上进行数据分片。<br>
缺点：<br>
集群模式的配置相对比较复杂，不如哨兵模式简单。<br>
因为数据分片存在，所以查询数据时可能需要跨越多个节点，这样会造成一定的性能影响。</p>
<p><code>集群分片</code><br>
Redis Cluster 采用的是虚拟槽分区，一个集群共有<code>16384</code>个哈希槽，Redis Cluster会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N个。每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash slot。</p>
<p><code>扩容集群</code><br>
当一个 Redis 新节点运行并加入现有集群后，我们需要为其迁移槽和数据。首先要为新节点指定槽的迁移计划，确保迁移后每个节点负责相似数量的槽，从而保证这些节点的数据均匀。假设现在有集群M1，M2，M3，现在需要新增一个M4，步骤如下：</p>
<ol>
<li>首先启动一个 Redis 节点，记为 M4。</li>
<li>使用 cluster meet 命令，让新 Redis 节点加入到集群中。新节点刚开始都是主节点状态，由于没有负责的槽，所以不能接受任何读写操作，后续我们就给他迁移槽和填充数据。</li>
<li>对 M4 节点发送 cluster setslot { slot } importing { sourceNodeId} 命令，让目标节点准备导入槽的数据。</li>
<li>对源节点，也就是 M1，M2，M3 节点发送 cluster setslot { slot } migrating { targetNodeId} 命令，让源节点准备迁出槽的数据。</li>
<li>源节点执行 cluster getkeysinslot { slot } { count } 命令，获取 count 个属于槽 { slot } 的键，然后执行步骤六的操作进行迁移键值数据。</li>
<li>在源节点上执行 migrate { targetNodeIp} &quot; &quot; 0 { timeout } keys { key... } 命令，把获取的键通过 pipeline 机制批量迁移到目标节点，批量迁移版本的 migrate 命令在 Redis 3.0.6 以上版本提供。</li>
<li>重复执行步骤 5 和步骤 6 直到槽下所有的键值数据迁移到目标节点。</li>
<li>向集群内所有主节点发送 cluster setslot { slot } node { targetNodeId } 命令，通知槽分配给目标节点。为了保证槽节点映射变更及时传播，需要遍历发送给所有主节点更新被迁移的槽执行新节点。</li>
</ol>
<p><code>收缩集群</code><br>
收缩节点就是将 Redis 节点下线，整个流程需要如下操作流程。</p>
<ol>
<li>首先需要确认下线节点是否有负责的槽，如果是，需要把槽迁移到其他节点，保证节点下线后整个集群槽节点映射的完整性。原理与之前节点扩容的迁移槽过程一致。</li>
<li>当下线节点不再负责槽或者本身是从节点时，就可以通知集群内其他节点忘记下线节点，当所有的节点忘记改节点后可以正常关闭。</li>
</ol>
<p><code>客户端路由</code><br>
在集群模式下，Redis 节点接收任何键相关命令时首先计算键对应的槽，在根据槽找出所对应的节点，如果节点是自身，则处理键命令；否则回复 <code>MOVED</code> 重定向错误，通知客户端请求正确的节点。这个过程称为 MOVED 重定向。</p>
<ol>
<li>客户端根据本地 slot 缓存发送命令到源节点，如果存在键对应则直接执行并返回结果给客户端。</li>
<li>如果节点返回 MOVED 错误，更新本地的 slot 到 Redis 节点的映射关系，然后重新发起请求。</li>
<li>如果数据正在迁移中，节点会回复 ASK 重定向异常。格式如下: ( error ) ASK { slot } { targetIP } : {targetPort}</li>
<li>客户端从 ASK 重定向异常提取出目标节点信息，发送 asking 命令到目标节点打开客户端连接标识，再执行键命令。</li>
</ol>
<blockquote>
<p>默认情况下，当集群 16384 个槽任何一个没有指派到节点时整个集群不可用。执行任何键命令返回 CLUSTERDOWN Hash slot not served 命令。当持有槽的主节点下线时，从故障发现到自动完成转移期间整个集群是不可用状态，对于大多数业务无法忍受这情况，因此建议将参数 <code>cluster-require-full-coverage</code> 配置为 <code>no</code> ，当主节点故障时只影响它负责槽的相关命令执行，不会影响其他主节点的可用性。</p>
</blockquote>
<h3 id="48redishash是如何扩容的">48.[redis]hash是如何扩容的？</h3>
<p>Redis中使用哈希表作为底层实现的是叫做（dict）字典的数据结构，字典又称为符号表、关联数组或映射(map)。是一种保存键值对的抽象数据结构。</p>
<p>首先dict有四个部分组成，分别是dictType(类型),dictht（核心），rehashidx(渐进式hash的标志)，iterators（迭代器），这里面最重要的就是dictht和rehashidx。<br>
<img src="https://q456qq520.github.io/post-images/1676627209014.png" alt="" loading="lazy"></p>
<pre><code class="language-java">//字典结构体 
 typedef struct dict {
    dictType *type;//类型，包括一些自定义函数，这些函数使得key和value能够存储 
    void *privdata;//私有数据 
    dictht ht[2];//两张hash表 
    long rehashidx; //渐进式hash标记，如果为-1，说明没在进行hash
    unsigned long iterators; //正在迭代的迭代器数量
} dict;
</code></pre>
<p><code>扩容过程和渐进式Hash图解</code><br>
dictht[2]为什么会要2个数组存放，真正的数据只要一个数组就够了？<br>
随着数据量的增加，hash碰撞发生的就越频繁，每个数组后面的链表就越长，整个链表显得非常累赘。这无疑是要进行扩容，所以第一个数组存放真正的数据，第二个数组用于扩容用。</p>
<p>rehashidx其实是一个标志量，如果为-1说明当前没有扩容，如果不为-1则表示当前扩容到哪个下标位置，方便下次进行从该下标位置继续扩容。</p>
<p>扩容步骤如下：</p>
<ol>
<li>首先是未扩容前，rehashidx为-1，表示未扩容，第一个数组的dictEntry长度为4，一共有5个节点，所以used为5。<br>
<img src="https://q456qq520.github.io/post-images/1676627795645.png" alt="" loading="lazy"></li>
<li>当发生扩容了，rahashidx为第一个数组的第一个下标位置，即0。扩容之后的大小为大于used<em>2的2的n次方的最小值，即能包含这些节点</em>2的2的倍数的最小值。因为当前为5个数据节点，所以used*2=10，扩容后的数组大小为大于10的2的次方的最小值，为16。从第一个数组0下标位置开始，查找第一个元素，找到key为name，value为张三的节点，将其hash过，找到在第二个数组的下标为1的位置，将节点移过去，其实是指针的移动。<br>
<img src="https://q456qq520.github.io/post-images/1676627803033.png" alt="" loading="lazy"></li>
<li>key为name，value为张三的节点移动结束后，继续移动第一个数组dictht[0]的下标为0的后续节点，移动步骤和上面相同。<br>
<img src="https://q456qq520.github.io/post-images/1676627816371.png" alt="" loading="lazy"></li>
<li>继续移动第一个数组dictht[0]的下标为0的后续节点都移动完了，开始移动下标为1的节点，发现其没有数据，所以移动下标为2的节点，同时修改rehashidx为2，移动步骤和上面相同。<br>
<img src="https://q456qq520.github.io/post-images/1676627822899.png" alt="" loading="lazy"><br>
整个过程的重点在于rehashidx，其为第一个数组正在移动的下标位置，如果当前内存不够，或者操作系统繁忙，扩容的过程可以随时停止。</li>
</ol>
<p>停止之后如果对该对象进行操作，那是什么样子的呢？<br>
如果是新增，则直接新增后第二个数组，因为如果新增到第一个数组，以后还是要移过来，没必要浪费时间<br>
如果是删除，更新，查询，则先查找第一个数组，如果没找到，则再查询第二个数组。</p>
<h3 id="49redis性能有问题如何分析缓存一致性问题删除大key会很慢吗">49.[redis]性能有问题如何分析，缓存一致性问题，删除大key会很慢吗？</h3>
<p><code>缓存一致性问题</code><br>
把 Redis 作为缓存的时候，当数据发生改变我们需要双写来保证缓存与数据库的数据一致，重点是写操作，数据库和缓存都需要修改，而两者就会存在一个先后顺序，可能会导致数据不再一致。</p>
<p>我们需要考虑两个问题：</p>
<ul>
<li>先更新缓存还是更新数据库？</li>
<li>当数据发生变化时，选择修改缓存（update），还是删除缓存（delete）？</li>
</ul>
<p>将这两个问题排列组合，会出现四种方案：</p>
<ol>
<li>先更新缓存，再更新数据库；</li>
<li>先更新数据库，再更新缓存；</li>
<li>先删除缓存，再更新数据库；</li>
<li>先更新数据库，再删除缓存。</li>
</ol>
<p><strong>一致性解决方案</strong></p>
<ol>
<li>
<p>缓存延时双删<br>
先删除缓存、再写数据库。最后休眠 x 毫秒，再删除缓存。延迟时间的目的就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。</p>
</li>
<li>
<p>删除缓存重试机制</p>
</li>
</ol>
<blockquote>
<p>缓存删除失败怎么办？比如延迟双删的第二次删除失败，那岂不是无法删除脏数据。<br>
使用重试机制，保证删除缓存成功。</p>
</blockquote>
<ol start="3">
<li>读取 binlog 异步删除</li>
</ol>
<ul>
<li>更新数据库；</li>
<li>数据库会把操作信息记录在 binlog 日志中；</li>
<li>使用 canal 订阅 binlog 日志获取目标数据和 key；</li>
<li>缓存删除系统获取 canal 的数据，解析目标 key，尝试删除缓存。</li>
<li>如果删除失败则将消息发送到消息队列；</li>
<li>缓存删除系统重新从消息队列获取数据，再次执行删除操作。</li>
</ul>
<p><code>redis删除大key</code><br>
因为大key的删除会造成阻塞。阻塞期间，所有请求都可能造成超时，当超时越来越多，新的请求不断进来，这样会造成redis连接池耗尽，尽而引发线上各种依赖redis的业务出现异常。</p>
<p><strong>解决办法</strong></p>
<ol>
<li>低峰期删除</li>
<li>scan分批</li>
<li>异步删除
<ul>
<li>redis提供了del的替代方法unlink，当我们在unlink的时候，redis会先检查要删除元素的个数（比如集合），如果集合的元素的小于等于64个的时候，就会直接执行同步删除，因为这不算一个大key，不会浪费很多的开销，但是当超过64个的时候，redis会认为是大key的概率比较大，这时候redis会在字典里，先把key删除，真正的value会交给异步线程来操作，这样的话就不会对主线程造成任何影响。</li>
</ul>
</li>
</ol>
<p><a href="https://help.aliyun.com/document_detail/353223.html" title="发现并处理Redis的大Key和热Key">发现并处理Redis的大Key和热Key</a></p>
<h2 id="mysql">mysql</h2>
<h3 id="1-mysql的逻辑架构图一个sql是如何执行的-分析器优化器执行器存储引擎">1. Mysql的逻辑架构图，一个sql是如何执行的。分析器，优化器，执行器，存储引擎</h3>
<ol>
<li>语法分析<br>
当客户端发送一个SQL语句给MySQL服务器时，MySQL服务器会首先对这个SQL语句进行语法分析，检查语句是否符合MySQL语法规范。如果语句存在语法错误，MySQL服务器将返回相应的错误信息给客户端。</li>
<li>语义分析<br>
如果SQL语句通过了语法分析，MySQL服务器会对语句进行语义分析，检查语句中使用的数据库对象是否存在、权限是否足够等。如果语句存在语义错误，MySQL服务器将返回相应的错误信息给客户端。</li>
<li>查询优化器<br>
MySQL服务器会使用查询优化器来分析SQL语句，选择最优的执行计划。查询优化器会考虑多种因素，例如索引使用情况、表连接顺序、子查询展开等，以尽可能地提高查询性能。</li>
<li>执行计划生成<br>
查询优化器会生成一个执行计划，告诉MySQL服务器如何执行SQL语句。执行计划通常包括以下几个步骤：<br>
a. 获取数据表<br>
MySQL服务器会获取SQL语句中所涉及到的数据表，以便之后的查询操作。<br>
b. 进行数据过滤<br>
如果SQL语句包含WHERE子句，MySQL服务器会根据WHERE条件对数据进行过滤，以减少查询的数据量。<br>
c. 进行数据排序<br>
如果SQL语句包含ORDER BY子句，MySQL服务器会对查询结果进行排序，以满足排序需求。<br>
d. 进行数据聚合<br>
如果SQL语句包含GROUP BY子句，MySQL服务器会对查询结果进行聚合操作，以统计数据。<br>
e. 进行数据连接<br>
如果SQL语句包含JOIN子句，MySQL服务器会对数据表进行连接操作，以满足查询需求。</li>
<li>执行SQL语句<br>
MySQL服务器会根据执行计划，执行SQL语句并返回结果给客户端。在执行SQL语句时，MySQL服务器会使用缓存、锁机制等技术来提高查询性能和保证数据的一致性。</li>
</ol>
<h3 id="2-innodb-和-myisam-对比-支持事务行锁外键">2. InnoDB 和 MyISAM 对比。支持事务，行锁，外键。</h3>
<ol>
<li>数据库事务<br>
InnoDB支持事务处理，而MyISAM不支持。事务是数据库中非常重要的特性，它可以保证数据的完整性和一致性，而且可以实现对数据的并发访问控制。</li>
<li>表锁与行锁<br>
InnoDB使用行级锁来保证数据的并发访问，而MyISAM使用表级锁。这意味着在使用InnoDB时，多个用户可以同时访问同一张表的不同行，而在使用MyISAM时，多个用户同时访问同一张表的不同行会导致性能下降。</li>
<li>索引方式<br>
InnoDB和MyISAM的索引方式也不同。InnoDB使用B+树索引，支持自适应哈希索引和全文索引，而MyISAM使用B树索引，只支持前缀索引和全文索引。因此，在需要进行大量全文搜索的应用中，MyISAM的性能可能更优。</li>
<li>外键约束<br>
InnoDB支持外键约束，而MyISAM不支持。外键约束可以保证数据的完整性，但也会影响性能。</li>
<li>空间占用<br>
InnoDB的空间占用较大，因为它需要存储多个版本的数据，以支持事务和MVCC（多版本并发控制）。而MyISAM的空间占用较小，因为它只存储一份数据。</li>
<li>崩溃恢复<br>
InnoDB具有更好的崩溃恢复能力，可以在恢复期间自动回滚未提交的事务。而MyISAM的崩溃恢复能力较差，可能会导致数据丢失或损坏。</li>
</ol>
<p>综上所述，InnoDB和MyISAM在性能、特性、空间占用、崩溃恢复等方面都有所差异。因此，在选择存储引擎时，需要根据应用程序的特性和需求来选择合适的存储引擎。例如，如果应用程序需要支持事务处理，那么InnoDB是更好的选择；如果应用程序需要进行大量全文搜索，那么MyISAM可能更适合。</p>
<h3 id="3-buffer-pool-作用是什么">3. Buffer Pool 作用是什么？</h3>
<p>Buffer Pool是MySQL中的一个缓存池，用于存储数据表和索引的数据页，其作用是加快数据库的访问速度，提高数据库的性能和响应速度。</p>
<p>当MySQL需要读取或写入数据时，它首先会从Buffer Pool中查找所需的数据页。如果数据页已经在Buffer Pool中，则可以直接从内存中获取数据，避免了磁盘I/O操作，大大提高了数据库的访问速度。如果数据页不在Buffer Pool中，则MySQL会从磁盘中读取数据，然后将其放入Buffer Pool中，以便下次访问时可以直接从内存中获取数据。</p>
<p>通过使用Buffer Pool，MySQL可以将常用的数据表和索引数据页存储在内存中，避免了频繁的磁盘I/O操作，从而大大提高了数据库的性能和响应速度。此外，Buffer Pool还可以用于管理内存使用情况，自动调整内存分配大小，以最大限度地利用可用内存，提高数据库的并发处理能力和性能表现。</p>
<h3 id="4-buffer-pool-如何管理free链表flush链表lru链表">4. Buffer Pool 如何管理？free链表，flush链表，lru链表</h3>
<p>Buffer Pool是MySQL中的一个缓存池，用于存储数据表和索引的数据页，其管理过程包括以下几个方面：</p>
<p>内存分配：Buffer Pool需要占用一定的内存空间来存储数据页。在MySQL启动时，可以通过参数配置来设置Buffer Pool的大小，或者使用默认值。MySQL使用操作系统的内存管理机制来分配和管理Buffer Pool的内存，根据需要动态调整内存大小。</p>
<p>数据页读取和写入：当MySQL需要读取或写入数据时，它会首先检查Buffer Pool中是否存在所需的数据页。如果数据页已经在Buffer Pool中，则可以直接从内存中读取或写入数据。如果数据页不在Buffer Pool中，则需要从磁盘中读取或写入数据，然后将其放入Buffer Pool中。</p>
<p>数据页替换：Buffer Pool的大小是有限的，如果所有的数据页都已经被占满，MySQL需要从Buffer Pool中删除一些数据页，以便为新的数据页腾出空间。MySQL使用一种称为LRU（Least Recently Used，最近最少使用）的算法来管理Buffer Pool中的数据页，即删除最久未使用的数据页。</p>
<p>统计信息：MySQL会定期收集和记录Buffer Pool的统计信息，如Buffer Pool的使用情况、缓存命中率、读取和写入操作的数量等。这些统计信息可以帮助MySQL优化Buffer Pool的性能和配置，以提高数据库的性能和响应速度。</p>
<p>通过对Buffer Pool进行有效的管理和优化，可以最大限度地提高MySQL的性能和响应速度，避免磁盘I/O操作，加快数据访问速度。</p>
<p>当我们最初启动MySQL服务器的时候，需要完成对Buffer Pool的初始化过程，就是先向操作系统申请Buffer Pool的内存空间，然后把它划分成若干对控制块和缓存页。但是此时并没有真实的磁盘页被缓存到Buffer Pool中（因为还没有用到），之后随着程序的运行，会不断的有磁盘上的页被缓存到Buffer Pool中。那么问题来了，从磁盘上读取一个页到Buffer Pool中的时候该放到哪个缓存页的位置呢？或者说怎么区分Buffer Pool中哪些缓存页是空闲的，哪些已经被使用了呢？我们最好在某个地方记录一下Buffer Pool中哪些缓存页是可用的，这个时候缓存页对应的控制块就派上大用场了，我们可以把所有空闲的缓存页对应的控制块作为一个节点放到一个链表中，这个链表也可以被称作free链表（或者说空闲链表）。</p>
<p>凡是修改过的缓存页对应的控制块都会作为一个节点加入到一个链表中，因为这个链表节点对应的缓存页都是需要被刷新到磁盘上的，所以也叫flush链表。</p>
<p>在MySQL的Buffer Pool中，所有的数据页都被组织成一个双向链表，称为LRU链表。每当一个数据页被访问时，它就会被移到链表头部。当需要腾出空间时，缓存管理器会从链表尾部开始，依次删除最老的、最近最少使用的数据页，直到腾出足够的空间为止。</p>
<h3 id="5-redo-log-作用是什么">5. redo log 作用是什么？</h3>
<p>在MySQL中，Redo Log（重做日志）是一种用于保证数据持久性的机制，它可以记录所有的数据修改操作，包括对数据的插入、修改和删除等操作。</p>
<p>Redo Log的作用在于当数据库出现异常宕机或者故障时，可以通过Redo Log中的信息将未持久化的数据重新恢复到宕机前的状态，从而保证数据库的数据一致性。当MySQL启动时，会首先将Redo Log中的数据恢复到内存中，然后再读取数据文件中的数据，这样就可以保证数据的完整性和一致性。</p>
<p>具体来说，当用户对数据库进行修改时，MySQL会将修改操作记录在Redo Log中，记录的信息包括修改的数据页号、修改的位置、修改前后的值等。在执行完修改操作后，MySQL会将Redo Log中的记录持久化到磁盘中，保证数据的可靠性。在MySQL将修改操作写入磁盘之前，即使数据库出现异常宕机，也可以通过Redo Log中的信息将数据恢复到修改前的状态。</p>
<p>需要注意的是，Redo Log只记录数据的修改操作，不记录查询操作。而且，Redo Log的记录方式是追加式的，即每次写入Redo Log时都会将新的记录追加到文件的末尾，而不会覆盖已有的记录。因此，Redo Log的大小会随着数据库的使用而不断增加，需要定期清理和维护，以避免对磁盘空间的过度占用。同时，为了保证数据的可靠性，Redo Log的写入操作也需要在磁盘I/O完成后才能返回给客户端，因此对数据库的写入性能会产生一定的影响。</p>
<h3 id="6-redo-log-啥时候刷盘定时刷盘-事务提交时刷盘-checkponin时-关闭服务时">6. redo log 啥时候刷盘，定时刷盘、事务提交时刷盘、checkponin时、关闭服务时</h3>
<h3 id="7-redo-log-日志的存储数据结构-多个文件轮流写入">7. 	redo log 日志的存储数据结构。多个文件轮流写入</h3>
<h3 id="8-redo-log-日志什么时候可以清楚或者覆盖checkpoint-作用是什么">8. 	redo log 日志什么时候可以清楚或者覆盖？checkpoint 作用是什么？</h3>
<h3 id="9-系统崩溃恢复后是如何从-redo-log-中恢复数据的">9. 系统崩溃恢复后，是如何从 redo log 中恢复数据的</h3>
<h3 id="10-undo-log-作用是什么事务回滚">10. undo log 作用是什么？事务回滚</h3>
<h3 id="11-隔离级别有哪些读未提交读已提交可重复读序列化">11. 隔离级别有哪些，读未提交，读已提交，可重复读，序列化</h3>
<h3 id="12-不可重复读和幻读是如何区分">12. 不可重复读和幻读是如何区分</h3>
<p>不可重复读和幻读是两种并发读取数据时可能出现的问题，它们的区别在于对数据的修改操作。</p>
<p>不可重复读指的是，在一个事务中多次读取同一份数据，但在此过程中，其他事务修改了该数据，导致多次读取的结果不同。这种情况下，每次读取的数据都是有效的，但由于其他事务的修改，数据的值发生了改变，因此多次读取得到的结果不同。不可重复读通常可以通过MVCC机制来解决，即在读取时只能读取早于该事务ID的版本。</p>
<p>幻读指的是，在一个事务中多次读取同一份数据，但在此过程中，其他事务插入或删除了该数据，导致多次读取的结果不同。这种情况下，每次读取的数据都是有效的，但由于其他事务的插入或删除，数据的数量发生了改变，因此多次读取得到的结果不同。幻读通常可以通过锁机制来解决，即在读取时对该数据行进行加锁，以保证数据的完整性。</p>
<p>因此，不可重复读和幻读的区别在于对数据的修改操作。<mark>不可重复读是由其他事务对数据进行修改导致的，而幻读是由其他事务对数据进行插入或删除导致的</mark>。在解决这两种问题时，可以采用不同的并发控制策略，如MVCC机制和锁机制等。</p>
<h3 id="13-mvcc-作用是什么可以在哪个隔离级别下工作">13. MVCC 作用是什么？可以在哪个隔离级别下工作？</h3>
<p>MVCC（Multi-Version Concurrency Control，多版本并发控制）是一种数据库并发控制机制，常用于支持事务和保证数据的一致性。它的主要作用是在数据库支持并发读写的同时，保证读写操作的正确性和数据的一致性。</p>
<p>在MVCC机制下，每个数据行都有一个版本号，表示该数据行的历史版本。当一个事务开始时，它会获取一个唯一的事务ID，并在整个事务过程中保持不变。在写入数据时，数据库会保存一个该数据的版本号，同时在每个事务中，读取操作只能读取到早于该事务ID的版本。这样，即使多个事务并发读写同一份数据，它们读取到的都是数据的旧版本，不会互相影响，保证了数据的一致性。</p>
<p>当多个事务同时访问同一个数据行时，MVCC机制采用了两种不同的策略：一是在写操作时对该数据行进行加锁，以保证数据的正确性和一致性；二是采用乐观并发控制策略，即不对数据行进行加锁，而是在写操作提交时进行冲突检测。如果发现冲突，则回滚该事务，重新执行操作。</p>
<p>总的来说，MVCC机制可以提高数据库的并发处理能力和数据的一致性，对于支持事务和并发读写的数据库系统来说，是非常重要的机制。</p>
<h3 id="14-当前读-快照读-是怎么区分的举个查询的例子102050三个进行中的事务当前事务id是30">14. 当前读、快照读。是怎么区分的？举个查询的例子，10，20，50三个进行中的事务，当前事务id是30</h3>
<h3 id="15-innodb-下索引存储的数据结构">15. innodb 下，索引存储的数据结构</h3>
<h3 id="16-innodb-下聚簇索引和非聚簇索引有什么区别">16. innodb 下，聚簇索引和非聚簇索引有什么区别？</h3>
<p>在InnoDB存储引擎下，聚簇索引和非聚簇索引是两种常见的索引类型，它们在索引的存储方式和查询效率上存在一些区别。</p>
<p>聚簇索引<br>
聚簇索引是指索引的顺序与数据存储的顺序相同，也就是说，聚簇索引的叶子节点存储了整个数据行的信息，包括所有的列。在InnoDB中，每张表只能有一个聚簇索引，它默认是以主键作为聚簇索引的。<br>
由于聚簇索引的叶子节点存储了整个数据行的信息，因此可以通过聚簇索引直接查询到需要的数据，无需再通过数据页来获取数据，从而提高了查询的效率。另外，由于数据行是按照聚簇索引的顺序存储的，因此可以利用聚簇索引实现基于范围的查询（例如 BETWEEN 和 ORDER BY）。</p>
<p>非聚簇索引<br>
非聚簇索引是指索引的顺序与数据存储的顺序不同，也就是说，非聚簇索引的叶子节点只存储了索引列和主键列，需要通过主键索引再查找数据行。在InnoDB中，每张表可以有多个非聚簇索引。<br>
由于非聚簇索引的叶子节点只存储了索引列和主键列，因此需要再通过主键索引来查找数据行，从而降低了查询效率。另外，由于数据行是按照主键索引的顺序存储的，因此不能利用非聚簇索引实现基于范围的查询，而只能实现基于索引列的查询。但是，非聚簇索引相比于聚簇索引可以更加节省存储空间，因为非聚簇索引只存储了索引列和主键列。</p>
<p>需要注意的是，InnoDB使用了MVCC（多版本并发控制）机制来实现数据的并发访问和事务隔离。由于聚簇索引存储了整个数据行的信息，因此对于同一行的多个版本，InnoDB会将它们存储在同一个数据页中，并通过额外的指针来指向不同的版本。而对于非聚簇索引，由于它只存储了索引列和主键列，因此在使用MVCC机制时，InnoDB需要将所有的版本都存储在不同的数据页中，从而增加了存储和查询的成本。</p>
<h3 id="7-innodb-下btree特点非叶子节点只保存key及指针叶子节点只保存data叶子节点有双向指针">7. innodb 下，B+TREE特点：非叶子节点只保存key及指针，叶子节点只保存data，叶子节点有双向指针</h3>
<p>InnoDB是MySQL的一种存储引擎，其默认使用B+Tree数据结构作为索引类型。B+Tree有以下特点：</p>
<p>多级索引：B+Tree是一种多级索引结构，它可以支持大量数据的高效查询和插入，而且查询性能基本不受数据规模的影响。</p>
<p>聚簇索引：InnoDB的B+Tree是聚簇索引，即将数据行存放在B+Tree的叶子节点上，这样可以避免多次磁盘IO操作，提高查询效率。</p>
<p>顺序访问：B+Tree支持有序访问，也就是说在B+Tree中相邻的节点都是相邻的数据块，这样就可以利用磁盘预读技术提高查询效率。</p>
<p>索引组织表：InnoDB中的表是索引组织表，也就是说每个表都必须有主键，主键将作为B+Tree的索引键。</p>
<p>自适应哈希索引：InnoDB支持自适应哈希索引，它可以根据查询频率自动将经常使用的B+Tree节点转化为哈希索引，提高查询效率。</p>
<p>综上所述，B+Tree是一种高效的索引结构，适合大规模数据的高效查询和插入，而且支持多种优化技术，如聚簇索引、有序访问和自适应哈希索引等。</p>
<h3 id="18-innodb-下btree有什么优势解决高度问题减少随机读-排序问题">18. innodb 下，B+TREE有什么优势？解决高度问题，减少随机读、排序问题</h3>
<p>在InnoDB存储引擎下，使用B+Tree索引结构有以下优势：</p>
<p>高效的范围查询：B+Tree索引支持范围查询，可以快速找到某个范围内的数据，适合处理复杂的查询条件。</p>
<p>聚簇索引提高查询效率：InnoDB中的B+Tree是聚簇索引，将数据行存放在B+Tree的叶子节点上，可以减少IO次数，提高查询效率。</p>
<p>顺序访问提高查询效率：B+Tree支持有序访问，也就是说在B+Tree中相邻的节点都是相邻的数据块，可以利用磁盘预读技术提高查询效率。</p>
<p>自适应哈希索引提高查询效率：InnoDB支持自适应哈希索引，可以根据查询频率自动将经常使用的B+Tree节点转化为哈希索引，提高查询效率。</p>
<p>支持高并发：B+Tree索引结构支持高并发访问，多个用户同时对数据库进行查询和修改操作时，B+Tree可以保证数据的一致性和可靠性。</p>
<p>综上所述，B+Tree索引结构在InnoDB存储引擎下具有高效的范围查询、聚簇索引、顺序访问、自适应哈希索引和高并发访问等优势，适合处理大量数据的高效查询和插入。</p>
<h3 id="19-覆盖索引是什么">19. 覆盖索引是什么？</h3>
<p>MySQL的覆盖索引（Covering Index）是指一个查询可以通过索引就能够满足查询的需要，而无需访问数据表。当查询需要访问的列都在索引中时，查询就可以使用覆盖索引，避免了访问数据表，从而提高了查询的性能。</p>
<p>覆盖索引的优点是可以减少磁盘IO，因为查询只需要读取索引而不需要读取数据表，可以节省磁盘IO的时间和资源。此外，覆盖索引可以避免排序和临时表的使用，因为所有需要的数据都已经在索引中。</p>
<p>覆盖索引的缺点是对索引的限制较大，需要查询的所有列都必须在索引中出现，否则无法使用覆盖索引。此外，覆盖索引对更新操作的影响也需要注意，因为索引中的数据会随着数据表的更新而变化，可能会影响索引的效率。</p>
<p>总之，覆盖索引可以提高查询的性能，减少磁盘IO，但需要满足一定的限制，如所有需要查询的列都必须在索引中出现，并且需要注意更新操作的影响。</p>
<h3 id="20-索引下推怎么理解">20. 索引下推怎么理解</h3>
<p>MySQL索引下推（Index Condition Pushdown）是一种查询优化技术，它可以在使用索引的同时，对查询条件进行筛选，从而减少访问数据表的次数，提高查询性能。</p>
<p>具体来说，索引下推是指将原本在数据表上执行的条件判断推到索引层面进行处理，这样可以减少访问数据表的次数，提高查询性能。在查询语句中使用索引时，MySQL会将索引上的条件筛选出来，然后将剩余的条件再在数据表上进行筛选。如果索引层面能够处理掉一些条件，就可以减少访问数据表的次数。</p>
<p>举个例子，假设有一个包含多个列的复合索引，查询语句中包含了多个筛选条件，其中一部分条件可以在索引层面处理掉，那么MySQL就可以使用索引下推来优化查询。具体操作过程如下：</p>
<ol>
<li>MySQL首先通过索引快速定位到数据行；</li>
<li>然后对索引中的条件进行判断，将能够处理的条件筛选出来；</li>
<li>最后将剩余的条件再在数据表上进行筛选。</li>
</ol>
<p>使用索引下推可以减少访问数据表的次数，提高查询性能。但需要注意，索引下推并不是适用于所有情况，需要根据实际情况进行判断和使用。同时，索引下推可能会导致索引的失效，需要注意优化查询语句的方式和条件的组合。</p>
<h3 id="21-innodb下行锁-gap锁next-key锁-是怎么区分的">21. innodb下，行锁、gap锁，next-key锁。是怎么区分的？</h3>
<h3 id="22-innodb下next-key锁可以解决什么问题可以解决幻读吗">22. 	innodb下，next-key锁可以解决什么问题？可以解决幻读吗？</h3>
<h3 id="23-自增主键有哪几种策略">23. 自增主键有哪几种策略？</h3>
<h3 id="24-并发场景下自增主键为什么会有间隙主键冲突-回滚-bulk-inserts">24. 并发场景下，自增主键为什么会有间隙？主键冲突、回滚、Bulk inserts</h3>
<h3 id="25-关联查询是怎么执行-nested-loop-join-a-left-join-b-where-abid-bid">25. 关联查询是怎么执行。Nested-Loop Join。a left join b where a.bid = b.id</h3>
<h3 id="26-执行计划里面的-type-有哪些system-const-eq_ref-ref-range-index-all">26. 执行计划里面的 type 有哪些？system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL</h3>
<h3 id="27-执行计划里面的-extra-有哪些需要关注的值using-indexusing-index-conditionusing-filesort">27. 执行计划里面的 Extra 有哪些需要关注的值？Using index，using index condition，Using filesort</h3>
<h3 id="28-一条sql的执行成本怎么计算-cpu1行数据io一个块">28. 一条sql的执行成本怎么计算？ cpu(1行数据)，io(一个块)</h3>
<h3 id="29-主从同步逻辑一般什么情况下会导致主从延迟">29. 	主从同步逻辑，一般什么情况下会导致主从延迟？</h3>
<h3 id="30-binlog是如何工作的-有哪几种模式-rowstatementmixedlevel">30. binlog是如何工作的。有哪几种模式。row，statement，Mixedlevel</h3>
<h3 id="31-binlog的刷盘策略有哪些">31. binlog的刷盘策略有哪些。</h3>
<h3 id="32-innodb-下mysql的一行记录是如何存储的有哪些隐藏字段row_idtrx_idroll_pointer">32. innodb 下，mysql的一行记录是如何存储的？有哪些隐藏字段：row_id,trx_id,roll_pointer</h3>
<h3 id="33-工作经验sql调优经验">33. 工作经验，sql调优经验？</h3>
<h2 id="elasticsearch">ElasticSearch</h2>
<h3 id="1-elasticsearch-是什么它的主要特点是什么">1. Elasticsearch 是什么？它的主要特点是什么？</h3>
<p>Elasticsearch 是一个开源的分布式搜索和分析引擎。它使用 Lucene 作为底层引擎，可以处理大量的数据，并且支持实时搜索和分析。Elasticsearch 的主要特点包括：</p>
<p>分布式：数据可以分布在不同的节点上，并且可以自动进行数据分片和副本；<br>
实时性：可以实时索引和搜索数据；<br>
灵活性：支持多种数据类型、文本处理、地理位置等；<br>
易用性：提供了 RESTful API、客户端库等多种接口，易于使用和集成；<br>
可扩展性：可以通过添加节点和分片来水平扩展。</p>
<h3 id="2-elasticsearch-的数据模型是什么">2. Elasticsearch 的数据模型是什么？</h3>
<p>Elasticsearch 的数据模型是文档-索引-类型。文档是存储在 Elasticsearch 中的基本数据单元，它由多个字段组成。每个文档都属于一个索引（index），而每个索引可以包含多个类型（type），每个类型又包含多个文档。每个文档都有一个唯一的 ID，用于在索引中进行唯一标识。</p>
<h3 id="3-什么是分片和副本">3. 什么是分片和副本？</h3>
<p>分片（Shard）是 Elasticsearch 中数据的基本单位，用于将数据分散存储在不同的节点上，以便实现数据的水平扩展。每个索引都可以划分为多个分片，每个分片可以存储一部分数据。分片的数量和大小可以根据数据量和性能需求进行调整。</p>
<p>副本（Replica）是 Elasticsearch 中数据的备份，用于提高数据的可靠性和可用性。每个分片可以有多个副本，副本分布在不同的节点上，以实现数据的冗余备份。副本也可以提高搜索的性能，因为可以将搜索请求分发到不同的节点上进行并行处理。</p>
<h3 id="4-elasticsearch-的搜索过程是怎样的">4. Elasticsearch 的搜索过程是怎样的？</h3>
<p>Elasticsearch 的搜索过程可以分为以下几个步骤：</p>
<p>客户端发送搜索请求：客户端向 Elasticsearch 发送一个搜索请求，包含搜索条件、索引和类型等信息；<br>
Coordinating 节点的处理：Elasticsearch 会选择一个 Coordinating 节点来处理搜索请求，该节点负责协调搜索过程中的各个节点；<br>
Querying 节点的处理：Coordinating 节点向对应的 Querying 节点发送查询请求，Querying 节点根据请求生成倒排索引，并返回匹配结果；<br>
数据合并和排序：Coordinating 节点会将各个节点返回的结果进行合并和排序，生成最终的结果集；<br>
返回结果：最终的</p>
<h3 id="5-什么是聚合aggregation">5. 什么是聚合（Aggregation）？</h3>
<p>聚合是 Elasticsearch 中一种高级的数据分析方法，用于对数据进行分组、统计、计算等操作，以便得出更全面、更深入的数据分析结果。聚合操作可以在查询请求中定义，可以对一个或多个字段进行聚合操作，支持多种聚合方式，如统计、分组、排序、过滤、嵌套等。</p>
<h3 id="6-elasticsearch-的查询语句是什么样的">6. Elasticsearch 的查询语句是什么样的？</h3>
<p>Elasticsearch 的查询语句使用 JSON 格式，主要包含以下几个部分：</p>
<ul>
<li>Query：用于指定查询类型和查询条件，如匹配、范围、布尔、聚合等；</li>
<li>Filter：用于指定过滤条件，它可以提高搜索性能，因为它不会计算相关性得分；</li>
<li>Sort：用于指定排序规则，可以按照字段值、文档得分、距离等排序；</li>
<li>Aggregations：用于指定聚合操作，可以对搜索结果进行分组、统计、计算等操作；</li>
<li>Highlight：用于指定关键词高亮显示的样式和位置；</li>
<li>Source：用于指定搜索结果的字段列表，可以控制返回的字段数量和内容。</li>
</ul>
<h3 id="7-elasticsearch-的分布式架构是如何保证数据的一致性和可靠性">7. Elasticsearch 的分布式架构是如何保证数据的一致性和可靠性？</h3>
<p>Elasticsearch 的分布式架构采用了多种技术来保证数据的一致性和可靠性，包括：</p>
<ol>
<li>分片和副本：数据被分散存储在不同的节点上，每个分片可以有多个副本，以实现数据的冗余备份和高可用性；</li>
<li>网络通信协议：Elasticsearch 使用 TCP/IP 协议进行节点间的通信，通过多播和单播技术保证数据传输的可靠性和稳定性；</li>
<li>集群状态管理：Elasticsearch 通过集群状态管理机制来检测和处理节点故障、数据丢失等问题，可以自动进行节点重分配、副本重建等操作；</li>
<li>其他技术：Elasticsearch 还采用了分片路由、节点选举、分片分配策略等技术来保证数据的一致性和可靠性。</li>
</ol>
<h3 id="8-什么是-elasticsearch-中的映射mapping">8. 什么是 Elasticsearch 中的映射（Mapping）？</h3>
<p>映射是 Elasticsearch 中定义数据类型的方法，它类似于关系数据库中的表结构。映射用于定义索引中的字段类型、分词器、存储方式、属性等信息。在索引创建之前，需要定义映射信息，以便 Elasticsearch 正确地解析和处理索引中的文档数据。</p>
<h3 id="9-elasticsearch-中的分片是如何工作的">9. Elasticsearch 中的分片是如何工作的？</h3>
<p>Elasticsearch 中的分片是将索引分成多个部分，每个部分称为一个分片。每个分片可以分布在不同的节点上，可以同时处理搜索请求和索引请求。分片的数量可以在索引创建时指定，一般情况下，建议将索引分片数设置为节点数的倍数，以充分利用分布式架构的优势。分片的工作原理是将搜索请求和索引请求路由到对应的分片上，进行处理和返回结果。</p>
<h3 id="10">10.</h3>
<h3 id="11">11.</h3>
<h2 id="算法-工具">算法、工具</h2>
<h3 id="1-布隆过滤器能解决什么问题">1. 布隆过滤器能解决什么问题？</h3>
<p>布隆过滤器解决缓存穿透问题。</p>
<p>使用布隆过滤器逻辑如下：</p>
<ol>
<li>根据 key 查询缓存，如果存在对应的值，直接返回；如果不存在则继续执行</li>
<li>根据 key 查询缓存在布隆过滤器的值，如果存在值，则说明该 key 不存在对应的值，直接返回空，如果不存在值，继续向下执行</li>
<li>查询 DB 对应的值，如果存在，则更新到缓存，并返回该值，如果不存在值，则更新到布隆过滤器中，并返回空</li>
</ol>
<h3 id="2-布隆过滤器实现原理及引发的问题">2. 布隆过滤器实现原理及引发的问题</h3>
<p>布隆过滤器的原理是，当一个元素被加入集合时，通过 K 个散列函数将这个元素映射成一个位数组中的 K 个点（offset），把它们置为 1。检索时，我们只要看看这些点是不是都是 1 就（大约）知道集合中有没有它了：如果这些点有任何一个 0，则被检元素一定不在；如果都是 1，则被检元素很可能在。这就是布隆过滤器的基本思想。</p>
<p>简单来说就是准备一个长度为 m 的位数组并初始化所有元素为 0，用 k 个散列函数对元素进行 k 次散列运算跟 len (m) 取余得到 k 个位置并将 m 中对应位置设置为 1。</p>
<figure data-type="image" tabindex="8"><img src="https://q456qq520.github.io/post-images/1676178253605.png" alt="" loading="lazy"></figure>
<p>布隆过滤器优缺点<br>
优点：</p>
<ul>
<li>空间占用极小，因为本身不存储数据而是用比特位表示数据是否存在，某种程度有保密的效果。</li>
<li>插入与查询时间复杂度均为 O (k)，常数级别，k 表示散列函数执行次数。</li>
<li>散列函数之间可以相互独立，可以在硬件指令层加速计算。</li>
</ul>
<p>缺点：</p>
<ul>
<li>误差（假阳性率）。算法判断key在集合中时，有一定的概率key其实不在集合中。<br>
布隆过滤器可以 100% 判断元素不在集合中，但是当元素在集合中时可能存在误判，因为当元素非常多时散列函数产生的 k 位点可能会重复。</li>
<li>无法删除。</li>
</ul>
<h3 id="3-令牌桶算法-漏斗算法-固定窗口算法滑动窗口算法">3. 	令牌桶算法、漏斗算法、固定窗口算法，滑动窗口算法</h3>
<p><code>固定时间窗口</code><br>
所谓时间窗口限流，是指在一定的时间内，维护一个访问总量的数值，当其超过阈值时，拒绝后续所有的请求，直到进入下一个时间窗口。<br>
<img src="https://q456qq520.github.io/post-images/1676359072929.png" alt="" loading="lazy"><br>
但是，这种算法有一个很明显的临界问题：假设限流阀值为 5 个请求，单位时间窗口是 1s，如果我们在单位时间内的前 0.8-1s 和 1-1.2s，分别并发 5 个请求。虽然都没有超过阀值，但是如果算 0.8-1.2s，则并发数高达 10，已经超过单位时间 1s 不超过 5 阀值的定义了。</p>
<p><code>滑动时间窗口</code><br>
滑动窗口限流可以解决固定窗口临界值的问题。它将单位时间周期分为n个小周期，分别记录每个小周期内接口的访问次数，并且根据时间滑动删除过期的小周期，随着时间流失，最开始的窗口将会失效，但是也会生成新的窗口；<br>
<img src="https://q456qq520.github.io/post-images/1676359259527.png" alt="" loading="lazy"><br>
滑动窗口的格子周期划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确，但是相对的，维护成本也就越高。</p>
<p>滑动时间窗口的创建过程，如下：<br>
1、根据当前时间，算出该时间的timeId，timeId就是在整个时间轴的位置<br>
2、据timeId算出当前时间窗口在采样窗口区间中的索引idx<br>
3、根据当前时间算出当前窗口应该对应的窗口开始时间time，以毫秒为单位<br>
4、循环判断直到获取到一个当前时间窗口<br>
5、根据索引idx，在采样窗口数组中取得一个时间窗口old</p>
<p>假设我们将1s划分为4个窗口，则每个窗口对应250ms。假设恶意用户还是在上一秒的最后一刻和下一秒的第一刻冲击服务，按照滑动窗口的原理，此时统计上一秒的最后750毫秒和下一秒的前250毫秒，这种方式能够判断出用户的访问依旧超过了1s的访问数量，因此依然会阻拦用户的访问。</p>
<p><code>令牌桶算法</code><br>
有一个虚拟的桶，桶里面放有一定数量的Token，请求访问资源之前，需要从桶里拿到令牌，拿不到令牌的请求会被拒绝掉，这就是令牌桶的思想。<br>
<img src="https://q456qq520.github.io/post-images/1676359525629.png" alt="" loading="lazy"></p>
<p>令牌桶算法的实现很轻量级，我们并不需要一个真正的桶，只需要维护以下几个数值，就能在请求到来时计算出是否有足够的Token分配给请求：</p>
<ul>
<li>上一次发出令牌的时间</li>
<li>令牌的生产速度</li>
<li>上次剩下的令牌数</li>
<li>桶的容量</li>
</ul>
<p><code>漏桶算法</code><br>
漏桶算法的算法原理是，设置一个漏桶，每次请求都将请求放入到漏桶当中，若漏桶已满则拒绝请求，漏桶按照一定速率将已放入漏桶的请求流出，流出的请求将被正常处理。</p>
<p>漏桶算法面对限流时，可以缓存一定的请求，不用直接粗暴拒绝（消息队列的限流本质上就是漏桶算法）。<br>
<img src="https://q456qq520.github.io/post-images/1676359604984.png" alt="" loading="lazy"></p>
<p>令牌桶与漏桶相比，本质的区别是没有一个队列来缓存请求，在更轻量级的同时也只能粗暴的直接舍弃请求。</p>
<h3 id="4-字典表普通hash取模一致性hash算法hash-slot算法hash槽">4. 字典表，普通hash取模，一致性hash算法，hash slot算法（hash槽）</h3>
<p><code>hash算法</code><br>
hash算法的话，主要是对一个key计算hash值，然后再对节点数量取模，映射到某个节点上。</p>
<p><code>一致性hash算法</code><br>
一致性hash的底层结构是一个环，环上有2的32次方个点，即0、1、2、4、8 、...、 2^32-1，环上的每一个点都有一个hash值，圆环上放着不同的节点机器。然后根据数据的Key值计算得到其Hash值（其分布也为[0, 2^32-1]），接着在Hash环上顺时针查找距离这个Key值的Hash值最近的服务器节点，完成Key到服务器的映射查找。</p>
<p>一致性hash的步骤：</p>
<ol>
<li>计算key的hash值</li>
<li>用上一步的值 % (2^32)，用于确保key能映射到环上的某一个点（避免映射到环外），即某个key在环上对应的点是：hash(服务器的IP地址) % 2^32</li>
<li>key落到圆环上以后，就会按照顺时针寻找距离自己最近的一个节点。<br>
<img src="https://q456qq520.github.io/post-images/1676448706071.png" alt="" loading="lazy"></li>
</ol>
<blockquote>
<p>假如一台节点机器宕机了，那么原本在那台机器上的数据会受到影响，按照顺时针的方式，之前的节点机器宕机了，就会走到下一台机器上去，而下一台机器上是没有数据的，导致部分流量瞬间涌入数据库，重新建立缓存数据。</p>
</blockquote>
<p>我们的一致性哈希算法是按照顺时针的方式来实现数据分布的，如果某个区间的哈希值比较多，就会导致大量的数据涌入一个节点，就会导致节点的热点问题，从而出现性能瓶颈。</p>
<p>为了解决这个问题，一致性哈希算法采用了“<strong>虚拟节点</strong>”。即在环上均匀生成多个 虚拟节点，后续 请求先找虚拟节点，然后再通过虚拟节点找到对应的真实节点。因此，只要保证虚拟节点是均匀分布的，就可以实现数据均匀分布在不同的节点上。</p>
<p><code>hash slot算法（hash槽）</code><br>
参考redis cluster的hash slot算法。</p>
<ol>
<li>redis cluster有固定的16384个hash slot，对每个key计算CRC16值，然后对16384取模，可以获取key对应的hash slot。</li>
<li>redis cluster中每个master都会持有部分slot，比如有3个master，那么可能每个master持有5000多个hash slot</li>
<li>hash slot使得node的增加和移除很简单，增加一个master，就将其他master的hash slot移动部分过去，减少一个master，就将它的hash slot移动到其他master上去（移动hash slot的成本是非常低的）</li>
</ol>
<h3 id="5-拉链法寻址法再hash线性探测公共溢出区">5. 拉链法，寻址法（再hash，线性探测，公共溢出区）</h3>
<p>根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同。两个不同的输入值，根据同一散列函数计算出的散列值相同的现象叫做碰撞。</p>
<p>常见的Hash函数有以下几个：</p>
<ol>
<li>直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址。</li>
<li>数字分析法：提取关键字中取值比较均匀的数字作为哈希地址。</li>
<li>除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址。</li>
<li>分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。</li>
<li>平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。</li>
<li>伪随机数法：采用一个伪随机数当作哈希函数。</li>
</ol>
<p>衡量一个哈希函数的好坏的重要指标就是发生碰撞的概率以及发生碰撞的解决方案。任何哈希函数基本都无法彻底避免碰撞，常见的解决碰撞的方法有以下几种：</p>
<ol>
<li>开放定址法：<br>
开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。</li>
<li>链地址法<br>
将哈希表的每个单元作为链表的头结点，所有哈希地址为i的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。</li>
<li>再哈希法<br>
当哈希地址发生冲突用其他的函数计算另一个哈希函数地址，直到冲突不再产生为止。</li>
<li>建立公共溢出区<br>
将哈希表分为基本表和溢出表两部分，发生冲突的元素都放入溢出表中。</li>
</ol>
<h3 id="6-paxos角色多每次都二阶段提交准备-提交实现难活锁问题">6. paxos，角色多，每次都二阶段提交（准备、提交），实现难，活锁问题</h3>
<p>在Paxos算法中有三种角色，分别具有三种不同的行为，但很多时候，一个进程可能同时充当着多种角色。<br>
Proposer：提案（Proposal）的提议者。<br>
Acceptor：提案的表决者，是否accept该方案，只有半数以上的Acceptor接受了某提案，那么该提案才会被接收。<br>
Learners：提案的学习者，当提案被选定时，其要执行提案内容。</p>
<p>一个提案的表决者(Acceptor)会存在多个，但是在一个集群中，提议者(Proposer)也可能存在多个，不同的提议者(Proposer)会提出不同的提案。<br>
一致性算法则可以保证如下几点：</p>
<ol>
<li>没有提案被提出则不会有提案被选定。</li>
<li>每个提议者在提出提案时都会首先获取到一个具有全局唯一性的、递增的提案编号N，即在整个集群中石唯一的编号N，然后修改该编号赋予其要提出的提案。</li>
<li>每个表决者在accept某提案之后，会将该提案的编号N记录在本地，这样每个表决者中保存的已经被accept的提案中会存在一个编号最大的提案，其编号假设为maxN，每个表决者仅会accept编号大于自己本地maxN的提案。</li>
<li>众多提案中港最终只能有一个提案被选定。</li>
<li>一旦一个提案被选定，则其他服务器会主动同步(Learn)该提案到本地。</li>
</ol>
<p><strong>算法过程描述</strong><br>
<code>prepare阶段</code></p>
<ol>
<li>提议者(Proposer)准备提交一个编号为N的提议，于是其首先向所有表决者(Acceptor)发送prepare(N)请求，用于试探集群是否支持该编号的提议。</li>
<li>每个表决者(Acceptor)都保存着自己曾经accept过的提议中的最大编号maxN，当一个表决者接收到其他主机发送过来的prepare(N)请求时，其会比较N与maxN的大小关系，有以下两种情况。<br>
若N小于maxN，则说明该提议已经过时，当前表决者采取不回应或者回应Error的方式来拒绝该prepare请求；<br>
若N大于maxN，则说明该提议是可以接受的，当前表决者会首先将该N记录下来，并将其曾经accept的编号最大的提案Proposal(myid, maxN, value)反馈给提议者，以向提议者展示自己支持的提案意愿，其中第一个参数myid表示表决者Acceptor的标识id，第二个参数表示其曾接受的提案的最大编号maxN，第三个参数表示该提案真正内容value，当然，若当前表决者还未曾accept过任何提议，则会将Proposal(myid, null, null)反馈给提议者。<br>
在prepare阶段N不可能等于maxN，这是由N的生成机制决定的，要获得N的值，其必定会在原来数值的基础上采用同步锁方式增一。<br>
<code>accept阶段</code></li>
<li>当提议者(Proposer)发出prepare(N)之后，若收到了超过半数的表决者(Acceptor)的反馈，那么该提议者会将其真正的提案Proposal(N, value)发送给所有的表决者。</li>
<li>当表决者(Acceptor)接收到提议者发送的Proposal(N, value)提案后，会再次拿出自己曾经accept过的提议中最大编号maxN和曾经记录下的prepare的最大编号，让N与它们进行比较，若N大于等于这两个编号，则当前表决者accept该提案，并反馈给提议者。若N小于这两个编号，则表决者采取不回应或者回应ERROR的方式来拒绝该提议。<br>
3.若提议者没有接收到超过半数的表决者的accept反馈，则重新进入prepare阶段，递增提案号N，重新提出prepare请求，若提议者接收到的反馈数量超过了半数，则其会向外广播两类信息。<br>
向曾accept其提案的表决者发送&quot;可执行数据同步信息&quot;，即让它们执行其接受到的提案。<br>
向未曾向其发送accept反馈的表决者发送“提案+可执行数据同步信号”，即让它们接收到该提案后马上执行。</li>
</ol>
<p><code>Paxos算法的活锁问题</code><br>
Paxos算法中每个进程均可提交提案，但是必须要获取到一个全局的唯一编号N，将该N值赋予提案，为了保证N的唯一性，对该N值操作就必须要放到同步锁（排他锁）中，N值就成了“竞争资源”，若一个进程为了提交提案，一直不停在申请资源N，但是每一次都没有分配给它，此时该进程就处于“活锁”状态。<br>
Fast Paxos算法对Paxos算法进行了改进：其只允许一个进程处理写请求，解决了活锁问题。</p>
<h3 id="7-raftzab只能主节点提交提案">7. raft，zab，只能主节点提交提案</h3>
<p>主节点的出现就是保证数据一致性，保证事务ID是顺序的</p>
<h3 id="8-lru可以解决什么问题如何实现">8. LRU可以解决什么问题？如何实现</h3>
<p>LRU 是 Least Recently Used 的缩写，这种算法认为最近使用的数据是热门数据，下一次很大概率将会再次被使用。而最近很少被使用的数据，很大概率下一次不再用到。当缓存容量的满时候，优先淘汰最近很少使用的数据。<br>
LRU 算法优势在于算法实现难度不大，对于对于热点数据， LRU 效率会很好。</p>
<p>LRU 算法劣势在于对于偶发的批量操作，比如说批量查询历史数据，就有可能使缓存中热门数据被这些历史数据替换，造成缓存污染，导致缓存命中率下降，减慢了正常数据查询。</p>
<p><code>实现思路: 双向链表 + 哈希表</code><br>
维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。</p>
<ol>
<li>如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。</li>
<li>如果此数据没有在缓存链表中，又可以分为两种情况：<br>
如果此时缓存未满，则将此结点直接插入到链表的头部；<br>
如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。</li>
</ol>
<pre><code class="language-java">public class LRUCache {

    Entry head, tail;
    int capacity;
    int size;
    Map&lt;Integer, Entry&gt; cache;
    public LRUCache(int capacity) {
        this.capacity = capacity;
        // 初始化链表
        initLinkedList();
        size = 0;
        cache = new HashMap&lt;&gt;(capacity + 2);
    }

    /**
     * 如果节点不存在，返回 -1.如果存在，将节点移动到头结点，并返回节点的数据。
     *
     * @param key
     * @return
     */
    public int get(int key) {
        Entry node = cache.get(key);
        if (node == null) {
            return -1;
        }
        // 存在移动节点
        moveToHead(node);
        return node.value;
    }

    /**
     * 将节点加入到头结点，如果容量已满，将会删除尾结点
     *
     * @param key
     * @param value
     */
    public void put(int key, int value) {
        Entry node = cache.get(key);
        if (node != null) {
            node.value = value;
            moveToHead(node);
            return;
        }
        // 不存在。先加进去，再移除尾结点
        // 此时容量已满 删除尾结点
        if (size == capacity) {
            Entry lastNode = tail.pre;
            deleteNode(lastNode);
            cache.remove(lastNode.key);
            size--;
        }
        // 加入头结点

        Entry newNode = new Entry();
        newNode.key = key;
        newNode.value = value;
        addNode(newNode);
        cache.put(key, newNode);
        size++;

    }

    private void moveToHead(Entry node) {
        // 首先删除原来节点的关系
        deleteNode(node);
        addNode(node);
    }

    private void addNode(Entry node) {
        head.next.pre = node;
        node.next = head.next;

        node.pre = head;
        head.next = node;
    }

    private void deleteNode(Entry node) {
        node.pre.next = node.next;
        node.next.pre = node.pre;
    }

    public static class Entry {
        public Entry pre;
        public Entry next;
        public int key;
        public int value;

        public Entry(int key, int value) {
            this.key = key;
            this.value = value;
        }
        public Entry() {
        }
    }

    private void initLinkedList() {
        head = new Entry();
        tail = new Entry();

        head.next = tail;
        tail.pre = head;

    }
    public static void main(String[] args) {
        LRUCache cache = new LRUCache(2);
        cache.put(1, 1);
        cache.put(2, 2);
        System.out.println(cache.get(1));
        cache.put(3, 3);
        System.out.println(cache.get(2));

    }
}
</code></pre>
<h3 id="9-lru可以如何优化redis近似lrumysql分段lru">9. LRU可以如何优化？Redis近似LRU？Mysql分段LRU？</h3>
<p><code>Mysql分段LRU</code><br>
将链表拆分成两部分，分为热数据区，与冷数据区，如图所示。<br>
<img src="https://q456qq520.github.io/post-images/1676456311846.png" alt="" loading="lazy"></p>
<ol>
<li>访问数据如果位于热数据区，与之前 LRU 算法一样，移动到热数据区的头结点。</li>
<li>插入数据时，若缓存已满，淘汰尾结点的数据。然后将数据插入冷数据区的头结点。</li>
<li>处于冷数据区的数据每次被访问需要做如下判断：
<ul>
<li>若该数据已在缓存中超过指定时间，比如说 1 s，则移动到热数据区的头结点。</li>
<li>若该数据存在在时间小于指定的时间，则位置保持不变。</li>
</ul>
</li>
</ol>
<p>对于偶发的批量查询，数据仅仅只会落入冷数据区，然后很快就会被淘汰出去。热门数据区的数据将不会受到影响，这样就解决了 LRU 算法缓存命中率下降的问题。</p>
<p><code>Redis近似LRU</code><br>
由于 LRU 算法需要用链表管理所有的数据，会造成大量额外的空间消耗。<br>
除此之外，大量的节点被访问就会带来频繁的链表节点移动操作，从而降低了 Redis 性能。<br>
所以 Redis 对该算法做了简化，Redis LRU 算法并不是真正的 LRU，Redis 通过对少量的 key 采样，并淘汰采样的数据中最久没被访问过的 key。<br>
这就意味着 Redis 无法淘汰数据库最久访问的数据。</p>
<p>Redis LRU 算法有一个重要的点在于可以更改样本数量来调整算法的精度，使其近似接近真实的 LRU 算法，同时又避免了内存的消耗，因为每次只需要采样少量样本，而不是全部数据。</p>
<h3 id="10-雪花算法的使用场景特点">10. 雪花算法的使用场景，特点</h3>
<p>分布式环境下的唯一ID生成算法。<br>
特点：</p>
<ol>
<li>能满足高并发分布式系统环境下ID不重复</li>
<li>基于时间戳，可以保证基本有序递增（有些业务场景对这个又要求）</li>
<li>不依赖第三方的库或者中间件</li>
<li>生成效率极高</li>
</ol>
<h3 id="11-雪花算法的数据结构会有哪些问题">11. 雪花算法的数据结构，会有哪些问题？</h3>
<p>在同一个进程中，它首先是通过时间位保证不重复，如果时间相同则是通过序列位保证。 同时由于时间位是单调递增的，且各个服务器如果大体做了时间同步，那么生成的主键在分布式环境可以认为是总体有序的。</p>
<p>使用雪花算法生成的主键，二进制表示形式包含4部分，从高位到低位分表为：1bit符号位、41bit时间戳位、10bit工作进程位以及12bit序列号位。</p>
<ol>
<li>
<p>符号位(1bit)<br>
预留的符号位，恒为零。</p>
</li>
<li>
<p>时间戳位(41bit)<br>
41位的时间戳可以容纳的毫秒数是2的41次幂，一年所使用的毫秒数是：365 * 24 * 60 * 60 * 1000。通过计算可知：<code>Math.pow(2, 41) / (365 * 24 * 60 * 60 * 1000L)</code>;<br>
结果约等于69.73年。ShardingSphere的雪花算法的时间纪元从2016年11月1日零点开始，可以使用到2086年，相信能满足绝大部分系统的要求。</p>
</li>
<li>
<p>工作进程位(10bit)<br>
该标志在Java进程内是唯一的，如果是分布式应用部署应保证每个工作进程的id是不同的。该值默认为0，可通过属性设置。</p>
</li>
<li>
<p>序列号位(12bit)<br>
该序列是用来在同一个毫秒内生成不同的ID。如果在这个毫秒内生成的数量超过4096(2的12次幂)，那么生成器会等待到下个毫秒继续生成。</p>
</li>
</ol>
<p><code>雪花算法的问题</code></p>
<ul>
<li>时间回拨问题<br>
由于机器的时间是动态的调整的，有可能会出现时间跑到之前几毫秒，如果这个时候获取到了这种时间，则会出现数据重复</li>
<li>机器id的分配和回收问题<br>
目前机器id需要每台机器不一样，这样的方式分配需要有方案进行处理，同时也要考虑，如果机器宕机了，对应的workerId分配后的回收问题</li>
<li>机器id的上限问题<br>
机器id是固定的bit，那么也就是对应的机器个数是有上限的，在有些业务场景下，需要所有机器共享同一个业务空间，那么10bit表示的1024台机器是不够的。</li>
</ul>
<h2 id="八-设计">八 设计</h2>
<h3 id="1-cap理论">1. CAP理论</h3>
<p>Consistency，一致性，是指所有节点在同一时刻的数据是相同的，及更新执行结束并相应用户完成后，所有节点存储的数据都会保持相同。</p>
<p>Availability，可用性，指系统一直处于可用状态，对用户的请求可即时响应。</p>
<p>Partition Tolerance，分区容错性，指分布式系统遇到网络分区的情况下，仍然能够响应用户的请求。网络分区指因为网络故障导致网络不连通，不同节点分布在不同自网络中，各个子网络内网络正常。</p>
<p>CAP 理论，在分布式系统中 C、A、P 这三个特征不能同时满足，只能满足其中两个。</p>
<h3 id="2-二阶段提交是如何进行的会有什么问题锁力度较大">2. 二阶段提交是如何进行的？会有什么问题（锁力度较大）</h3>
<figure data-type="image" tabindex="9"><img src="https://q456qq520.github.io/post-images/1676513104619.png" alt="" loading="lazy"></figure>
<ol>
<li>
<p>资源被同步阻塞<br>
在执行过程中，所有参与节点都是事务独占状态，当参与者占有公共资源时，那么第三方节点访问公共资源会被阻塞。</p>
</li>
<li>
<p>协调者可能出现单点故障<br>
一旦协调者发生故障，参与者会一直阻塞下去。</p>
</li>
<li>
<p>在 Commit 阶段出现数据不一致<br>
在第二阶段中，假设协调者发出了事务 Commit 的通知，但是由于网络问题该通知仅被一部分参与者所收到并执行 Commit，其余的参与者没有收到通知，一直处于阻塞状态，那么，这段时间就产生了数据的不一致性。</p>
</li>
</ol>
<h3 id="3-xatccatsaga">3. xa，tcc，at，saga</h3>
<p>四种分布式事务模式，分别在不同的时间被提出，每种模式都有它的适用场景：</p>
<ol>
<li>
<p>AT 模式是无侵入的分布式事务解决方案，适用于不希望对业务进行改造的场景，几乎0学习成本。</p>
</li>
<li>
<p>TCC 模式是高性能分布式事务解决方案，适用于核心系统等对性能有很高要求的场景。</p>
</li>
<li>
<p>Saga 模式是长事务解决方案，适用于业务流程长且需要保证事务最终一致性的业务系统，Saga 模式一4阶段就会提交本地事务，无锁，长流程情况下可以保证性能，多用于渠道层、集成层业务系统。事务参与者可能是其它公司的服务或者是遗留系统的服务，无法进行改造和提供 TCC 要求的接口，也可以使用 Saga 模式。</p>
</li>
<li>
<p>XA模式是分布式强一致性的解决方案，但性能低而使用较少。<br>
XA将分布式事务分为两个阶段，一个是准备阶段，一个是执行阶段。<br>
准备阶段： 事务协调者会向事务参与者RM发送一个请求，这里的RM其实是由数据库实现的，所以可以认为RM就是数据库。让数据库去执行事务，但执行完不要提交，而是把结果告知事务协调者。<br>
执行阶段： 事务协调者根据结果，通知RM回滚或者提交事务。<br>
优点：<br>
这是一种强一致性的解决方案，因为每一个微服务都是基于各自的事务的，各自的事务是满足ACID的，而且等到大家都执行完了且都成功了才提交，所以全局事务是满足ACID的。<br>
实现比较简单，因为很多数据库都实现了这种模式，使用Seata的XA模式只需要简单的封装上TM。</p>
</li>
</ol>
<p>缺点：<br>
第一阶段不提交，等到第二阶段再提交，但是等的过程中要占用数据库锁，如果一个分布式事务中跨越了很多个分支事务，则可能造成很多资源的浪费，使得别的请求无法访问，降低了可用性；<br>
依赖于数据库，对于如果有的数据库没有实现这种模式，则无法使用这个模式来实现分布式事务。</p>
<h3 id="4-分布式环境下如何防止雪崩隔离-流控-降级-配置超时">4. 分布式环境下如何防止雪崩？隔离、流控、降级、配置超时</h3>
<p>雪崩问题：分布式系统都存在这样一个问题，由于网络的不稳定性，决定了任何一个服务的可用性都不是 100% 的。当网络不稳定的时候，作为服务的提供者，自身可能会被拖死，导致服务调用者阻塞，最终可能引发雪崩效应。</p>
<p>当在高并发的情况下，如果某一外部依赖的服务（第三方系统或者自研系统出现故障）超时阻塞，就有可能使得整个主线程池被占满，增加内存消耗，这是长请求拥塞反模式（一种单次请求时延变长而导致系统性能恶化甚至崩溃的恶化模式）。更进一步，如果线程池被占满，那么整个服务将不可用，就又可能会重复产生上述问题。因此整个系统就像雪崩一样，最终崩塌掉。</p>
<p><code>雪崩效应产生的几种场景</code></p>
<ol>
<li>流量激增：比如异常流量、用户重试导致系统负载升高；</li>
<li>缓存刷新：假设A为client端，B为Server端，假设A系统请求都流向B系统，请求超出了B系统的承载能力，就会造成B系统崩溃；</li>
<li>程序有Bug：代码循环调用的逻辑问题，资源未释放引起的内存泄漏等问题；</li>
<li>硬件故障：比如宕机，机房断电，光纤被挖断等。</li>
<li>线程同步等待：系统间经常采用同步服务调用模式，核心服务和非核心服务共用一个线程池和消息队列。如果一个核心业务线程调用非核心线程，这个非核心线程交由第三方系统完成，当第三方系统本身出现问题，导致核心线程阻塞，一直处于等待状态，而进程间的调用是有超时限制的，最终这条线程将断掉，也可能引发雪崩；</li>
</ol>
<p>针对上述雪崩情景，有很多应对方案，但没有一个万能的模式能够应对所有场景。</p>
<ol>
<li>针对流量激增，采用自动扩缩容以应对突发流量，或在负载均衡器上安装限流模块。</li>
<li>针对缓存刷新，参考Cache应用中的服务过载案例研究</li>
<li>针对硬件故障，多机房容灾，跨机房路由，异地多活等。</li>
<li>针对同步等待，使用Hystrix做故障隔离，熔断器机制等可以解决依赖服务不可用的问题。</li>
</ol>
<p><code>雪崩的整体解决方案</code></p>
<ol>
<li>熔断模式<br>
如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继 续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。</li>
</ol>
<p>设计：<br>
（1）熔断请求判断机制算法：使用无锁循环队列计数，每个熔断器默认维护10个bucket，每1秒一个bucket，每个blucket记录请求的成功、失败、超时、拒绝的状态，默认错误超过50%且10秒内超过 20个请求进行中断拦截。<br>
（2）熔断恢复：对于被熔断的请求，每隔5s允许部分请求通过，若请求都是健康的（RT&lt; 250ms） 则对请求健康恢复。<br>
（3）熔断报警：对于熔断的请求打日志，异常请求超过某些设定则报警。</p>
<ol start="2">
<li>隔离模式<br>
可以对不同类型的请求使用线程池来资源隔离，每种类型的请求互不影响，如果一种类型的请求 线程资源耗尽，则对后续的该类型请求直接返回，不再调用后续资源。</li>
</ol>
<p>隔离的方式一般使用两种<br>
（1）线程池隔离模式：使用一个线程池来存储当前的请求，线程池对请求作处理，设置任务返回处理 超时时间，堆积的请求堆积入线程池队列。这种方式需要为每个依赖的服务申请线程池，有一定的资源 消耗，好处是可以应对突发流量（流量洪峰来临时，处理不完可将数据存储到线程池队里慢慢处理）<br>
（2）信号量隔离模式：使用一个原子计数器（或信号量）来记录当前有多少个线程在运行，请求来先 判断计数器的数值，若超过设置的最大线程个数则丢弃改类型的新请求，若不超过则执行计数操作请求 来计数器+1，请求返回计数器-1。这种方式是严格的控制线程且立即返回模式，无法应对突发流量（流 量洪峰来临时，处理的线程超过数量，其他的请求会直接返回，不继续去请求依赖的服务）</p>
<ol start="3">
<li>
<p>限流模式<br>
主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调 用后续资源。这种模式不能解决服务依赖的问题，只能解决系统整体资源分配问题，因为没有被限流的请求依然有可能造成雪崩效应。</p>
</li>
<li>
<p>配置超时<br>
（1）超时分两种，一种是请求的等待超时，一种是请求运行超时。<br>
（2）等待超时：在任务入队列时设置任务入队列时间，并判断队头的任务入队列时间是否大于超时时 间，超过则丢弃任务。<br>
（3）运行超时：直接可使用线程池提供的get方法。</p>
</li>
</ol>
<h3 id="5-应用服务高可用有哪些措施缓存-冗余-读写分离-降级兜底-横向扩容">5. 应用服务高可用，有哪些措施？缓存、冗余、读写分离、降级兜底、横向扩容</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringBoot启动流程]]></title>
        <id>https://q456qq520.github.io/post/springboot/</id>
        <link href="https://q456qq520.github.io/post/springboot/">
        </link>
        <updated>2023-02-09T02:35:00.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="1-springboot启动流程">1 Springboot启动流程</h2>
]]></summary>
        <content type="html"><![CDATA[<h2 id="1-springboot启动流程">1 Springboot启动流程</h2>
<!-- more -->
<h3 id="11-springapplication创建">1.1 SpringApplication创建</h3>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1675939261552.png" alt="" loading="lazy"></figure>
<h4 id="111-入口">1.1.1 入口</h4>
<pre><code class="language-java">public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) {
    this.sources = new LinkedHashSet();
    this.bannerMode = Mode.CONSOLE;
    this.logStartupInfo = true;
    this.addCommandLineProperties = true;
    this.headless = true;
    this.registerShutdownHook = true;
    this.additionalProfiles = new HashSet();
    this.isCustomEnvironment = false;
    this.resourceLoader = resourceLoader;
    Assert.notNull(primarySources, &quot;PrimarySources must not be null&quot;);
    // 将 Main Class 设置为自己的元素
    this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources));
    // 检查当先的 app 类型
    this.webApplicationType = WebApplicationType.deduceFromClasspath();
    // 先从 Spring.factories 文件中加载 ApplicationContextInitializer 类信息。
    setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class));
    // 先从 Spring.factories 文件中加载 ApplicationListener 类信息。
    setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));
    // 获取 main class 信息，并设置到本地属性中
    this.mainApplicationClass = deduceMainApplicationClass();
}
</code></pre>
<h4 id="112-检查应用类型">1.1.2 检查应用类型</h4>
<p>在将Main class 设置primarySources 后，调用了 WebApplicationType.deduceFromClasspath() 方法，该方法是为了检查当前的应用类型，并设置给 webApplicationType。</p>
<pre><code class="language-java">static WebApplicationType deduceFromClasspath() {
        if (ClassUtils.isPresent(&quot;org.springframework.web.reactive.DispatcherHandler&quot;, (ClassLoader)null) &amp;&amp; !ClassUtils.isPresent(&quot;org.springframework.web.servlet.DispatcherServlet&quot;, (ClassLoader)null) &amp;&amp; !ClassUtils.isPresent(&quot;org.glassfish.jersey.servlet.ServletContainer&quot;, (ClassLoader)null)) {
            return REACTIVE;
        } else {
            String[] var0 = SERVLET_INDICATOR_CLASSES;
            int var1 = var0.length;

            for(int var2 = 0; var2 &lt; var1; ++var2) {
                String className = var0[var2];
                if (!ClassUtils.isPresent(className, (ClassLoader)null)) {
                    return NONE;
                }
            }
        }
    }
</code></pre>
<p>这里主要是通过类加载器判断是否存在 <code>REACTIVE</code>相关的类信息，假如有就代表是一个 REACTIVE 的应用，假如不是就检查是否存在<code>Servelt</code>和 <code>ConfigurableWebApplicationContext</code> ，假如都没有，就代表应用为非 WEB 类应用，返回 <code>NONE</code>，默认返回<code>SERVLET</code>类型</p>
<h4 id="113-设置初始化器-initializer">1.1.3 设置初始化器 Initializer</h4>
<p>我们设置完成应用类型后，就寻找所有的 Initializer 实现类，并设置到SpringApplication 的 Initializers 中。</p>
<p>容器刷新之前调用此类的initialize方法。这个点允许被用户自己扩展。用户可以在整个spring容器还没被初始化之前做一些事情。可以想到的场景可能为，在最开始激活一些配置，或者利用这时候class还没被类加载器加载的时机，进行动态字节码注入等操作。</p>
<p>这里先说一下 getSpringFactoriesInstances 方法，我们知道在我们使用 SpringBoot 程序中，会经常在 META-INF/spring.factories 目录下看到一些EnableAutoConfiguration，来出发 config 类注入到容器中，我们知道一般一个 config 类要想被 SpringBoot 扫描到需要使用 @CompnentScan 来扫描具体的路径，对于 jar 包来说这无疑是非常不方便的，所以 SpringBoot 提供了另外一种方式来实现，就是使用 spring.factories。但是要实现，得先进行加载，过程如下：</p>
<pre><code class="language-java">private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) {
    ClassLoader classLoader = getClassLoader();
    // Use names and ensure unique to protect against duplicates
    Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;(SpringFactoriesLoader.loadFactoryNames(type, classLoader));
    List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names);
    AnnotationAwareOrderComparator.sort(instances);
    return instances;
}
</code></pre>
<p>我们先来看一下传入参数，这里需要注意的是 args，这个是初始化对应 type 的时候传入的构造参数，我们先看一下 SpringFactoriesLoader#loadFactoryNames 方法：</p>
<pre><code class="language-java">public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) {
    String factoryClassName = factoryClass.getName();
    return (List)loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());
}

private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) {
    MultiValueMap&lt;String, String&gt; result = (MultiValueMap)cache.get(classLoader);
    if (result != null) {
        return result;
    } else {
        try {
            //加载配置文件
            Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources(&quot;META-INF/spring.factories&quot;) : ClassLoader.getSystemResources(&quot;META-INF/spring.factories&quot;);
            LinkedMultiValueMap result = new LinkedMultiValueMap();

            while(urls.hasMoreElements()) {
                URL url = (URL)urls.nextElement();
                UrlResource resource = new UrlResource(url);
                Properties properties = PropertiesLoaderUtils.loadProperties(resource);
                Iterator var6 = properties.entrySet().iterator();

                while(var6.hasNext()) {
                    Entry&lt;?, ?&gt; entry = (Entry)var6.next();
                    List&lt;String&gt; factoryClassNames = Arrays.asList(StringUtils.commaDelimitedListToStringArray((String)entry.getValue()));
                    result.addAll((String)entry.getKey(), factoryClassNames);
                }
            }

            cache.put(classLoader, result);
            return result;
        } catch (IOException var9) {
            throw new IllegalArgumentException(&quot;Unable to load factories from location [META-INF/spring.factories]&quot;, var9);
        }
    }
}
</code></pre>
<p>首先是会先检查缓存，假如缓存中存在就直接返回，假如没有就调用 classLoader#getResources 方法，传入META-INF/spring.factories，即获取所有 jar 包下的对应文件，并封装成 UrlResource ，然后使用 PropertiesLoaderUtils 将这些信息读取成一个对一对的 properties，我们观察一下 spring.factories 都是按 properties 格式排版的，假如有多个就用逗号隔开，所以这里还需要将逗号的多个类分隔开来，并加到 result 中，由于 result 是一个 LinkedMultiValueMap 类型，支持多个值插入，最后放回缓存中。最终完成加载 META-INF/spring.factories 中的配置。</p>
<p>在获取到所有的 Initializer 后接下来是调用 createSpringFactoriesInstances 方法进行初始化。</p>
<pre><code class="language-java">private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) {
    ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
    Set&lt;String&gt; names = new LinkedHashSet(SpringFactoriesLoader.loadFactoryNames(type, classLoader));
    List&lt;T&gt; instances = this.createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names);
    AnnotationAwareOrderComparator.sort(instances);
    return instances;
}
</code></pre>
<pre><code class="language-java">private &lt;T&gt; List&lt;T&gt; createSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, ClassLoader classLoader, Object[] args, Set&lt;String&gt; names) {
    List&lt;T&gt; instances = new ArrayList(names.size());
    Iterator var7 = names.iterator();
  // 这里包括很多初始化类信息，包括 apollo , shardingShepre 都是在这里初始化。
    while(var7.hasNext()) {
        String name = (String)var7.next();

        try {
            Class&lt;?&gt; instanceClass = ClassUtils.forName(name, classLoader);
            Assert.isAssignable(type, instanceClass);
            Constructor&lt;?&gt; constructor = instanceClass.getDeclaredConstructor(parameterTypes);
            T instance = BeanUtils.instantiateClass(constructor, args);
            instances.add(instance);
        } catch (Throwable var12) {
            throw new IllegalArgumentException(&quot;Cannot instantiate &quot; + type + &quot; : &quot; + name, var12);
        }
    }
    return instances;
}
</code></pre>
<p>这里的 names 就是我们上面通过类加载器加载到的类名，到这里会先通过反射生成 class 对象，然后判断该类是否继承与 ApplicationContextInitializer ，最后通过反射的方式获取这个类的构造方法，并调用该构造方法，传入已经定义好的构造参数，对于 ApplicationContextInitializer 是无参的构造方法，然后初始化实例并返回，回到原来的方法，这里会先对所有的 ApplicationContextInitializer 进行排序，调用 <code>AnnotationAwareOrderComparator#sort(instances)</code>方法，这里就是根据 @Order 中的顺序进行排序。</p>
<h4 id="114-设置监听器">1.1.4 设置监听器</h4>
<p>接下来是设置 ApplicationListener，我们跟进去就会发现这里和上面获取 ApplicationContextInitializer 的方法如出一辙。这里不过多介绍。</p>
<h3 id="12-springapplication-run">1.2 SpringApplication run()</h3>
<p>在完成 SpringApplication 对象的初始化后，我们进入了他的 run 方法，这个方法几乎涵盖了 SpringBoot 生命周期的所有内容，主要分为九个步骤。</p>
<pre><code class="language-java">public ConfigurableApplicationContext run(String... args) {
    // 启动计时器计算初始化完成耗时
    StopWatch stopWatch = new StopWatch();
    stopWatch.start();
    ConfigurableApplicationContext context = null;
    configureHeadlessProperty();
    // 第一步：获取 SpringApplicationRunListener， 然后调用他的 staring 方法启动监听器。
    SpringApplicationRunListeners listeners = getRunListeners(args);
    listeners.starting();
    try {
        ApplicationArguments applicationArguments = new DefaultApplicationArguments(args);
        // 第二步:根据 SpringApplicationRunListeners以及参数来准备环境
        ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments);
        configureIgnoreBeanInfo(environment);
        // 准备打印 Banner
        Banner printedBanner = printBanner(environment);
        // 第三步：创建 Spring 容器
        context = createApplicationContext();
        // 第四步： Spring 容器的前置处理
        prepareContext(context, environment, listeners, applicationArguments, printedBanner);
        // 第五步：刷新 Spring 容器
        refreshContext(context);
        // 第六步： Spring 容器的后置处理器
        afterRefresh(context, applicationArguments);
        // 停止计时
        stopWatch.stop();
        if (this.logStartupInfo) {
            new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch);
        }
        //第七步：通知所有 listener 结束启动
        listeners.started(context);
        //第八步：调用所有 runner 的 run 方法
        callRunners(context, applicationArguments);
    }
    catch (Throwable ex) {
        handleRunFailure(context, ex, listeners);
        throw new IllegalStateException(ex);
    }
    //第九步：通知所有 listener running 事件
    try {
        listeners.running(context);
    }
    catch (Throwable ex) {
        handleRunFailure(context, ex, null);
        throw new IllegalStateException(ex);
    }
    return context;
}
</code></pre>
<h4 id="121-获取-springapplicationrunlistener">1.2.1 获取 SpringApplicationRunListener</h4>
<pre><code class="language-java">private SpringApplicationRunListeners getRunListeners(String[] args) {
    Class&lt;?&gt;[] types = new Class[]{SpringApplication.class, String[].class};
    return new SpringApplicationRunListeners(logger, this.getSpringFactoriesInstances(SpringApplicationRunListener.class, types, this, args));
}
</code></pre>
<p>这里和上面获取 initializer 和 listener 的方式基本一致，都是通过 getSpringFactoriesInstances, 最终只找到一个类就是：org.springframework.boot.context.event.EventPublishingRunListener ，然后调用其构造方法并传入产生 args , 和 SpringApplication 本身:</p>
<pre><code class="language-java">public class EventPublishingRunListener implements SpringApplicationRunListener, Ordered {

   private final SpringApplication application;

   private final String[] args;

   private final SimpleApplicationEventMulticaster initialMulticaster;

   public EventPublishingRunListener(SpringApplication application, String[] args) {
       this.application = application;
       this.args = args;
       this.initialMulticaster = new SimpleApplicationEventMulticaster();
       for (ApplicationListener&lt;?&gt; listener : application.getListeners()) {
           this.initialMulticaster.addApplicationListener(listener);
       }
   }
}
</code></pre>
<p>我们先看一下构造函数，首先将我们获取到的ApplicationListener 集合添加到<code>initialMulticaster</code>中， 最后都是通过操作<code>SimpleApplicationEventMulticaster</code> 来进行广播，我，他继承于 <code>AbstractApplicationEventMulticaster</code>。</p>
<h4 id="122-环境准备">1.2.2 环境准备</h4>
<pre><code class="language-java">private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners,
        ApplicationArguments applicationArguments) {
    // 根据类型创建对应的 environment
    ConfigurableEnvironment environment = getOrCreateEnvironment();
    // 配置 environment 信息
    configureEnvironment(environment, applicationArguments.getSourceArgs());
    ConfigurationPropertySources.attach(environment);
    // 发送 prepareEnviroment 事件
    listeners.environmentPrepared(environment);
    bindToSpringApplication(environment);
    if (!this.isCustomEnvironment) {
        environment = new EnvironmentConverter(getClassLoader()).convertEnvironmentIfNecessary(environment,
                deduceEnvironmentClass());
    }
    ConfigurationPropertySources.attach(environment);
    return environment;
}
</code></pre>
<p>首先是调用 <code>getOrCreateEnvironment</code>方法来创建<code>environment</code>，我们跟进去可以发现这里是根据我们上面设置的环境的类型来进行选择的。</p>
<pre><code class="language-java">private ConfigurableEnvironment getOrCreateEnvironment() {
    if (this.environment != null) {
        return this.environment;
    } else {
        switch(this.webApplicationType) {
        case SERVLET:
            return new StandardServletEnvironment();
        case REACTIVE:
            return new StandardReactiveWebEnvironment();
        default:
            return new StandardEnvironment();
        }
    }
}
</code></pre>
<p>在创建完成 Environment 后，接下来就到了调用 configureEnvironment 方法：</p>
<pre><code class="language-java">protected void configureEnvironment(ConfigurableEnvironment environment, String[] args) {
    if (this.addConversionService) {
        ConversionService conversionService = ApplicationConversionService.getSharedInstance();
        environment.setConversionService((ConfigurableConversionService) conversionService);
    }
    // 配置PropertySources
    configurePropertySources(environment, args);
    // 配置Profiles
    configureProfiles(environment, args);
}
</code></pre>
<p>我们先看一下 <code>configurePropertySources</code> 方法，这里主要分两部分，首先是查询当前是否存在 <code>defaultProperties</code> ，假如不为空就会添加到 <code>environment</code>的<code>propertySources</code>中，接着是处理命令行参数，将命令行参数作为一个<code>CompositePropertySource</code>或者<code>SimpleCommandLinePropertySource</code>添加到 <code>environment</code>的<code>propertySources</code>里面，</p>
<pre><code class="language-java">protected void configurePropertySources(ConfigurableEnvironment environment, String[] args) {
    MutablePropertySources sources = environment.getPropertySources();
    if (this.defaultProperties != null &amp;&amp; !this.defaultProperties.isEmpty()) {
        sources.addLast(new MapPropertySource(&quot;defaultProperties&quot;, this.defaultProperties));
    }
    if (this.addCommandLineProperties &amp;&amp; args.length &gt; 0) {
        String name = &quot;commandLineArgs&quot;;
        if (sources.contains(name)) {
            PropertySource&lt;?&gt; source = sources.get(name);
            CompositePropertySource composite = new CompositePropertySource(name);
            composite.addPropertySource(new SimpleCommandLinePropertySource(&quot;springApplicationCommandLineArgs&quot;, args));
            composite.addPropertySource(source);
            sources.replace(name, composite);
        } else {
            sources.addFirst(new SimpleCommandLinePropertySource(args));
        }
    }
}
</code></pre>
<p>接着调用 <code>ConfigurationPropertySources#attach</code>方法,他会先去 environment 中查找 configurationProperties , 假如寻找到了，先检查 configurationProperties 和当前 environment 是否匹配，假如不相等，就先去除，最后添加 configurationProperties 并将其 sources 属性设置进去。</p>
<pre><code class="language-java">public static void attach(Environment environment) {
    Assert.isInstanceOf(ConfigurableEnvironment.class, environment);
    MutablePropertySources sources = ((ConfigurableEnvironment) environment).getPropertySources();
    PropertySource&lt;?&gt; attached = sources.get(ATTACHED_PROPERTY_SOURCE_NAME);
    if (attached != null &amp;&amp; attached.getSource() != sources) {
        sources.remove(ATTACHED_PROPERTY_SOURCE_NAME);
        attached = null;
    }
    if (attached == null) {
        sources.addFirst(new ConfigurationPropertySourcesPropertySource(ATTACHED_PROPERTY_SOURCE_NAME,
                new SpringConfigurationPropertySources(sources)));
    }
}
</code></pre>
<p>下一步是通知观察者，发送<code>ApplicationEnvironmentPreparedEvent</code>事件，调用的是 <code>SpringApplicationRunListeners#environmentPrepared</code>方法。会唤醒<code>ConfigFileApplicationListener</code>监听器执行相应逻辑。最主要的加载方法load中，首先会获取到配置文件的搜索路径。如果设置了<code>spring.config.location</code>则会去指定目录下搜索，否则就去默认的搜索目录下<code>classpath:/,classpath:/config/,file:./,file:./config/</code>。</p>
<p>拿到所有待搜索目录后，遍历每个目录获取需要加载的配置文件。如果指定了spring.config.name，则加载指定名称的配置文件。否则使用默认的application作为配置文件的前缀名。然后，会利用<code>PropertiesPropertySourceLoader</code>和<code>YamlPropertySourceLoader</code>加载后缀名为<code>properties</code>、<code>xml</code>、<code>yml</code>或者<code>yaml</code>的文件。</p>
<p>拿到文件目录和文件名后，就可以去对应的路径下加载配置文件了。核心的过程是利用输入流读取配置文件，并根据读到的分隔符进行判断来切分配置文件的key和value。并将内容以key-value键值对的形式封装成一个<code>OriginTrackedMapPropertySource</code>，最后再将一个个配置文件封装成<code>Document</code>。最后遍历这些<code>Documents</code>，调用consumer.accept(profile, document));供上层调用访问。</p>
<p>由于监听器的真正执行是通过调用<code>listener.onApplicationEvent(event)</code>方法来执行的，因此我们从该方法开始分析：</p>
<pre><code class="language-java">public void onApplicationEvent(ApplicationEvent event) {
    if (event instanceof ApplicationEnvironmentPreparedEvent) {
        this.onApplicationEnvironmentPreparedEvent((ApplicationEnvironmentPreparedEvent)event);
    }

    if (event instanceof ApplicationPreparedEvent) {
        this.onApplicationPreparedEvent(event);
    }

}

private void onApplicationEnvironmentPreparedEvent(ApplicationEnvironmentPreparedEvent event) {
    List&lt;EnvironmentPostProcessor&gt; postProcessors = this.loadPostProcessors();
    postProcessors.add(this);
    AnnotationAwareOrderComparator.sort(postProcessors);
    Iterator var3 = postProcessors.iterator();

    while(var3.hasNext()) {
        EnvironmentPostProcessor postProcessor = (EnvironmentPostProcessor)var3.next();
        postProcessor.postProcessEnvironment(event.getEnvironment(), event.getSpringApplication());
    }

}
</code></pre>
<p>这里loadPostProcessors方法就是从spring.factories中加载EnvironmentPostProcessor接口对应的实现类，并把当前对象也添加进去(因为ConfigFileApplicationListener也实现了EnvironmentPostProcessor接口，所以可以添加)。因此在下方遍历时，会访问该类下的postProcessEnvironment方法，从该方法中进入：</p>
<pre><code class="language-java">public void load() {
    this.profiles = new LinkedList();
    this.processedProfiles = new LinkedList();
    this.activatedProfiles = false;
    this.loaded = new LinkedHashMap();
    this.initializeProfiles();

    while(!this.profiles.isEmpty()) {
        ConfigFileApplicationListener.Profile profile = (ConfigFileApplicationListener.Profile)this.profiles.poll();
        if (profile != null &amp;&amp; !profile.isDefaultProfile()) {
            this.addProfileToEnvironment(profile.getName());
        }

        this.load(profile, this::getPositiveProfileFilter, this.addToLoaded(MutablePropertySources::addLast, false));
        this.processedProfiles.add(profile);
    }

    this.resetEnvironmentProfiles(this.processedProfiles);
    this.load((ConfigFileApplicationListener.Profile)null, this::getNegativeProfileFilter, this.addToLoaded(MutablePropertySources::addFirst, true));
    this.addLoadedPropertySources();
}
</code></pre>
<p>其中 apply 方法主要是加载 defaultProperties ，假如已经存在，就进行替换，而替换的目标 PropertySource 就是 load这里最后的一个 consumer 函数加载出来的，这里列一下主要做的事情：<br>
1、加载系统中设置的所有的 Profile 。<br>
2、遍历所有的 Profile，假如是默认的 Profile， 就将这个 Profile 加到 environment 中。<br>
3、调用load 方法，加载配置，我们深入看一下这个方法：</p>
<pre><code class="language-java">private void load(ConfigFileApplicationListener.Profile profile, ConfigFileApplicationListener.DocumentFilterFactory filterFactory, ConfigFileApplicationListener.DocumentConsumer consumer) {
        this.getSearchLocations().forEach((location) -&gt; {
            boolean isFolder = location.endsWith(&quot;/&quot;);
            Set&lt;String&gt; names = isFolder ? this.getSearchNames() : ConfigFileApplicationListener.NO_SEARCH_NAMES;
            names.forEach((name) -&gt; {
                this.load(location, name, profile, filterFactory, consumer);
            });
        });
    }
</code></pre>
<p>首先调用了getSearchLocations方法</p>
<pre><code class="language-java">private Set&lt;String&gt; getSearchLocations() {
    if (this.environment.containsProperty(&quot;spring.config.location&quot;)) {
        return this.getSearchLocations(&quot;spring.config.location&quot;);
    } else {
        Set&lt;String&gt; locations = this.getSearchLocations(&quot;spring.config.additional-location&quot;);
        locations.addAll(this.asResolvedSet(ConfigFileApplicationListener.this.searchLocations, &quot;classpath:/,classpath:/config/,file:./,file:./config/&quot;));
        return locations;
    }
}
</code></pre>
<p>该方法用于获取配置文件的路径，如果利用spring.config.location指定了配置文件路径，则根据该路径进行加载。否则则根据默认路径加载，而默认路径就是我们最初提到的那四个路径。接下来，再深入asResolvedSet方法内部分析一下:</p>
<pre><code class="language-java">private Set&lt;String&gt; asResolvedSet(String value, String fallback) {
        List&lt;String&gt; list = Arrays.asList(StringUtils.trimArrayElements(StringUtils.commaDelimitedListToStringArray(value != null ? this.environment.resolvePlaceholders(value) : fallback)));
        Collections.reverse(list);
        return new LinkedHashSet(list);
    }
</code></pre>
<p>这里的value表示ConfigFileApplicationListener初始化时设置的搜索路径，而fallback就是<code>DEFAULT_SEARCH_LOCATIONS</code>默认搜索路径<code>。StringUtils.trimArrayElements(StringUtils.commaDelimitedListToStringArray()）</code>方法就是以逗号作为分隔符对&quot;<code>classpath:/,classpath:/config/,file:./,file:./config/</code>&quot;进行切割，并返回一个字符数组。而这里的<code>Collections.reverse(list)</code>;之后，就是体现优先级的时候了，先被扫描到的配置文件会优先生效。</p>
<p>这里我们拿到搜索路径之后，load方法里对每个搜索路径进行遍历，首先调用了getSearchNames()方法</p>
<pre><code class="language-java">private Set&lt;String&gt; getSearchNames() {
    if (this.environment.containsProperty(&quot;spring.config.name&quot;)) {
        String property = this.environment.getProperty(&quot;spring.config.name&quot;);
        return this.asResolvedSet(property, (String)null);
    } else {
        return this.asResolvedSet(ConfigFileApplicationListener.this.names, &quot;application&quot;);
    }
}
</code></pre>
<p>该方法中如果我们通过spring.config.name设置了要检索的配置文件前缀，会按设置进行加载，否则加载默认的配置文件前缀即application。<br>
拿到所有需要加载的配置文件前缀后，则遍历每个需要加载的配置文件，进行搜索加载，加载过程如下：</p>
<pre><code class="language-java">private void load(String location, String name, ConfigFileApplicationListener.Profile profile, ConfigFileApplicationListener.DocumentFilterFactory filterFactory, ConfigFileApplicationListener.DocumentConsumer consumer) {
     //下面的if分支默认是不走的，除非我们设置spring.config.name为空或者null
    //或者是spring.config.location指定了配置文件的完整路径，也就是入参location的值
    if (!StringUtils.hasText(name)) {
        Iterator var6 = this.propertySourceLoaders.iterator();

        while(var6.hasNext()) {
            PropertySourceLoader loader = (PropertySourceLoader)var6.next();
            //检查配置文件名的后缀是否符合要求，
            //文件名后缀要求是properties、xml、yml或者yaml
            if (this.canLoadFileExtension(loader, location)) {
                this.load(loader, location, profile, filterFactory.getDocumentFilter(profile), consumer);
                return;
            }
        }
    }

    Set&lt;String&gt; processed = new HashSet();
    Iterator var14 = this.propertySourceLoaders.iterator();
    //propertySourceLoaders属性是在Load类的构造方法中设置的，可以加载文件后缀为properties、xml、yml或者yaml的文件
    while(var14.hasNext()) {
        PropertySourceLoader loaderx = (PropertySourceLoader)var14.next();
        String[] var9 = loaderx.getFileExtensions();
        int var10 = var9.length;

        for(int var11 = 0; var11 &lt; var10; ++var11) {
            String fileExtension = var9[var11];
            if (processed.add(fileExtension)) {
                this.loadForFileExtension(loaderx, location + name, &quot;.&quot; + fileExtension, profile, filterFactory, consumer);
            }
        }
    }

}
</code></pre>
<p><code>this.propertySourceLoaders</code>既包含了上面提到的两个<code>PropertiesPropertySourceLoader</code>和<code>YamlPropertySourceLoader</code>，PropertiesPropertySourceLoader可以加载文件扩展名为properties和xml的文件，YamlPropertySourceLoader可以加载文件扩展名为yml和yaml的文件。获取到搜索路径、文件名和扩展名后，就可以到对应的路径下去检索配置文件并加载了。</p>
<pre><code class="language-java">private void loadForFileExtension(PropertySourceLoader loader, String prefix, String fileExtension, ConfigFileApplicationListener.Profile profile, ConfigFileApplicationListener.DocumentFilterFactory filterFactory, ConfigFileApplicationListener.DocumentConsumer consumer) {
    ConfigFileApplicationListener.DocumentFilter defaultFilter = filterFactory.getDocumentFilter((ConfigFileApplicationListener.Profile)null);
    ConfigFileApplicationListener.DocumentFilter profileFilter = filterFactory.getDocumentFilter(profile);
    if (profile != null) {
         //在文件名上加上profile值，之后调用load方法加载配置文件，入参带有过滤器，可以防止重复加载
        String profileSpecificFile = prefix + &quot;-&quot; + profile + fileExtension;
        this.load(loader, profileSpecificFile, profile, defaultFilter, consumer);
        this.load(loader, profileSpecificFile, profile, profileFilter, consumer);
        Iterator var10 = this.processedProfiles.iterator();

        while(var10.hasNext()) {
            ConfigFileApplicationListener.Profile processedProfile = (ConfigFileApplicationListener.Profile)var10.next();
            if (processedProfile != null) {
                String previouslyLoaded = prefix + &quot;-&quot; + processedProfile + fileExtension;
                this.load(loader, previouslyLoaded, profile, profileFilter, consumer);
            }
        }
    }
    //加载不带profile的配置文件
    this.load(loader, prefix + fileExtension, profile, profileFilter, consumer);
}
</code></pre>
<pre><code class="language-java">// 加载配置文件
private void load(PropertySourceLoader loader, String location, Profile profile, DocumentFilter filter,
                DocumentConsumer consumer) {
    try {
               //调用Resource类到指定路径加载配置文件
               // location比如file:./config/application.properties
        Resource resource = this.resourceLoader.getResource(location);
        if (resource == null || !resource.exists()) {
            if (this.logger.isTraceEnabled()) {
                StringBuilder description = getDescription(&quot;Skipped missing config &quot;, location, resource,
                        profile);
                this.logger.trace(description);
            }
            return;
        }
        if (!StringUtils.hasText(StringUtils.getFilenameExtension(resource.getFilename()))) {
            if (this.logger.isTraceEnabled()) {
                StringBuilder description = getDescription(&quot;Skipped empty config extension &quot;, location,
                        resource, profile);
                this.logger.trace(description);
            }
            return;
        }
        String name = &quot;applicationConfig: [&quot; + location + &quot;]&quot;;
               //读取配置文件内容，将其封装到Document类中，解析文件内容主要是找到
        //配置spring.profiles.active和spring.profiles.include的值
        List&lt;Document&gt; documents = loadDocuments(loader, name, resource);
               //如果文件没有配置数据，则跳过
        if (CollectionUtils.isEmpty(documents)) {
            if (this.logger.isTraceEnabled()) {
                StringBuilder description = getDescription(&quot;Skipped unloaded config &quot;, location, resource,
                        profile);
                this.logger.trace(description);
            }
            return;
        }
        List&lt;Document&gt; loaded = new ArrayList&lt;&gt;();
               //遍历配置文件，处理里面配置的profile
        for (Document document : documents) {
            if (filter.match(document)) {
                       //将配置文件中配置的spring.profiles.active和
                   //spring.profiles.include的值写入集合profiles中，
                   //上层调用方法会读取profiles集合中的值，并读取对应的配置文件
                   //addActiveProfiles方法只在第一次调用时会起作用，里面有判断
                addActiveProfiles(document.getActiveProfiles());
                addIncludedProfiles(document.getIncludeProfiles());
                loaded.add(document);
            }
        }
        Collections.reverse(loaded);
        if (!loaded.isEmpty()) {
            loaded.forEach((document) -&gt; consumer.accept(profile, document));
            if (this.logger.isDebugEnabled()) {
                StringBuilder description = getDescription(&quot;Loaded config file &quot;, location, resource, profile);
                this.logger.debug(description);
            }
        }
    }
    catch (Exception ex) {
        throw new IllegalStateException(&quot;Failed to load property source from location '&quot; + location + &quot;'&quot;, ex);
    }
}
</code></pre>
<p>该方法首先调用<code>this.resourceLoader.getResource(location)</code>;用来判断<code>location路径</code>下的文件是否存在，如果存在，会调用loadDocuments方法对配置文件进行加载：</p>
<pre><code class="language-java">private List&lt;ConfigFileApplicationListener.Document&gt; loadDocuments(PropertySourceLoader loader, String name, Resource resource) throws IOException {
    ConfigFileApplicationListener.DocumentsCacheKey cacheKey = new ConfigFileApplicationListener.DocumentsCacheKey(loader, resource);
    List&lt;ConfigFileApplicationListener.Document&gt; documents = (List)this.loadDocumentsCache.get(cacheKey);
    if (documents == null) {
        List&lt;PropertySource&lt;?&gt;&gt; loaded = loader.load(name, resource);
        documents = this.asDocuments(loaded);
        this.loadDocumentsCache.put(cacheKey, documents);
    }

    return documents;
}
</code></pre>
<p>再内部根据不同的<code>PropertySourceLoader</code>调用相应的load方法和<code>loadProperties(resource)</code>方法</p>
<pre><code class="language-java">public List&lt;PropertySource&lt;?&gt;&gt; load(String name, Resource resource) throws IOException {
    Map&lt;String, ?&gt; properties = this.loadProperties(resource);
    return properties.isEmpty() ? Collections.emptyList() : Collections.singletonList(new OriginTrackedMapPropertySource(name, properties));
}

private Map&lt;String, ?&gt; loadProperties(Resource resource) throws IOException {
    String filename = resource.getFilename();
    return (Map)(filename != null &amp;&amp; filename.endsWith(&quot;.xml&quot;) ? PropertiesLoaderUtils.loadProperties(resource) : (new OriginTrackedPropertiesLoader(resource)).load());
}
</code></pre>
<p>由于我们目前的配置文件只有application.properties，也就是文件结尾不是以xml作为扩展名。因此loadProperties方法会进入到<code>new OriginTrackedPropertiesLoader</code>。因此再进入到<code>new OriginTrackedPropertiesLoader(resource).load()</code>;。</p>
<pre><code class="language-java">public Map&lt;String, OriginTrackedValue&gt; load(boolean expandLists) throws IOException {
    OriginTrackedPropertiesLoader.CharacterReader reader = new OriginTrackedPropertiesLoader.CharacterReader(this.resource);
    Throwable var3 = null;

    try {
        Map&lt;String, OriginTrackedValue&gt; result = new LinkedHashMap();
        StringBuilder buffer = new StringBuilder();

        while(reader.read()) {
            String key = this.loadKey(buffer, reader).trim();
            if (expandLists &amp;&amp; key.endsWith(&quot;[]&quot;)) {
                key = key.substring(0, key.length() - 2);
                int var19 = 0;

                while(true) {
                    OriginTrackedValue value = this.loadValue(buffer, reader, true);
                    this.put(result, key + &quot;[&quot; + var19++ + &quot;]&quot;, value);
                    if (!reader.isEndOfLine()) {
                        reader.read();
                    }

                    if (reader.isEndOfLine()) {
                        break;
                    }
                }
            } else {
                OriginTrackedValue value = this.loadValue(buffer, reader, false);
                this.put(result, key, value);
            }
        }

        LinkedHashMap var18 = result;
        return var18;
    } catch (Throwable var16) {
        var3 = var16;
        throw var16;
    } 
}
</code></pre>
<pre><code class="language-java">CharacterReader(Resource resource) throws IOException {
            this.reader = new LineNumberReader(new InputStreamReader(resource.getInputStream(), StandardCharsets.ISO_8859_1));
        }
</code></pre>
<pre><code class="language-java">private String loadKey(StringBuilder buffer, OriginTrackedPropertiesLoader.CharacterReader reader) throws IOException {
    buffer.setLength(0);
    boolean previousWhitespace = false;

    while(!reader.isEndOfLine()) {
        // 判断读取到的字节是否为'=' 或者为 ':'，如果是则直接返回读取都的buffer内容
        if (reader.isPropertyDelimiter()) {
            reader.read();
            return buffer.toString();
        }

        if (!reader.isWhiteSpace() &amp;&amp; previousWhitespace) {
            return buffer.toString();
        }

        previousWhitespace = reader.isWhiteSpace();
        buffer.append(reader.getCharacter());
        reader.read();
    }

    return buffer.toString();
}

private OriginTrackedValue loadValue(StringBuilder buffer, OriginTrackedPropertiesLoader.CharacterReader reader, boolean splitLists) throws IOException {
    buffer.setLength(0);

    while(reader.isWhiteSpace() &amp;&amp; !reader.isEndOfLine()) {
        reader.read();
    }

    Location location = reader.getLocation();

    while(!reader.isEndOfLine() &amp;&amp; (!splitLists || !reader.isListDelimiter())) {
        buffer.append(reader.getCharacter());
        reader.read();
    }

    Origin origin = new TextResourceOrigin(this.resource, location);
    return OriginTrackedValue.of(buffer.toString(), origin);
}
</code></pre>
<p>在这个方法里，首先<code>CharacterReader</code>方法将我们的resource也就是配置文件转为了<code>输入流</code>，然后利用<code>reader.read()</code>进行读取，在loadKey方法中我们看到，这里判断读取到的是否为<code>'='</code>或者为<code>':'</code>，也就是我们在配置文件中以'='或者':'分割的key-value。因此看到这里，我们可以直观的感受到这里应该是读取配置文件，并切分key和value的地方。<br>
最终，对配置文件读取完成后，会将其以key-value的形式封装到一个<code>Map集合</code>中进行返回，然后封装到<code>OriginTrackedMapPropertySource</code>中作为一个<code>MapPropertySource</code>对象。再层层往上回退发现会最终封装成一个<code>asDocuments(loaded)</code>;Document对象。最后回到最上层的load方法中，loadDocuments(loader, name, resource);方法即返回我们加载好的配置文件Document对象集合。并对集合中的每一个配置文件document对象进行遍历，调用loaded.forEach((document) -&gt; consumer.accept(profile, document));</p>
<h4 id="123-创建-applicationcontext">1.2.3 创建 ApplicationContext</h4>
<p>首先是检查 Application的类型，然后获取对应的<code>ApplicationContext</code>类，我们这里是获取到了 <code>org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext</code>接着调用 <code>BeanUtils.instantiateClass(contextClass)</code>; 方法进行对象的初始化。</p>
<pre><code class="language-java">protected ConfigurableApplicationContext createApplicationContext() {
    Class&lt;?&gt; contextClass = this.applicationContextClass;
    if (contextClass == null) {
        try {
            switch(this.webApplicationType) {
            case SERVLET:
                contextClass = Class.forName(&quot;org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext&quot;);
                break;
            case REACTIVE:
                contextClass = Class.forName(&quot;org.springframework.boot.web.reactive.context.AnnotationConfigReactiveWebServerApplicationContext&quot;);
                break;
            default:
                contextClass = Class.forName(&quot;org.springframework.context.annotation.AnnotationConfigApplicationContext&quot;);
            }
        } catch (ClassNotFoundException var3) {
            throw new IllegalStateException(&quot;Unable create a default ApplicationContext, please specify an ApplicationContextClass&quot;, var3);
        }
    }

    return (ConfigurableApplicationContext)BeanUtils.instantiateClass(contextClass);
}
</code></pre>
<p>终其实是调用了 <code>AnnotationConfigServletWebServerApplicationContext</code>的默认构造方法。我们看一下这个方法做了什么事情。这里只是简单的设置了一个 <code>reader</code>和一个 <code>scanner</code>，作用于 bean 的扫描工作。</p>
<p>接下来是获取 ExceptionReporter，获取 ExceptionReporter 的方式主要还是和之前 Listener 的方式一致,通过 getSpringFactoriesInstances 来获取所有的 SpringBootExceptionReporter。</p>
<h4 id="124-准备容器">1.2.4 准备容器</h4>
<pre><code class="language-java">private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment,
        SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) {
    // 为 ApplicationContext 设置 environment
    context.setEnvironment(environment);
    // 执行容器后置处理器
    postProcessApplicationContext(context);
    // 执行容器中的ApplicationContextInitializer
    applyInitializers(context);
        // 发送 ContextPrepareEvent，通知各个监听器。
    listeners.contextPrepared(context);
    if (this.logStartupInfo) {
        // 打印启动新包括 pid 和 用户等。
        logStartupInfo(context.getParent() == null);
        // 打印 Profile 信息
        logStartupProfileInfo(context);
    }
    // Add boot specific singleton beans
    ConfigurableListableBeanFactory beanFactory = context.getBeanFactory();
    // 将启动参数作为 bean 注入到容器中
    beanFactory.registerSingleton(&quot;springApplicationArguments&quot;, applicationArguments);
    if (printedBanner != null) {
        // 将banner 注入到容器中
        beanFactory.registerSingleton(&quot;springBootBanner&quot;, printedBanner);
    }
    if (beanFactory instanceof DefaultListableBeanFactory) {
        // 设置不允许定义同名的BeanDefinition，重复注册时抛出异常
        ((DefaultListableBeanFactory) beanFactory)
                .setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding);
    }
    if (this.lazyInitialization) {
        // 如果是懒加载，则添加懒加载后置处理器。
        context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor());
    }
    // 获取启动类的参数
    Set&lt;Object&gt; sources = getAllSources();
    Assert.notEmpty(sources, &quot;Sources must not be empty&quot;);
    // 加载启动类，并将其注入到容器中
    load(context, sources.toArray(new Object[0]));
    // 发布 ApplicationPreparedEvent 事件
    listeners.contextLoaded(context);
}
</code></pre>
<p><strong>postProcessApplicationContext</strong><br>
其主要实现如下：<br>
1、首先是指定<code>beanNameGenerator</code>,默认情况下不会进入这里，在没有自定义 beanNameGenerator的情况下，<code>AnnotatedBeanDefinitionReader</code>和<code>ClassPathBeanDefinitionScanner</code>的默认实现是AnnotationBeanNameGenerator，即看是否有 value 定义值，假如没有就将首字母变成小写做为bean的名称。<br>
2、查看是否存在<code>resourceLoader</code>有的话就添加到 beanFactory 中。</p>
<pre><code class="language-java">protected void postProcessApplicationContext(ConfigurableApplicationContext context) {
    if (this.beanNameGenerator != null) {
        context.getBeanFactory().registerSingleton(&quot;org.springframework.context.annotation.internalConfigurationBeanNameGenerator&quot;, this.beanNameGenerator);
    }

    if (this.resourceLoader != null) {
        if (context instanceof GenericApplicationContext) {
            ((GenericApplicationContext)context).setResourceLoader(this.resourceLoader);
        }

        if (context instanceof DefaultResourceLoader) {
            ((DefaultResourceLoader)context).setClassLoader(this.resourceLoader.getClassLoader());
        }
    }
}
</code></pre>
<p><strong>执行 initializer</strong><br>
我们上面提到在初始化 SpringApplication 的时候会加载所有的 ApplicationContextInitializer，到这里就使用到了这些 initializer ，调用每个initializer 的 initialize 方法，并将 Context 作为参数传递进去。</p>
<pre><code class="language-java">protected void applyInitializers(ConfigurableApplicationContext context) {
    Iterator var2 = this.getInitializers().iterator();

    while(var2.hasNext()) {
        ApplicationContextInitializer initializer = (ApplicationContextInitializer)var2.next();
        Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument(initializer.getClass(), ApplicationContextInitializer.class);
        Assert.isInstanceOf(requiredType, context, &quot;Unable to call initializer.&quot;);
        initializer.initialize(context);
    }
}
</code></pre>
<p>1、DelegatingApplicationContextInitializer: 从environment中获取context.initializer.classes属性，默认为 null，可以使用多个使用逗号隔开，然后将调用这些类的 initialize 方法。<br>
2、SharedMetadataReaderFactoryContextInitializer 主要是在 beanFactory 中添加一个CachingMetadataReaderFactoryPostProcessor 会在 refreshContext 中被执行。<br>
3、ContextIdApplicationContextInitializer 将 Spring.application.name 作为 ContextId 设置到容器中。<br>
4、ConfigurationWarningsApplicationContextInitializer 向beanFacotory 中注册一个 ConfigurationWarningsPostProcessor 作用是添加一下检查。默认有一个ComponentScanPackageCheck，作用是检查@ComponentScan扫描的包路径是否合法.<br>
5、ServerPortInfoApplicationContextInitializer 向 ApplicationContext 中注册一个 ApplicationListener 用于监听WebServerInitializedEvent事件，向Environment中添加端口号local.sever.port。<br>
ConditionEvaluationReportLoggingListener 向容器中注册一个 ConditionEvaluationReportListener 主要用于打印日志。</p>
<p><strong>执行 ApplicationPrepareContext 通知</strong></p>
<p><strong>load 加载</strong></p>
<pre><code class="language-java">protected void load(ApplicationContext context, Object[] sources) {
    // 打印日志
    if (logger.isDebugEnabled()) {
        logger.debug(&quot;Loading source &quot; + StringUtils.arrayToCommaDelimitedString(sources));
    }
    // 初始化 BeanDefinitionLoader
    BeanDefinitionLoader loader = createBeanDefinitionLoader(getBeanDefinitionRegistry(context), sources);
    // 假如 BeanDefinition 不为空，就将其设置到 loader 中。
    if (this.beanNameGenerator != null) {
        loader.setBeanNameGenerator(this.beanNameGenerator);
    }
    // 如果 resourceLoader  不为空，就将 resourceLoader 设置到 loader 中
    if (this.resourceLoader != null) {
        loader.setResourceLoader(this.resourceLoader);
    }
    // 如果 environment  不为空，就将 environment 设置到 loader 中
    if (this.environment != null) {
        loader.setEnvironment(this.environment);
    }
    // 调用 loader 的 load 方法
    loader.load();
}
</code></pre>
<p>我们先来看一下 createBeanDefinitionLoader 方法：</p>
<pre><code class="language-java">BeanDefinitionLoader(BeanDefinitionRegistry registry, Object... sources) {
    Assert.notNull(registry, &quot;Registry must not be null&quot;);
    Assert.notEmpty(sources, &quot;Sources must not be empty&quot;);
    this.sources = sources;
    this.annotatedReader = new AnnotatedBeanDefinitionReader(registry);
    this.xmlReader = new XmlBeanDefinitionReader(registry);
    if (this.isGroovyPresent()) {
        this.groovyReader = new GroovyBeanDefinitionReader(registry);
    }

    this.scanner = new ClassPathBeanDefinitionScanner(registry);
    this.scanner.addExcludeFilter(new BeanDefinitionLoader.ClassExcludeFilter(sources));
}
</code></pre>
<p>主要做了两件事情：<br>
1、设置 Reader ，包括 AnnotatedBeanDefinitionReader 和 XmlBeanDefinitionReader 假如是Groovy 环境就生成 GroovyBeanDefinitionReader 。<br>
2、设置 Scanner ，主要是 ClassPathBeanDefinitionScanner ,然后检查 Application 中是否存在 ExcludeFilter ，有的话加入到 scanner 中。</p>
<p>接着看load方法：</p>
<pre><code class="language-java">public int load() {
    int count = 0;
    Object[] var2 = this.sources;
    int var3 = var2.length;

    for(int var4 = 0; var4 &lt; var3; ++var4) {
        Object source = var2[var4];
        count += this.load(source);
    }

    return count;
}
</code></pre>
<p>这里的主要逻辑是遍历所有的 sources，这里的其实就是我们的 Main 类。最终调用了 load(Class&lt;?&gt; source) 方法，最终调用了 annotatedReader#register(source)方法。</p>
<h4 id="125-刷新容器">1.2.5 刷新容器</h4>
<pre><code class="language-java">private void refreshContext(ConfigurableApplicationContext context) {
    this.refresh(context);
    if (this.registerShutdownHook) {
        try {
            context.registerShutdownHook();
        } catch (AccessControlException var3) {
        }
    }
}
</code></pre>
<p>主要做两件事情：<br>
1、假如需要注册关闭钩子的话，向 Context 注册关闭钩子。<br>
2、调用 refresh 方法，刷新容器。<br>
我们直接来看一下 refresh 方法，其最终调用了 AbstractApplicationContext 的 refresh 方法。其主要内容如下：</p>
<pre><code class="language-java"> public void refresh() throws BeansException, IllegalStateException {
        synchronized(this.startupShutdownMonitor) {
            //1、准备刷新容器。
            this.prepareRefresh();
            ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory();
            this.prepareBeanFactory(beanFactory);

            try {
                this.postProcessBeanFactory(beanFactory);
                this.invokeBeanFactoryPostProcessors(beanFactory);
                this.registerBeanPostProcessors(beanFactory);
                this.initMessageSource();
                this.initApplicationEventMulticaster();
                this.onRefresh();
                this.registerListeners();
                this.finishBeanFactoryInitialization(beanFactory);
                this.finishRefresh();
            } catch (BeansException var9) {
                if (this.logger.isWarnEnabled()) {
                    this.logger.warn(&quot;Exception encountered during context initialization - cancelling refresh attempt: &quot; + var9);
                }

                this.destroyBeans();
                this.cancelRefresh(var9);
                throw var9;
            } finally {
                this.resetCommonCaches();
            }
        }
    }
</code></pre>
<p>这个方法主要有如下步骤：<br>
1、准备刷新容器。<br>
2、初始化 BeanFactory。<br>
3、对 BeanFactory 进行各种功能的填充，如对 @Autowrite 和 @Qualify 的支持就是这步加入的。<br>
4、调用 postProcessBeanFactory 的扩展点。<br>
5、激活各种 beanFactory 处理器。<br>
6、注册拦截 bean 创建的 bean处理器，这里仅仅是创建而已，最后 getBean 的时候才会真正的调用。<br>
7、初始化 Context 的 MessageSource，为一些国际化的内容。<br>
8、初始化 ApplicationEventMulticaster 并放到 bean 工厂中。<br>
9、扩展点，为其他的 Context 子类来初始化其 bean。<br>
10、在所有的 bean 中找到 listener bean，并将其注册到广播器中。<br>
11、初始化剩下的单例 （no-lazy-init）<br>
12、完成刷新过程，并发出 ContextRefreshEvent 通知。<br>
13、清除缓存。</p>
<h5 id="1251-准备刷新容器">1.2.5.1 准备刷新容器</h5>
<pre><code class="language-java">protected void prepareRefresh() {
    this.startupDate = System.currentTimeMillis();
    this.closed.set(false);
    this.active.set(true);
    if (this.logger.isInfoEnabled()) {
        this.logger.info(&quot;Refreshing &quot; + this);
    }

    this.initPropertySources();
    this.getEnvironment().validateRequiredProperties();
    this.earlyApplicationEvents = new LinkedHashSet();
}
</code></pre>
<p>上面代码比较简单，主要做了如下事情：<br>
1、设置容器启动时间。<br>
2、设置启动状态。<br>
3、调用 initPropertySources 方法，调用到的是 GenericWebApplicationContext 的 initPropertySources 方法，最终调用如下方法：<br>
4、将当前的 ApplicationListeners 放置到 earlyApplicationListeners 中。</p>
<pre><code class="language-java">public static void initServletPropertySources(MutablePropertySources sources, @Nullable ServletContext servletContext, @Nullable ServletConfig servletConfig) {
    Assert.notNull(sources, &quot;'propertySources' must not be null&quot;);
    String name = &quot;servletContextInitParams&quot;;
    if (servletContext != null &amp;&amp; sources.contains(name) &amp;&amp; sources.get(name) instanceof StubPropertySource) {
        sources.replace(name, new ServletContextPropertySource(name, servletContext));
    }

    name = &quot;servletConfigInitParams&quot;;
    if (servletConfig != null &amp;&amp; sources.contains(name) &amp;&amp; sources.get(name) instanceof StubPropertySource) {
        sources.replace(name, new ServletConfigPropertySource(name, servletConfig));
    }

    //1、如果 `servletContext` 不为空，且是 StubPropertySource 的子类，那么将其转为 `ServletContextPropertySource`.
    //2、如果 `servletConfig` 不为空，且是 StubPropertySource 的子类，那么将其转为 `ServletContextPropertySource`.
    //但是这里的  `servletContext`  和  `servletConfig`  都为空，所以不会进入。
}
</code></pre>
<h5 id="1252-初始化-beanfactory">1.2.5.2 初始化 BeanFactory</h5>
<pre><code class="language-java">protected ConfigurableListableBeanFactory obtainFreshBeanFactory() {
    this.refreshBeanFactory();
    ConfigurableListableBeanFactory beanFactory = this.getBeanFactory();
    if (this.logger.isDebugEnabled()) {
        this.logger.debug(&quot;Bean factory for &quot; + this.getDisplayName() + &quot;: &quot; + beanFactory);
    }

    return beanFactory;
}
</code></pre>
<p>主要做两件事情， <code>refreshBeanFactory</code>，<code>初始化BeanFactory</code>，最终调用了 <code>GenericApplicationContext#refreshBeanFactory</code>，如下：</p>
<pre><code class="language-java">protected final void refreshBeanFactory() throws IllegalStateException {
    if (!this.refreshed.compareAndSet(false, true)) {
        throw new IllegalStateException(&quot;GenericApplicationContext does not support multiple refresh attempts: just call 'refresh' once&quot;);
    } else {
        this.beanFactory.setSerializationId(this.getId());
    }
}
</code></pre>
<p>1、设置 refresh 的状态为 <code>TRUE</code>。<br>
2、为 beanFactory 设置<code>setSerializationId</code> ，这个里是 <code>application</code>，其主要由三段式组成 <code>ApplicationName:profile:port</code>。</p>
<p>接下来分析一下 getBeanFactory 方法：</p>
<pre><code class="language-java">public final ConfigurableListableBeanFactory getBeanFactory() {
    return this.beanFactory;
}
</code></pre>
<p>最终还是调用了返回当前 context 的beanFactory，返回一个<code>DefaultListableBeanFactory</code>。</p>
<h5 id="1253-preparebeanfactory">1.2.5.3 prepareBeanFactory</h5>
<pre><code class="language-java">protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) {
    beanFactory.setBeanClassLoader(this.getClassLoader());
    beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader()));
    beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, this.getEnvironment()));
    beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this));
    beanFactory.ignoreDependencyInterface(EnvironmentAware.class);
    beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class);
    beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class);
    beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class);
    beanFactory.ignoreDependencyInterface(MessageSourceAware.class);
    beanFactory.ignoreDependencyInterface(ApplicationContextAware.class);
    beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory);
    beanFactory.registerResolvableDependency(ResourceLoader.class, this);
    beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this);
    beanFactory.registerResolvableDependency(ApplicationContext.class, this);
    beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this));
    if (beanFactory.containsBean(&quot;loadTimeWeaver&quot;)) {
        beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory));
        beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader()));
    }

    if (!beanFactory.containsLocalBean(&quot;environment&quot;)) {
        beanFactory.registerSingleton(&quot;environment&quot;, this.getEnvironment());
    }

    if (!beanFactory.containsLocalBean(&quot;systemProperties&quot;)) {
        beanFactory.registerSingleton(&quot;systemProperties&quot;, this.getEnvironment().getSystemProperties());
    }

    if (!beanFactory.containsLocalBean(&quot;systemEnvironment&quot;)) {
        beanFactory.registerSingleton(&quot;systemEnvironment&quot;, this.getEnvironment().getSystemEnvironment());
    }
}
</code></pre>
<p>1、为 beanFactory 设置类加载器，为当前 context 的类加载器。<br>
2、设置 beanFactory 的 BeanExpressionResolver 为 StandardBeanExpressionResolver。<br>
3、beanFactory增加一个默认的 PropertyEditor,主要用于对 bean 的属性设置进行管理。<br>
4、为 beanFactory 增加一个 BeanPostProcessor 为 ApplicationContextAwareProcessor。<br>
5、将 EnvironmentAware、EmbeddedValueResolverAware、ResourceLoaderAware、ApplicationEventPublisherAware、MessageSourceAware、ApplicationContextAware、添加到忽略自动装配的接口中。,当spring将ApplicationContextAwareProcessor注册后,那么在invokeAwareInterfaces中直接,调用的Aware类已经不是普通的bean了,如ResourceLoaderAware,那么需要在spring做bean的依赖注入时忽略它们。<br>
6、将当前 Context 注册为解析如下依赖的注入对象，包括 BeanFactory、ResourceLoader、ApplicationEventPublisher、ApplicationContext。比如说我们调用 @Autowrite 注入 ApplicationContext 就是注入当前的 Context。<br>
7、注册 BeanPostProcessor ， ApplicationListenerDetector 。<br>
8、添加默认的系统环境bean。</p>
<h5 id="1254-postprocessbeanfactory">1.2.5.4 postProcessBeanFactory</h5>
<p>该方法最终调用了子类的 <code>AnnotationConfigServletWebApplicationContext#postProcessBeanFactory</code> ,</p>
<pre><code class="language-java">protected void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) {
    super.postProcessBeanFactory(beanFactory);
    if (this.basePackages != null &amp;&amp; this.basePackages.length &gt; 0) {
        this.scanner.scan(this.basePackages);
    }

    if (!this.annotatedClasses.isEmpty()) {
        this.reader.register(ClassUtils.toClassArray(this.annotatedClasses));
    }
}
</code></pre>
<p>主要做了三件事情：<br>
1、为 BeanFactory 设置了一个为ServletContextAwareProcessor 类型的 BeanPostProcessor，并设置了忽略接口ServletContextAware.<br>
2、假如basePackage 大于 0 的话，就调用 scanner 的 scan 方法。<br>
3、如果 annotatedClasses 大于 0 的话，就调用 AnnotatedBeanDefinitionReader 的 register 方法。</p>
<h5 id="1255-激活各种-bean-处理器">1.2.5.5 激活各种 bean 处理器</h5>
<pre><code class="language-java">protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) {
    PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, this.getBeanFactoryPostProcessors());
    if (beanFactory.getTempClassLoader() == null &amp;&amp; beanFactory.containsBean(&quot;loadTimeWeaver&quot;)) {
        beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory));
        beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader()));
    }
}
</code></pre>
<p>1、调用PostProcessorRegistrationDelegate#invokeBeanFactoryPostProcessors.<br>
2、如果beanFactory.getTempClassLoader() 等于null并且 beanFactory含有loadTimeWeaver的定义的话,就向beanFactory添加一个LoadTimeWeaverAwareProcessor,然后设置TempClassLoader 为 ContextTypeMatchClassLoader.</p>
<p>其中最重要的就是调用 invokeBeanDefinitionRegistryPostProcessors 方法中，调用了 <code>ConfigurationClassPostProcessor</code>,主要负责加载大部分的 <code>BeanDefinition</code>注册到 registry 中。具体流程如下：</p>
<pre><code class="language-java">public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) {
    int registryId = System.identityHashCode(registry);
    if (this.registriesPostProcessed.contains(registryId)) {
        throw new IllegalStateException(&quot;postProcessBeanDefinitionRegistry already called on this post-processor against &quot; + registry);
    } else if (this.factoriesPostProcessed.contains(registryId)) {
        throw new IllegalStateException(&quot;postProcessBeanFactory already called on this post-processor against &quot; + registry);
    } else {
        this.registriesPostProcessed.add(registryId);
        this.processConfigBeanDefinitions(registry);
    }
}
</code></pre>
<p>主要做了两件事情：<br>
1、生成当前 registry 的 id，然后到 <code>registriesPostProcessed</code>和 <code>registriesPostProcessed</code>中查找，是否存在，主要作用是去重。<br>
2、假如没有重复调用 <code>processConfigBeanDefinitions</code>去加载。</p>
<p>我们进入<code>processConfigBeanDefinitions</code>中：</p>
<pre><code class="language-java">public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) {
    List&lt;BeanDefinitionHolder&gt; configCandidates = new ArrayList&lt;&gt;();
// 获取已经注册的 bean 名称，这里一般是我们的启动 Application 类。
    String[] candidateNames = registry.getBeanDefinitionNames();

    for (String beanName : candidateNames) {
        BeanDefinition beanDef = registry.getBeanDefinition(beanName);
// 假如 beanDefinition 中的 ConfigurationClass 属性为 full 或者 lite 那代表已经解析过了，跳过。
        if (beanDef.getAttribute(ConfigurationClassUtils.CONFIGURATION_CLASS_ATTRIBUTE) != null) {
            if (logger.isDebugEnabled()) {
                logger.debug(&quot;Bean definition has already been processed as a configuration class: &quot; + beanDef);
            }
        }
// 判断当前类是否是 config 类，假如是就加入到 configCandidates 中。
        else if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) {
            configCandidates.add(new BeanDefinitionHolder(beanDef, beanName));
        }
    }
// 假如 configCandidates 为空就返回
    if (configCandidates.isEmpty()) {
        return;
    }
// 对 configCandidates 使用 @Order 注解进行排序
    configCandidates.sort((bd1, bd2) -&gt; {
        int i1 = ConfigurationClassUtils.getOrder(bd1.getBeanDefinition());
        int i2 = ConfigurationClassUtils.getOrder(bd2.getBeanDefinition());
        return Integer.compare(i1, i2);
    });

    // Detect any custom bean name generation strategy supplied through the enclosing application context
    SingletonBeanRegistry sbr = null;
    if (registry instanceof SingletonBeanRegistry) {
        sbr = (SingletonBeanRegistry) registry;
        if (!this.localBeanNameGeneratorSet) {
            BeanNameGenerator generator = (BeanNameGenerator) sbr.getSingleton(
                    AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR);
            if (generator != null) {
// 设置 beanNameGenerator
                this.componentScanBeanNameGenerator = generator;
                this.importBeanNameGenerator = generator;
            }
        }
    }
    if (this.environment == null) {
        this.environment = new StandardEnvironment();
    }
//实例化 ConfigurationClassParser 为后续解析准备。
    ConfigurationClassParser parser = new ConfigurationClassParser(
            this.metadataReaderFactory, this.problemReporter, this.environment,
            this.resourceLoader, this.componentScanBeanNameGenerator, registry);
// 初始话 candidates 和 alreadyParsed 两个集合
    Set&lt;BeanDefinitionHolder&gt; candidates = new LinkedHashSet&lt;&gt;(configCandidates);
    Set&lt;ConfigurationClass&gt; alreadyParsed = new HashSet&lt;&gt;(configCandidates.size());
    do {
// 进行解析
        parser.parse(candidates);
        parser.validate();
// 获取解析到的 ConfigurationClass
        Set&lt;ConfigurationClass&gt; configClasses = new LinkedHashSet&lt;&gt;(parser.getConfigurationClasses());
        configClasses.removeAll(alreadyParsed);
        // Read the model and create bean definitions based on its content
        if (this.reader == null) {
            this.reader = new ConfigurationClassBeanDefinitionReader(
                    registry, this.sourceExtractor, this.resourceLoader, this.environment,
                    this.importBeanNameGenerator, parser.getImportRegistry());
        }
// 加载 ConfigurationClass 的 beanDefinition
        this.reader.loadBeanDefinitions(configClasses);
// 添加到 ConfigurationClass 中。
        alreadyParsed.addAll(configClasses);
        candidates.clear();
        if (registry.getBeanDefinitionCount() &gt; candidateNames.length) {
            String[] newCandidateNames = registry.getBeanDefinitionNames();
            Set&lt;String&gt; oldCandidateNames = new HashSet&lt;&gt;(Arrays.asList(candidateNames));
            Set&lt;String&gt; alreadyParsedClasses = new HashSet&lt;&gt;();
            for (ConfigurationClass configurationClass : alreadyParsed) {
                alreadyParsedClasses.add(configurationClass.getMetadata().getClassName());
            }
            for (String candidateName : newCandidateNames) {
                if (!oldCandidateNames.contains(candidateName)) {
                    BeanDefinition bd = registry.getBeanDefinition(candidateName);
                    if (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) &amp;&amp;
                            !alreadyParsedClasses.contains(bd.getBeanClassName())) {
                        candidates.add(new BeanDefinitionHolder(bd, candidateName));
                    }
                }
            }
            candidateNames = newCandidateNames;
        }
    }
    while (!candidates.isEmpty());
    // Register the ImportRegistry as a bean in order to support ImportAware @Configuration classes
    if (sbr != null &amp;&amp; !sbr.containsSingleton(IMPORT_REGISTRY_BEAN_NAME)) {
        sbr.registerSingleton(IMPORT_REGISTRY_BEAN_NAME, parser.getImportRegistry());
    }

    if (this.metadataReaderFactory instanceof CachingMetadataReaderFactory) {
        // Clear cache in externally provided MetadataReaderFactory; this is a no-op
        // for a shared cache since it'll be cleared by the ApplicationContext.
        ((CachingMetadataReaderFactory) this.metadataReaderFactory).clearCache();
    }
}
</code></pre>
<p>主要做了如下7件事情：<br>
1、获取已经注册的bean名称进行遍历：<br>
2、对configCandidates 进行 排序,按照@Order 配置的值进行排序。<br>
3、如果BeanDefinitionRegistry 是SingletonBeanRegistry 子类的话,将registry强转为SingletonBeanRegistry。<br>
4、实例化ConfigurationClassParser 为了解析各个配置类.实例化2个set,candidates 用于将之前加入的configCandidates 进行去重,alreadyParsed 用于判断是否处理过。<br>
5、进行解析。<br>
6、如果SingletonBeanRegistry 不包含org.springframework.context.annotation.ConfigurationClassPostProcessor.importRegistry,则注册一个,bean 为 ImportRegistry. 一般都会进行注册的。<br>
7、清除缓存。</p>
<p>我们先来看一下判断该 bean 是否为<code>configClass</code>的方法。</p>
<pre><code class="language-java">public static boolean checkConfigurationClassCandidate(
        BeanDefinition beanDef, MetadataReaderFactory metadataReaderFactory) {
// 获取类名，假如不存在则返回。
    String className = beanDef.getBeanClassName();
    if (className == null || beanDef.getFactoryMethodName() != null) {
        return false;
    }
// 获取 AnnotationMetadata
    AnnotationMetadata metadata;
    if (beanDef instanceof AnnotatedBeanDefinition &amp;&amp;
            className.equals(((AnnotatedBeanDefinition) beanDef).getMetadata().getClassName())) {
// 检查是否可以是 AnnotatedBeanDefinition ，是就直接从 BeanDefinition 中获取
        metadata = ((AnnotatedBeanDefinition) beanDef).getMetadata();
    }
    else if (beanDef instanceof AbstractBeanDefinition &amp;&amp; ((AbstractBeanDefinition) beanDef).hasBeanClass()) {
// 如果BeanDefinition 是 AnnotatedBeanDefinition的实例,并且beanDef 有 beanClass 属性存在没有则实例化StandardAnnotationMetadata
        Class&lt;?&gt; beanClass = ((AbstractBeanDefinition) beanDef).getBeanClass();
        if (BeanFactoryPostProcessor.class.isAssignableFrom(beanClass) ||
                BeanPostProcessor.class.isAssignableFrom(beanClass) ||
                AopInfrastructureBean.class.isAssignableFrom(beanClass) ||
                EventListenerFactory.class.isAssignableFrom(beanClass)) {
            return false;
        }
        metadata = AnnotationMetadata.introspect(beanClass);
    }
    else {
        try {
// 否则 通过MetadataReaderFactory 中的MetadataReader 进行读取
            MetadataReader metadataReader = metadataReaderFactory.getMetadataReader(className);
            metadata = metadataReader.getAnnotationMetadata();
        }
        catch (IOException ex) {
            if (logger.isDebugEnabled()) {
                logger.debug(&quot;Could not find class file for introspecting configuration annotations: &quot; +
                        className, ex);
            }
            return false;
        }
    }
// 如果存在Configuration 注解,则为BeanDefinition 设置configurationClass属性为full
    Map&lt;String, Object&gt; config = metadata.getAnnotationAttributes(Configuration.class.getName());
    if (config != null &amp;&amp; !Boolean.FALSE.equals(config.get(&quot;proxyBeanMethods&quot;))) {
        beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_FULL);
    }
// 如果AnnotationMetadata 中有Component,ComponentScan,Import,ImportResource 注解中的任意一个,或者存在 被@bean 注解的方法,则返回true
    else if (config != null || isConfigurationCandidate(metadata)) {
        beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_LITE);
    }
    else {
        return false;
    }
    Integer order = getOrder(metadata);
    if (order != null) {
        beanDef.setAttribute(ORDER_ATTRIBUTE, order);
    }
    return true;
}
</code></pre>
<p>接着来看<code>ConfigurationClassParser</code>的<code>parser()</code>:</p>
<pre><code class="language-java">public void parse(Set&lt;BeanDefinitionHolder&gt; configCandidates) {
    for (BeanDefinitionHolder holder : configCandidates) {
        BeanDefinition bd = holder.getBeanDefinition();
        try {
            if (bd instanceof AnnotatedBeanDefinition) {
                parse(((AnnotatedBeanDefinition) bd).getMetadata(), holder.getBeanName());
            }
            else if (bd instanceof AbstractBeanDefinition &amp;&amp; ((AbstractBeanDefinition) bd).hasBeanClass()) {
                parse(((AbstractBeanDefinition) bd).getBeanClass(), holder.getBeanName());
            }
            else {
                parse(bd.getBeanClassName(), holder.getBeanName());
            }
        }
        catch (BeanDefinitionStoreException ex) {
            throw ex;
        }
        catch (Throwable ex) {
            throw new BeanDefinitionStoreException(
                    &quot;Failed to parse configuration class [&quot; + bd.getBeanClassName() + &quot;]&quot;, ex);
        }
    }
    this.deferredImportSelectorHandler.process();
}
</code></pre>
<p>其主要做了两件事情：<br>
1、遍历configCandidates ,进行处理.根据BeanDefinition 的类型 做不同的处理,一般都会调用ConfigurationClassParser#parse 进行解析。<br>
2、处理ImportSelect。<br>
我们先来看一下第一步：</p>
<pre><code class="language-java">protected void processConfigurationClass(ConfigurationClass configClass, Predicate&lt;String&gt; filter) throws IOException {
    if (this.conditionEvaluator.shouldSkip(configClass.getMetadata(), ConfigurationPhase.PARSE_CONFIGURATION)) {
        return;
    }
    ConfigurationClass existingClass = this.configurationClasses.get(configClass);
    if (existingClass != null) {
        if (configClass.isImported()) {
            if (existingClass.isImported()) {
                existingClass.mergeImportedBy(configClass);
            }
            // Otherwise ignore new imported config class; existing non-imported class overrides it.
            return;
        }
        else {
            // Explicit bean definition found, probably replacing an import.
            // Let's remove the old one and go with the new one.
            this.configurationClasses.remove(configClass);
            this.knownSuperclasses.values().removeIf(configClass::equals);
        }
    }

    // Recursively process the configuration class and its superclass hierarchy.
    SourceClass sourceClass = asSourceClass(configClass, filter);
    do {
        sourceClass = doProcessConfigurationClass(configClass, sourceClass, filter);
    }
    while (sourceClass != null);

    this.configurationClasses.put(configClass, configClass);
}
</code></pre>
<p>主要做如下4件事情：<br>
1、调用 shouldSkip 方法来判断该 configClass 是否需要 跳过。<br>
2、处理Imported 的情况。<br>
3、递归调用进行解析。<br>
4、添加到configurationClasses中。<br>
我们先来看一下真正解析的步骤 <code>doProcessConfigurationClass</code> 方法：</p>
<pre><code class="language-java">protected final SourceClass doProcessConfigurationClass(
        ConfigurationClass configClass, SourceClass sourceClass, Predicate&lt;String&gt; filter)
        throws IOException {

    if (configClass.getMetadata().isAnnotated(Component.class.getName())) {
        // Recursively process any member (nested) classes first
        processMemberClasses(configClass, sourceClass, filter);
    }

    // Process any @PropertySource annotations
    for (AnnotationAttributes propertySource : AnnotationConfigUtils.attributesForRepeatable(
            sourceClass.getMetadata(), PropertySources.class,
            org.springframework.context.annotation.PropertySource.class)) {
        if (this.environment instanceof ConfigurableEnvironment) {
            processPropertySource(propertySource);
        }
        else {
            logger.info(&quot;Ignoring @PropertySource annotation on [&quot; + sourceClass.getMetadata().getClassName() +
                    &quot;]. Reason: Environment must implement ConfigurableEnvironment&quot;);
        }
    }

    // Process any @ComponentScan annotations
    Set&lt;AnnotationAttributes&gt; componentScans = AnnotationConfigUtils.attributesForRepeatable(
            sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class);
    if (!componentScans.isEmpty() &amp;&amp;
            !this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) {
        for (AnnotationAttributes componentScan : componentScans) {
            // The config class is annotated with @ComponentScan -&gt; perform the scan immediately
            Set&lt;BeanDefinitionHolder&gt; scannedBeanDefinitions =
                    this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName());
            // Check the set of scanned definitions for any further config classes and parse recursively if needed
            for (BeanDefinitionHolder holder : scannedBeanDefinitions) {
                BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition();
                if (bdCand == null) {
                    bdCand = holder.getBeanDefinition();
                }
                if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) {
                    parse(bdCand.getBeanClassName(), holder.getBeanName());
                }
            }
        }
    }
    // Process any @Import annotations
    processImports(configClass, sourceClass, getImports(sourceClass), filter, true);
    // Process any @ImportResource annotations
    AnnotationAttributes importResource =
            AnnotationConfigUtils.attributesFor(sourceClass.getMetadata(), ImportResource.class);
    if (importResource != null) {
        String[] resources = importResource.getStringArray(&quot;locations&quot;);
        Class&lt;? extends BeanDefinitionReader&gt; readerClass = importResource.getClass(&quot;reader&quot;);
        for (String resource : resources) {
            String resolvedResource = this.environment.resolveRequiredPlaceholders(resource);
            configClass.addImportedResource(resolvedResource, readerClass);
        }
    }
    // Process individual @Bean methods
    Set&lt;MethodMetadata&gt; beanMethods = retrieveBeanMethodMetadata(sourceClass);
    for (MethodMetadata methodMetadata : beanMethods) {
        configClass.addBeanMethod(new BeanMethod(methodMetadata, configClass));
    }

    // Process default methods on interfaces
    processInterfaces(configClass, sourceClass);

    // Process superclass, if any
    if (sourceClass.getMetadata().hasSuperClass()) {
        String superclass = sourceClass.getMetadata().getSuperClassName();
        if (superclass != null &amp;&amp; !superclass.startsWith(&quot;java&quot;) &amp;&amp;
                !this.knownSuperclasses.containsKey(superclass)) {
            this.knownSuperclasses.put(superclass, configClass);
            // Superclass found, return its annotation metadata and recurse
            return sourceClass.getSuperClass();
        }
    }
    // No superclass -&gt; processing is complete
    return null;
}
</code></pre>
<p>主要做了如下8件事情：<br>
1、如果该类使用 @Component 注解，调用 processMemberClasses 方法，其主要作用是将类放到 importStack 中，并且判断是否有循环依赖度问题。<br>
2、处理@PropertySource.通过遍历该类中的@PropertySource的注解,如果该类中的environment是ConfigurableEnvironment 子类的话,则调用processPropertySource进行处理。<br>
3、处理@ComponentScan,通过遍历该类上的@ComponentScan 注解，并使用 conditionEvaluator.shouldSkip 进行判断是否需要跳过。没有就通过ComponentScanAnnotationParser#parse方法进行扫描：<br>
4、处理@Import 注解<br>
5、处理 @ImportResource 注解，先重 config 类中查找是否存在该注解，假如存在，就获取其 location 属性，然后遍历 location 位置中的 bean，加入到 configClass 中的 ImportedResource。<br>
6、处理 @Bean 的方法，遍历 @Bean 的方法，并放到 configClass 的 BeanMethod 中。<br>
7、遍历 configClass 的所有接口的 @Bean 的方法，并放到 configClass 的 BeanMethod 中。<br>
8、如果存在父类的话，就将父类放到 knownSuperclasses 中，并返回，返回就类似于递归调用。否则返回 null。</p>
<p>我们回到ConfigurationClassPostProcessor的processConfigBeanDefinitions方法中，接下来是调用 ConfigurationClassBeanDefinitionReader#loadBeanDefinitions 方法。</p>
<pre><code class="language-java">public void loadBeanDefinitions(Set&lt;ConfigurationClass&gt; configurationModel) {
    TrackedConditionEvaluator trackedConditionEvaluator = new TrackedConditionEvaluator();
    for (ConfigurationClass configClass : configurationModel) {
        loadBeanDefinitionsForConfigurationClass(configClass, trackedConditionEvaluator);
    }
}
</code></pre>
<p>主要做两件事情：<br>
1、实例化 TrackedConditionEvaluator；<br>
2、遍历configurationModel ，使用loadBeanDefinitionsForConfigurationClass 方法加载 BeanDefinition。</p>
<pre><code class="language-java">private void loadBeanDefinitionsForConfigurationClass(
        ConfigurationClass configClass, TrackedConditionEvaluator trackedConditionEvaluator) {

    if (trackedConditionEvaluator.shouldSkip(configClass)) {
        String beanName = configClass.getBeanName();
        if (StringUtils.hasLength(beanName) &amp;&amp; this.registry.containsBeanDefinition(beanName)) {
            this.registry.removeBeanDefinition(beanName);
        }
        this.importRegistry.removeImportingClass(configClass.getMetadata().getClassName());
        return;
    }

    if (configClass.isImported()) {
        registerBeanDefinitionForImportedConfigurationClass(configClass);
    }
    for (BeanMethod beanMethod : configClass.getBeanMethods()) {
        loadBeanDefinitionsForBeanMethod(beanMethod);
    }

    loadBeanDefinitionsFromImportedResources(configClass.getImportedResources());
    loadBeanDefinitionsFromRegistrars(configClass.getImportBeanDefinitionRegistrars());
}
</code></pre>
<p>主要做了如下几件事情：<br>
1、调用 trackedConditionEvaluator 来判断条件注解，是否需要跳过这个 config 类。如果需要，就将这个类从容器中移除，并且从 importRegistry 中移除。<br>
2、如果当前类中存在@Import 注解，调用 registerBeanDefinitionForImportedConfigurationClass 方法进行注册<br>
3、遍历BeanMethods,依次对其调用loadBeanDefinitionsForBeanMethod进行注册。<br>
4、处理 @ImportResource 注解,具体如下：</p>
<pre><code class="language-java">private void loadBeanDefinitionsFromImportedResources(
        Map&lt;String, Class&lt;? extends BeanDefinitionReader&gt;&gt; importedResources) {
    Map&lt;Class&lt;?&gt;, BeanDefinitionReader&gt; readerInstanceCache = new HashMap&lt;&gt;();
// 遍历所有的 importedResources
    importedResources.forEach((resource, readerClass) -&gt; {
// 如果是 BeanDefinitionReader，就查看是否是 groovy 类，假如不是就使用 XmlBeanDefinitionReader 类
        if (BeanDefinitionReader.class == readerClass) {
            if (StringUtils.endsWithIgnoreCase(resource, &quot;.groovy&quot;)) {
                readerClass = GroovyBeanDefinitionReader.class;
            }
            else {
                readerClass = XmlBeanDefinitionReader.class;
            }
        }
// 尝试重 readerInstanceCache 读取 BeanDefinitionReader 假如没有就实例化。
        BeanDefinitionReader reader = readerInstanceCache.get(readerClass);
        if (reader == null) {
            try {
                reader = readerClass.getConstructor(BeanDefinitionRegistry.class).newInstance(this.registry);
                if (reader instanceof AbstractBeanDefinitionReader) {
                    AbstractBeanDefinitionReader abdr = ((AbstractBeanDefinitionReader) reader);
                    abdr.setResourceLoader(this.resourceLoader);
                    abdr.setEnvironment(this.environment);
                }
                readerInstanceCache.put(readerClass, reader);
            }
            catch (Throwable ex) {
                throw new IllegalStateException(
                        &quot;Could not instantiate BeanDefinitionReader class [&quot; + readerClass.getName() + &quot;]&quot;);
            }
        }
// 加载 bean
        reader.loadBeanDefinitions(resource);
    });
}
</code></pre>
<p>主要做了四件事情：<br>
1、遍历所有的 importedResources 。<br>
2、选择 BeanDefinitionReader，假如是 groovy 类，就使用 GroovyBeanDefinitionReader 不是就使用 XmlBeanDefinitionReader<br>
3、尝试从readerInstanceCache中获取对应的BeanDefinitionReader,如果不存在,则实例化一个,然后放入到readerInstanceCache缓存中。<br>
4、调用 BeanDefinitionReader#loadBeanDefinitions 进行加载 bean。<br>
5、注册@Import注解中的ImportBeanDefinitionRegistrar接口的registerBeanDefinitions。</p>
<p>接下来，我们继续看容器刷新流程</p>
<h5 id="1256-registerbeanpostprocessors-方法">1.2.5.6 registerBeanPostProcessors 方法</h5>
<p>这个方法最终调用了 PostProcessorRegistrationDelegate#registerBeanPostProcessors，如下：</p>
<pre><code class="language-java">public static void registerBeanPostProcessors(
        ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) {

    String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false);

    // Register BeanPostProcessorChecker that logs an info message when
    // a bean is created during BeanPostProcessor instantiation, i.e. when
    // a bean is not eligible for getting processed by all BeanPostProcessors.
    int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length;
    beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount));

    // Separate between BeanPostProcessors that implement PriorityOrdered,
    // Ordered, and the rest.
    List&lt;BeanPostProcessor&gt; priorityOrderedPostProcessors = new ArrayList&lt;&gt;();
    List&lt;BeanPostProcessor&gt; internalPostProcessors = new ArrayList&lt;&gt;();
    List&lt;String&gt; orderedPostProcessorNames = new ArrayList&lt;&gt;();
    List&lt;String&gt; nonOrderedPostProcessorNames = new ArrayList&lt;&gt;();
    for (String ppName : postProcessorNames) {
        if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) {
            BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class);
            priorityOrderedPostProcessors.add(pp);
            if (pp instanceof MergedBeanDefinitionPostProcessor) {
                internalPostProcessors.add(pp);
            }
        }
        else if (beanFactory.isTypeMatch(ppName, Ordered.class)) {
            orderedPostProcessorNames.add(ppName);
        }
        else {
            nonOrderedPostProcessorNames.add(ppName);
        }
    }

    // First, register the BeanPostProcessors that implement PriorityOrdered.
    sortPostProcessors(priorityOrderedPostProcessors, beanFactory);
    registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors);

    // Next, register the BeanPostProcessors that implement Ordered.
    List&lt;BeanPostProcessor&gt; orderedPostProcessors = new ArrayList&lt;&gt;(orderedPostProcessorNames.size());
    for (String ppName : orderedPostProcessorNames) {
        BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class);
        orderedPostProcessors.add(pp);
        if (pp instanceof MergedBeanDefinitionPostProcessor) {
            internalPostProcessors.add(pp);
        }
    }
    sortPostProcessors(orderedPostProcessors, beanFactory);
    registerBeanPostProcessors(beanFactory, orderedPostProcessors);

    // Now, register all regular BeanPostProcessors.
    List&lt;BeanPostProcessor&gt; nonOrderedPostProcessors = new ArrayList&lt;&gt;(nonOrderedPostProcessorNames.size());
    for (String ppName : nonOrderedPostProcessorNames) {
        BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class);
        nonOrderedPostProcessors.add(pp);
        if (pp instanceof MergedBeanDefinitionPostProcessor) {
            internalPostProcessors.add(pp);
        }
    }
    registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors);

    // Finally, re-register all internal BeanPostProcessors.
    sortPostProcessors(internalPostProcessors, beanFactory);
    registerBeanPostProcessors(beanFactory, internalPostProcessors);

    // Re-register post-processor for detecting inner beans as ApplicationListeners,
    // moving it to the end of the processor chain (for picking up proxies etc).
    beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext));
}
</code></pre>
<p>1、先从 beanFactory 中获取 BeanPostProcessor 类型的 bean。<br>
2、添加一个BeanPostProcessor ， BeanPostProcessorChecker ，主要用于日志打印。<br>
3、遍历所有的 postProcessorNames ：<br>
（1）将所有实现了 PriorityOrdered 接口的 bean 放到 priorityOrderedPostProcessors 中。<br>
（2）如果bean 即实现了 PriorityOrdered 接口，也实现了 MergedBeanDefinitionPostProcessor 接口的话，将其放到 internalPostProcessors 中。<br>
（3）假如 bean 实现了 Ordered 接口放到 orderedPostProcessorNames 中。<br>
（4）假如都没有，就放到 nonOrderedPostProcessorNames 中。<br>
4、注册 priorityOrderedPostProcessors 的 BPP<br>
5、注册 orderedPostProcessors 的 BPP<br>
6、注册所有 nonOrderedPostProcessors 的 BPP<br>
7、 注册所有MergedBeanDefinitionPostProcessor类型的BeanPostProcessor,并非是重复注册.如下:<br>
8、在最后新增一个BPP 是 ApplicationListenerDetector。</p>
<h5 id="1257-initmessagesource-方法">1.2.5.7 initMessageSource 方法</h5>
<p>1、从 beanFactory 中读取 messageSource ，看是否存在，假如存在，获取之，然后判断是是HierarchicalMessageSource 类型假如是，就将其 ParentMessageSource 设置为 nternalParentMessageSource。<br>
2、如果不存在，就实例化 DelegatingMessageSource 作为 getInternalParentMessageSource 调用的结果。</p>
<h5 id="1258-初始化-applicationeventmulticaster">1.2.5.8 初始化 ApplicationEventMulticaster</h5>
<p>这里的逻辑主要是 如果存在用户自定义的广播器，那么就将其设置为默认广播器。假如不存在就初始化 SimpleApplicationEventMulticaster 作为默认的广播器。</p>
<h5 id="1259-onrefresh">1.2.5.9 Onrefresh</h5>
<p>这个接口是留给子类的扩展点 ServletWebServerApplicationContext 的代码如下：</p>
<pre><code class="language-java">@Override
protected void onRefresh() {
    super.onRefresh();
    try {
        createWebServer();
    }
    catch (Throwable ex) {
        throw new ApplicationContextException(&quot;Unable to start web server&quot;, ex);
    }
}
</code></pre>
<p>1 、先调用父类的 onRefresh 方法<br>
2、调用完父类的 Onfresh 后，创建一个嵌入的Servlet容器.</p>
<pre><code class="language-java">public static ThemeSource initThemeSource(ApplicationContext context) {
    if (context.containsLocalBean(THEME_SOURCE_BEAN_NAME)) {
        ThemeSource themeSource = context.getBean(THEME_SOURCE_BEAN_NAME, ThemeSource.class);
        // Make ThemeSource aware of parent ThemeSource.
        if (context.getParent() instanceof ThemeSource &amp;&amp; themeSource instanceof HierarchicalThemeSource) {
            HierarchicalThemeSource hts = (HierarchicalThemeSource) themeSource;
            if (hts.getParentThemeSource() == null) {
                // Only set parent context as parent ThemeSource if no parent ThemeSource
                // registered already.
                hts.setParentThemeSource((ThemeSource) context.getParent());
            }
        }
        if (logger.isDebugEnabled()) {
            logger.debug(&quot;Using ThemeSource [&quot; + themeSource + &quot;]&quot;);
        }
        return themeSource;
    }
    else {
        // Use default ThemeSource to be able to accept getTheme calls, either
        // delegating to parent context's default or to local ResourceBundleThemeSource.
        HierarchicalThemeSource themeSource = null;
        if (context.getParent() instanceof ThemeSource) {
            themeSource = new DelegatingThemeSource();
            themeSource.setParentThemeSource((ThemeSource) context.getParent());
        }
        else {
            themeSource = new ResourceBundleThemeSource();
        }
        if (logger.isDebugEnabled()) {
            logger.debug(&quot;Unable to locate ThemeSource with name '&quot; + THEME_SOURCE_BEAN_NAME +
                    &quot;': using default [&quot; + themeSource + &quot;]&quot;);
        }
        return themeSource;
    }
}
</code></pre>
<p>1、如果context中有themeSource的定义<br>
（1）从context 获取,id 为themeSource type为ThemeSource 的 bean<br>
（2）如果父容器实现了ThemeSource,并且ThemeSource 是HierarchicalThemeSource 的子类,并且HierarchicalThemeSource 的ParentThemeSource 没有进行设置.则将父容器赋值给HierarchicalThemeSource的ParentThemeSource<br>
2、如果context中没有themeSource的定义<br>
（1）如果父容器为ThemeSource的子类,则实例化DelegatingThemeSource,并将父容器赋值给DelegatingThemeSource的ParentThemeSource<br>
（2）否则实例化为DelegatingThemeSource</p>
<h5 id="12510-registerlisteners-注册监听器">1.2.5.10 registerListeners 注册监听器</h5>
<p>这个方法的主要作用是初始化所有的 listener</p>
<pre><code class="language-java">protected void registerListeners() {
    // Register statically specified listeners first.
    for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) {
        getApplicationEventMulticaster().addApplicationListener(listener);
    }

    // Do not initialize FactoryBeans here: We need to leave all regular beans
    // uninitialized to let post-processors apply to them!
    String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false);
    for (String listenerBeanName : listenerBeanNames) {
        getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName);
    }

    // Publish early application events now that we finally have a multicaster...
    Set&lt;ApplicationEvent&gt; earlyEventsToProcess = this.earlyApplicationEvents;
    this.earlyApplicationEvents = null;
    if (!CollectionUtils.isEmpty(earlyEventsToProcess)) {
        for (ApplicationEvent earlyEvent : earlyEventsToProcess) {
            getApplicationEventMulticaster().multicastEvent(earlyEvent);
        }
    }
}
</code></pre>
<p>1、硬编码方式注册的监听器添加到SimpleApplicationEventMulticaster中的defaultRetriever的applicationListeners<br>
2、将注册到配置文件中的 ApplicationListener 找出来，并添加到SimpleApplicationEventMulticaster中的defaultRetriever。<br>
3、 将之前发生的 earlyApplicationEvents 重复发送一遍。</p>
<h5 id="12511-finishbeanfactoryinitialization">1.2.5.11 finishBeanFactoryInitialization</h5>
<p>该方法主要作用是 初始化剩余的单例（non-lazy-init)）</p>
<pre><code class="language-java">protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) {
    // Initialize conversion service for this context.
    if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp;
        beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) {
        beanFactory.setConversionService(
                beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class));
    }

    // Register a default embedded value resolver if no bean post-processor
    // (such as a PropertyPlaceholderConfigurer bean) registered any before:
    // at this point, primarily for resolution in annotation attribute values.
    if (!beanFactory.hasEmbeddedValueResolver()) {
        beanFactory.addEmbeddedValueResolver(strVal -&gt; getEnvironment().resolvePlaceholders(strVal));
    }

    // Initialize LoadTimeWeaverAware beans early to allow for registering their transformers early.
    String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false);
    for (String weaverAwareName : weaverAwareNames) {
        getBean(weaverAwareName);
    }

    // Stop using the temporary ClassLoader for type matching.
    beanFactory.setTempClassLoader(null);

    // Allow for caching all bean definition metadata, not expecting further changes.
    beanFactory.freezeConfiguration();

    // Instantiate all remaining (non-lazy-init) singletons.
    beanFactory.preInstantiateSingletons();
}
</code></pre>
<p>1、如果 beaFactory 中存在 CONVERSION_SERVICE_BEAN_NAME name 的 bean，并且类型为 ConversionService.class ，将其设置到 beanFactory 中。<br>
2、如果 beanFactory 中没有 EmbeddedValueResolver，添加一个。<br>
3、设置 type 为 LoadTimeWeaverAware 的bean。<br>
4、设置TempClassLoader 为null<br>
5、冻结所有 bean 的定义，也就是从这里开始，所有的 bean 后面都不允许被修改了。<br>
6、初始化剩下的单实例.</p>
<h5 id="12512-finishrefresh">1.2.5.12 finishRefresh</h5>
<pre><code class="language-java">protected void finishRefresh() {
    // Clear context-level resource caches (such as ASM metadata from scanning).
    clearResourceCaches();

    // Initialize lifecycle processor for this context.
    initLifecycleProcessor();

    // Propagate refresh to lifecycle processor first.
    getLifecycleProcessor().onRefresh();

    // Publish the final event.
    publishEvent(new ContextRefreshedEvent(this));

    // Participate in LiveBeansView MBean, if active.
    LiveBeansView.registerApplicationContext(this);
}
</code></pre>
<p>1、清理 resource caches。<br>
2、初始化LifecycleProcessor.<br>
3、调用 LifecycleProcessor 的 onrefresh 方法。<br>
4、发布ContextRefreshedEvent 事件.</p>
<h5 id="12513-resetcommoncaches">1.2.5.13 resetCommonCaches</h5>
<p>1、清除 ReflectionUtils 缓存。<br>
2、清除 AnnotationUtils 缓存。<br>
3、清除 ResolvableType 缓存。</p>
<h4 id="126-afterrefresh-spring-容器的后置处理器">1.2.6 afterRefresh Spring 容器的后置处理器</h4>
<h4 id="127-通知所有-listener-结束启动">1.2.7 通知所有 listener 结束启动</h4>
<p>这里最终调用了 EventPublishingRunListener#started 方法：</p>
<pre><code class="language-java">public void started(ConfigurableApplicationContext context) {
    context.publishEvent(new ApplicationStartedEvent(this.application, this.args, context));
    AvailabilityChangeEvent.publish(context, LivenessState.CORRECT);
}
</code></pre>
<p>1、首先是调用 context.publishEvent 来发布启动完成事件。<br>
2、调用 AvailabilityChangeEvent 发布 CORRECT 事件，代表启动成功。</p>
<h4 id="128-调用所有-runner-的-run-方法">1.2.8 调用所有 runner 的 run 方法</h4>
<pre><code class="language-java">private void callRunners(ApplicationContext context, ApplicationArguments args) {
    List&lt;Object&gt; runners = new ArrayList&lt;&gt;();
    runners.addAll(context.getBeansOfType(ApplicationRunner.class).values());
    runners.addAll(context.getBeansOfType(CommandLineRunner.class).values());
    AnnotationAwareOrderComparator.sort(runners);
    for (Object runner : new LinkedHashSet&lt;&gt;(runners)) {
        if (runner instanceof ApplicationRunner) {
            callRunner((ApplicationRunner) runner, args);
        }
        if (runner instanceof CommandLineRunner) {
            callRunner((CommandLineRunner) runner, args);
        }
    }
}
</code></pre>
<p>首先查找所有的 ApplicationRunner 和 CommandLineRunner ，然后遍历调用他们的 run 方法。</p>
<h4 id="129-通知所有-listener-running-事件">1.2.9 通知所有 listener running 事件。</h4>
<blockquote>
<p>⚠️大家可以参考这个图，因为没钱买，只能放链接了 <a href="https://www.processon.com/view/60605358f346fb6d9ef1bd9c?fromnew=1" title="springboot启动流程">springboot启动流程</a></p>
</blockquote>
<h2 id="2-springboot-扩展点">2 springboot 扩展点</h2>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1676171242267.png" alt="" loading="lazy"></figure>
<h3 id="21-applicationcontextinitializer">2.1 ApplicationContextInitializer</h3>
<blockquote>
<p>org.springframework.context.ApplicationContextInitializer<br>
这是整个spring容器在刷新之前初始化<code>ConfigurableApplicationContext</code>的回调接口，简单来说，就是在容器刷新之前调用此类的<code>initialize</code>方法。这个点允许被用户自己扩展。用户可以在整个spring容器还没被初始化之前做一些事情。</p>
</blockquote>
<p>可以想到的场景可能为，在最开始激活一些配置，或者利用这时候class还没被类加载器加载的时机，进行动态字节码注入等操作。</p>
<pre><code class="language-java">public class TestApplicationContextInitializer implements ApplicationContextInitializer {
    @Override
    public void initialize(ConfigurableApplicationContext applicationContext) {
        System.out.println(&quot;[ApplicationContextInitializer]&quot;);
    }
}
</code></pre>
<p>因为这时候spring容器还没被初始化，所以想要自己的扩展的生效，有以下三种方式：</p>
<ol>
<li>在启动类中用<code>springApplication.addInitializers(new TestApplicationContextInitializer())</code>语句加入</li>
<li>配置文件配置<br>
<code>context.initializer.classes=com.example.demo.TestApplicationContextInitializer</code></li>
<li>Spring SPI扩展，在<code>spring.factories</code>中加入<code>org.springframework.context.ApplicationContextInitializer=com.example.demo.TestApplicationContextInitializer</code></li>
</ol>
<h3 id="22-beandefinitionregistrypostprocessor">2.2 BeanDefinitionRegistryPostProcessor</h3>
<blockquote>
<p>org.springframework.beans.factory.support.BeanDefinitionRegistryPostProcessor<br>
这个接口在读取项目中的<code>beanDefinition</code>之后执行，提供一个补充的扩展点</p>
</blockquote>
<p>使用场景：你可以在这里动态注册自己的beanDefinition，可以加载classpath之外的bean</p>
<pre><code class="language-java">public class TestBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor {
    @Override
    public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException {
        System.out.println(&quot;[BeanDefinitionRegistryPostProcessor] postProcessBeanDefinitionRegistry&quot;);
    }

    @Override
    public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
        System.out.println(&quot;[BeanDefinitionRegistryPostProcessor] postProcessBeanFactory&quot;);
    }
}
</code></pre>
<h3 id="23-beanfactorypostprocessor">2.3 BeanFactoryPostProcessor</h3>
<blockquote>
<p>org.springframework.beans.factory.config.BeanFactoryPostProcessor<br>
这个接口是<code>beanFactory</code>的扩展接口，调用时机在spring在读取beanDefinition信息之后，实例化bean之前。</p>
</blockquote>
<p>在这个时机，用户可以通过实现这个扩展接口来自行处理一些东西，比如修改已经注册的beanDefinition的元信息。</p>
<pre><code class="language-java">public class TestBeanFactoryPostProcessor implements BeanFactoryPostProcessor {
@Override
public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
    System.out.println(&quot;[BeanFactoryPostProcessor]&quot;);
}
}
</code></pre>
<h3 id="24-instantiationawarebeanpostprocessor">2.4 InstantiationAwareBeanPostProcessor</h3>
<blockquote>
<p>org.springframework.beans.factory.config.InstantiationAwareBeanPostProcessor<br>
该接口继承了BeanPostProcess接口，区别如下：</p>
</blockquote>
<p><font color="red">BeanPostProcess接口只在bean的初始化阶段进行扩展（注入spring上下文前后），而InstantiationAwareBeanPostProcessor接口在此基础上增加了3个方法，把可扩展的范围增加了实例化阶段和属性注入阶段。</font></p>
<p>该类主要的扩展点有以下5个方法，主要在bean生命周期的两大阶段：<font color=red>实例化阶段</font>和<font color=red>初始化阶段</font>，下面一起进行说明，按调用顺序为：</p>
<ol>
<li>postProcessBeforeInstantiation：实例化bean之前，相当于new这个bean之前</li>
<li>postProcessAfterInstantiation：实例化bean之后，相当于new这个bean之后</li>
<li>postProcessPropertyValues：bean已经实例化完成，在属性注入时阶段触发，@Autowired,@Resource等注解原理基于此方法实现</li>
<li>postProcessBeforeInitialization：初始化bean之前，相当于把bean注入spring上下文之前</li>
<li>postProcessAfterInitialization：初始化bean之后，相当于把bean注入spring上下文之后</li>
</ol>
<p>使用场景：这个扩展点非常有用 ，无论是写中间件和业务中，都能利用这个特性。比如对实现了某一类接口的bean在各个生命期间进行收集，或者对某个类型的bean进行统一的设值等等。</p>
<pre><code class="language-java">public class TestInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor {

@Override
public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {
    System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] before initialization &quot; + beanName);
    return bean;
}

@Override
public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {
    System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] after initialization &quot; + beanName);
    return bean;
}

@Override
public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException {
    System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] before instantiation &quot; + beanName);
    return null;
}

@Override
public boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException {
    System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] after instantiation &quot; + beanName);
    return true;
}

@Override
public PropertyValues postProcessPropertyValues(PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException {
    System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] postProcessPropertyValues &quot; + beanName);
    return pvs;
}
</code></pre>
<h3 id="25-smartinstantiationawarebeanpostprocessor">2.5 SmartInstantiationAwareBeanPostProcessor</h3>
<blockquote>
<p>org.springframework.beans.factory.config.SmartInstantiationAwareBeanPostProcessor</p>
</blockquote>
<p>该扩展接口有3个触发点方法：</p>
<ol>
<li><code>predictBeanType</code>：该触发点发生在postProcessBeforeInstantiation之前(在图上并没有标明，因为一般不太需要扩展这个点)，这个方法用于预测Bean的类型，返回第一个预测成功的Class类型，如果不能预测返回null；当你调用BeanFactory.getType(name)时当通过bean的名字无法得到bean类型信息时就调用该回调方法来决定类型信息。</li>
<li><code>determineCandidateConstructors</code>：该触发点发生在postProcessBeforeInstantiation之后，用于确定该bean的构造函数之用，返回的是该bean的所有构造函数列表。用户可以扩展这个点，来自定义选择相应的构造器来实例化这个bean。</li>
<li><code>getEarlyBeanReference</code>：该触发点发生在postProcessAfterInstantiation之后，当有循环依赖的场景，当bean实例化好之后，为了防止有循环依赖，会提前暴露回调方法，用于bean实例化的后置处理。这个方法就是在提前暴露的回调方法中触发。</li>
</ol>
<pre><code class="language-java">public class TestSmartInstantiationAwareBeanPostProcessor implements SmartInstantiationAwareBeanPostProcessor {

    @Override
    public Class&lt;?&gt; predictBeanType(Class&lt;?&gt; beanClass, String beanName) throws BeansException {
        System.out.println(&quot;[TestSmartInstantiationAwareBeanPostProcessor] predictBeanType &quot; + beanName);
        return beanClass;
    }

    @Override
    public Constructor&lt;?&gt;[] determineCandidateConstructors(Class&lt;?&gt; beanClass, String beanName) throws BeansException {
        System.out.println(&quot;[TestSmartInstantiationAwareBeanPostProcessor] determineCandidateConstructors &quot; + beanName);
        return null;
    }

    @Override
    public Object getEarlyBeanReference(Object bean, String beanName) throws BeansException {
        System.out.println(&quot;[TestSmartInstantiationAwareBeanPostProcessor] getEarlyBeanReference &quot; + beanName);
        return bean;
    }
}
</code></pre>
<h3 id="26-beanfactoryaware">2.6 BeanFactoryAware</h3>
<blockquote>
<p>org.springframework.beans.factory.BeanFactoryAware<br>
这个类只有一个触发点，发生在bean的实例化之后，注入属性之前，也就是Setter之前。这个类的扩展点方法为setBeanFactory，可以拿到BeanFactory这个属性。</p>
</blockquote>
<p>使用场景为，你可以在bean实例化之后，但还未初始化之前，拿到 BeanFactory，在这个时候，可以对每个bean作特殊化的定制。也或者可以把BeanFactory拿到进行缓存，日后使用。</p>
<p>扩展方式为：</p>
<pre><code class="language-java">public class TestBeanFactoryAware implements BeanFactoryAware {
    @Override
    public void setBeanFactory(BeanFactory beanFactory) throws BeansException {
        System.out.println(&quot;[TestBeanFactoryAware] &quot; + beanFactory.getBean(TestBeanFactoryAware.class).getClass().getSimpleName());
    }
}
</code></pre>
<h3 id="27-applicationcontextawareprocessor">2.7 ApplicationContextAwareProcessor</h3>
<blockquote>
<p>org.springframework.context.support.ApplicationContextAwareProcessor<br>
该类本身并没有扩展点，但是该类内部却有6个扩展点可供实现 ，这些类触发的时机在bean实例化之后，初始化之前</p>
</blockquote>
<p>该类用于执行各种驱动接口，在bean实例化之后，属性填充之后，通过执行扩展接口，来获取对应容器的变量。</p>
<ol>
<li><code>EnvironmentAware</code>：用于获取EnviromentAware的一个扩展类，这个变量非常有用， 可以获得系统内的所有参数。当然个人认为这个Aware没必要去扩展，因为spring内部都可以通过注入的方式来直接获得。</li>
<li><code>EmbeddedValueResolverAware</code>：用于获取StringValueResolver的一个扩展类， StringValueResolver用于获取基于String类型的properties的变量，一般我们都用@Value的方式去获取，如果实现了这个Aware接口，把StringValueResolver缓存起来，通过这个类去获取String类型的变量，效果是一样的。</li>
<li><code>ResourceLoaderAware</code>：用于获取ResourceLoader的一个扩展类，ResourceLoader可以用于获取classpath内所有的资源对象，可以扩展此类来拿到ResourceLoader对象。</li>
<li><code>ApplicationEventPublisherAware</code>：用于获取ApplicationEventPublisher的一个扩展类，ApplicationEventPublisher可以用来发布事件，结合ApplicationListener来共同使用，下文在介绍ApplicationListener时会详细提到。这个对象也可以通过spring注入的方式来获得。</li>
<li><code>MessageSourceAware</code>：用于获取MessageSource的一个扩展类，MessageSource主要用来做国际化。</li>
<li><code>ApplicationContextAware</code>：用来获取ApplicationContext的一个扩展类，ApplicationContext应该是很多人非常熟悉的一个类了，就是spring上下文管理器，可以手动的获取任何在spring上下文注册的bean，我们经常扩展这个接口来缓存spring上下文，包装成静态方法。同时ApplicationContext也实现了BeanFactory，MessageSource，ApplicationEventPublisher等接口，也可以用来做相关接口的事情。</li>
</ol>
<h3 id="28-beannameaware">2.8 BeanNameAware</h3>
<blockquote>
<p>org.springframework.beans.factory.BeanNameAware<br>
可以看到，这个类也是Aware扩展的一种，触发点在bean的初始化之前，也就是<code>postProcessBeforeInitialization</code>之前，这个类的触发点方法只有一个：<code>setBeanName</code></p>
</blockquote>
<p>使用场景为：用户可以扩展这个点，在初始化bean之前拿到spring容器中注册的的beanName，来自行修改这个beanName的值。</p>
<p>扩展方式为：</p>
<pre><code class="language-java">public class NormalBeanA implements BeanNameAware{
    public NormalBeanA() {
        System.out.println(&quot;NormalBean constructor&quot;);
    }

    @Override
    public void setBeanName(String name) {
        System.out.println(&quot;[BeanNameAware] &quot; + name);
    }
}
</code></pre>
<h3 id="29-postconstruct">2.9 @PostConstruct</h3>
<blockquote>
<p>javax.annotation.PostConstruct<br>
这个并不算一个扩展点，其实就是一个标注。其作用是在bean的初始化阶段，如果对一个方法标注了@PostConstruct，会先调用这个方法。这里重点是要关注下这个标准的触发点，这个触发点是在<code>postProcessBeforeInitialization</code>之后，<code>InitializingBean.afterPropertiesSet</code>之前。</p>
</blockquote>
<p>使用场景：用户可以对某一方法进行标注，来进行初始化某一个属性</p>
<p>扩展方式为：</p>
<pre><code class="language-java">public class NormalBeanA {
    public NormalBeanA() {
        System.out.println(&quot;NormalBean constructor&quot;);
    }

    @PostConstruct
    public void init(){
        System.out.println(&quot;[PostConstruct] NormalBeanA&quot;);
    }
}
</code></pre>
<h3 id="210-initializingbean">2.10 InitializingBean</h3>
<blockquote>
<p>org.springframework.beans.factory.InitializingBean<br>
这个类，顾名思义，也是用来初始化bean的。InitializingBean接口为bean提供了初始化方法的方式，它只包括<code>afterPropertiesSet</code>方法，凡是继承该接口的类，在初始化bean的时候都会执行该方法。这个扩展点的触发时机在<code>postProcessAfterInitialization</code>之前。</p>
</blockquote>
<p>使用场景：用户实现此接口，来进行系统启动的时候一些业务指标的初始化工作。</p>
<p>扩展方式为：</p>
<pre><code class="language-java">public class NormalBeanA implements InitializingBean{
    @Override
    public void afterPropertiesSet() throws Exception {
        System.out.println(&quot;[InitializingBean] NormalBeanA&quot;);
    }
}
</code></pre>
<h3 id="211-factorybean">2.11 FactoryBean</h3>
<blockquote>
<p>org.springframework.beans.factory.FactoryBean<br>
一般情况下，Spring通过反射机制利用bean的class属性指定支线类去实例化bean，在某些情况下，实例化Bean过程比较复杂，如果按照传统的方式，则需要在bean中提供大量的配置信息。配置方式的灵活性是受限的，这时采用编码的方式可能会得到一个简单的方案。Spring为此提供了一个<code>org.springframework.bean.factory.FactoryBean</code>的工厂类接口，用户可以通过实现该接口定制实例化Bean的逻辑。</p>
</blockquote>
<p>它们隐藏了实例化一些复杂bean的细节，给上层应用带来了便利。从Spring3.0开始，FactoryBean开始支持泛型，即接口声明改为<code>FactoryBean&lt;T&gt;</code>的形式</p>
<p>使用场景：用户可以扩展这个类，来为要实例化的bean作一个代理，比如为该对象的所有的方法作一个拦截，在调用前后输出一行log，模仿ProxyFactoryBean的功能。</p>
<pre><code class="language-java">public class TestFactoryBean implements FactoryBean&lt;TestFactoryBean.TestFactoryInnerBean&gt; {

    @Override
    public TestFactoryBean.TestFactoryInnerBean getObject() throws Exception {
        System.out.println(&quot;[FactoryBean] getObject&quot;);
        return new TestFactoryBean.TestFactoryInnerBean();
    }

    @Override
    public Class&lt;?&gt; getObjectType() {
        return TestFactoryBean.TestFactoryInnerBean.class;
    }

    @Override
    public boolean isSingleton() {
        return true;
    }

    public static class TestFactoryInnerBean{

    }
}
</code></pre>
<h3 id="212-smartinitializingsingleton">2.12 SmartInitializingSingleton</h3>
<blockquote>
<p>org.springframework.beans.factory.SmartInitializingSingleton<br>
这个接口中只有一个方法<code>afterSingletonsInstantiated</code>，其作用是是 在spring容器管理的所有单例对象（非懒加载对象）初始化完成之后调用的回调接口。其触发时机为<code>postProcessAfterInitialization</code>之后。</p>
</blockquote>
<p>使用场景：用户可以扩展此接口在对所有单例对象初始化完毕后，做一些后置的业务处理。</p>
<pre><code class="language-java">public class TestSmartInitializingSingleton implements SmartInitializingSingleton {
    @Override
    public void afterSingletonsInstantiated() {
        System.out.println(&quot;[TestSmartInitializingSingleton]&quot;);
    }
}
</code></pre>
<h3 id="213-commandlinerunner">2.13 CommandLineRunner</h3>
<blockquote>
<p>org.springframework.boot.CommandLineRunner<br>
这个接口也只有一个方法：<code>run(String... args)</code>，触发时机为整个项目启动完毕后，自动执行。如果有多个<code>CommandLineRunner</code>，可以利用<code>@Order</code>来进行排序。</p>
</blockquote>
<p>使用场景：用户扩展此接口，进行启动项目之后一些业务的预处理。</p>
<pre><code class="language-java">public class TestCommandLineRunner implements CommandLineRunner {

    @Override
    public void run(String... args) throws Exception {
        System.out.println(&quot;[TestCommandLineRunner]&quot;);
    }
}
</code></pre>
<h3 id="214-disposablebean">2.14 DisposableBean</h3>
<blockquote>
<p>org.springframework.beans.factory.DisposableBean<br>
这个扩展点也只有一个方法：<code>destroy()</code>，其触发时机为当此对象销毁时，会自动执行这个方法。比如说运行<code>applicationContext.registerShutdownHook</code>时，就会触发这个方法。</p>
</blockquote>
<pre><code class="language-java">public class NormalBeanA implements DisposableBean {
    @Override
    public void destroy() throws Exception {
        System.out.println(&quot;[DisposableBean] NormalBeanA&quot;);
    }
}
</code></pre>
<h3 id="215-applicationlistener">2.15 ApplicationListener</h3>
<blockquote>
<p>org.springframework.context.ApplicationListener<br>
准确的说，这个应该不算spring&amp;springboot当中的一个扩展点，ApplicationListener可以监听某个事件的<code>event</code>，触发时机可以穿插在业务方法执行过程中，用户可以自定义某个业务事件。但是spring内部也有一些内置事件，这种事件，可以穿插在启动调用中。我们也可以利用这个特性，来自己做一些内置事件的监听器来达到和前面一些触发点大致相同的事情。</p>
</blockquote>
<p>接下来罗列下spring主要的内置事件：</p>
<ol>
<li>
<p>ContextRefreshedEvent<br>
ApplicationContext 被初始化或刷新时，该事件被发布。这也可以在 ConfigurableApplicationContext接口中使用 refresh() 方法来发生。此处的初始化是指：所有的Bean被成功装载，后处理Bean被检测并激活，所有Singleton Bean 被预实例化，ApplicationContext容器已就绪可用。</p>
</li>
<li>
<p>ContextStartedEvent<br>
当使用 ConfigurableApplicationContext （ApplicationContext子接口）接口中的 start() 方法启动 ApplicationContext 时，该事件被发布。你可以调查你的数据库，或者你可以在接受到这个事件后重启任何停止的应用程序。</p>
</li>
<li>
<p>ContextStoppedEvent<br>
当使用 ConfigurableApplicationContext 接口中的 stop() 停止 ApplicationContext 时，发布这个事件。你可以在接受到这个事件后做必要的清理的工作</p>
</li>
<li>
<p>ContextClosedEvent<br>
当使用 ConfigurableApplicationContext接口中的 close()方法关闭 ApplicationContext 时，该事件被发布。一个已关闭的上下文到达生命周期末端；它不能被刷新或重启</p>
</li>
<li>
<p>RequestHandledEvent<br>
这是一个 web-specific 事件，告诉所有 bean HTTP 请求已经被服务。只能应用于使用DispatcherServlet的Web应用。在使用Spring作为前端的MVC控制器时，当Spring处理用户请求结束后，系统会自动触发该事件</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semaphore实现原理]]></title>
        <id>https://q456qq520.github.io/post/semaphore-shi-xian-yuan-li/</id>
        <link href="https://q456qq520.github.io/post/semaphore-shi-xian-yuan-li/">
        </link>
        <updated>2023-02-07T02:31:24.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="1-构造方法">1 构造方法</h2>
<p>从概念上讲，Semaphore维护一组许可，由一个可以递增或递减的计数器值表示，用来控制同时访问特定资源的线程数目。</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="1-构造方法">1 构造方法</h2>
<p>从概念上讲，Semaphore维护一组许可，由一个可以递增或递减的计数器值表示，用来控制同时访问特定资源的线程数目。</p>
<!-- more -->
<p>Semaphore信号量来实现线程间通信，Semaphore支持公平锁和非公平锁，Semaphore底层是通过共享锁来实现的，其支持两种构造函数，如下所示：</p>
<pre><code class="language-java"> // 默认使用非公平锁实现
 public Semaphore(int permits) {
     sync = new NonfairSync(permits);
 }
 ​
 public Semaphore(int permits, boolean fair) {
     sync = fair ? new FairSync(permits) : new NonfairSync(permits);
 }
</code></pre>
<h2 id="2-semaphore方法">2 Semaphore方法</h2>
<pre><code class="language-java">//尝试获取一个信号量，如果信号量不为0，那么将信号量-1，返回
//如果信号量为0，WAITING直到信号量不为0
//可中断
public void acquire() throws InterruptedException

//尝试获取多个信号量，如果信号量足够，那么将信号量-permits，返回
//如果信号量不够，WAITING直到信号量不为0
//可中断 
public void acquire(int permits) throws InterruptedException
    
//同acquire()，但不可中断
public void acquireUninterruptibly()
    
//同acquire(int permits),但不可中断
public void acquireUninterruptibly(int permits)

//释放一个信号量
public void release()
    
//释放permits个信号量
public void release(int permits)

</code></pre>
<h2 id="3-semaphore内部类及继承关系">3 Semaphore内部类及继承关系</h2>
<p><img src="https://q456qq520.github.io/post-images/1675737631273.png" alt="" loading="lazy"><br>
Semaphore与ReentrantLock的内部类的结构相同，类内部总共存在Sync、NonfairSync、FairSync三个类，NonfairSync与FairSync类继承自Sync类，Sync类继承自AbstractQueuedSynchronizer抽象类。</p>
<h3 id="31-类的内部类-sync类">3.1 类的内部类 - Sync类</h3>
<pre><code class="language-java">// 内部类，继承自AQS
abstract static class Sync extends AbstractQueuedSynchronizer {
    // 版本号
    private static final long serialVersionUID = 1192457210091910933L;
    
    // 构造函数
    Sync(int permits) {
        // 设置状态数
        setState(permits);
    }
    
    // 获取许可
    final int getPermits() {
        return getState();
    }

    // 共享模式下非公平策略获取
    final int nonfairTryAcquireShared(int acquires) {
        for (;;) { // 无限循环
            // 获取许可数
            int available = getState();
            // 剩余的许可
            int remaining = available - acquires;
            if (remaining &lt; 0 ||
                compareAndSetState(available, remaining)) // 许可小于0或者比较并且设置状态成功
                return remaining;
        }
    }
    
    // 共享模式下进行释放
    protected final boolean tryReleaseShared(int releases) {
        for (;;) { // 无限循环
            // 获取许可
            int current = getState();
            // 可用的许可
            int next = current + releases;
            if (next &lt; current) // overflow
                throw new Error(&quot;Maximum permit count exceeded&quot;);
            if (compareAndSetState(current, next)) // 比较并进行设置成功
                return true;
        }
    }

    // 根据指定的缩减量减小可用许可的数目
    final void reducePermits(int reductions) {
        for (;;) { // 无限循环
            // 获取许可
            int current = getState();
            // 可用的许可
            int next = current - reductions;
            if (next &gt; current) // underflow
                throw new Error(&quot;Permit count underflow&quot;);
            if (compareAndSetState(current, next)) // 比较并进行设置成功
                return;
        }
    }

    // 获取并返回立即可用的所有许可
    final int drainPermits() {
        for (;;) { // 无限循环
            // 获取许可
            int current = getState();
            if (current == 0 || compareAndSetState(current, 0)) // 许可为0或者比较并设置成功
                return current;
        }
    }
}
</code></pre>
<h3 id="32-类的内部类-nonfairsync类">3.2 类的内部类 - NonfairSync类</h3>
<pre><code class="language-java">static final class NonfairSync extends Sync {
    // 版本号
    private static final long serialVersionUID = -2694183684443567898L;
    
    // 构造函数
    NonfairSync(int permits) {
        super(permits);
    }
    // 共享模式下获取
    protected int tryAcquireShared(int acquires) {
        return nonfairTryAcquireShared(acquires);
    }
}
</code></pre>
<p>从tryAcquireShared方法的源码可知，其会调用父类Sync的nonfairTryAcquireShared方法，表示按照非公平策略进行资源的获取。</p>
<h3 id="33-类的内部类-fairsync类">3.3 类的内部类 - FairSync类</h3>
<pre><code class="language-java">protected int tryAcquireShared(int acquires) {
    for (;;) { // 无限循环
        if (hasQueuedPredecessors()) // 同步队列中存在其他节点
            return -1;
        // 获取许可
        int available = getState();
        // 剩余的许可
        int remaining = available - acquires;
        if (remaining &lt; 0 ||
            compareAndSetState(available, remaining)) // 剩余的许可小于0或者比较设置成功
            return remaining;
    }
}
</code></pre>
<p>从tryAcquireShared方法的源码可知，它使用公平策略来获取资源，它会判断同步队列中是否存在其他的等待节点。</p>
<h3 id="34-类的属性">3.4 类的属性</h3>
<pre><code class="language-java">public class Semaphore implements java.io.Serializable {
    // 版本号
    private static final long serialVersionUID = -3222578661600680210L;
    // 属性
    private final Sync sync;
}
</code></pre>
<p>Semaphore自身只有两个属性，最重要的是sync属性，基于Semaphore对象的操作绝大多数都转移到了对sync的操作。</p>
<h2 id="4-semaphoreacquire流程分析以非公平锁为例">4 Semaphore.acquire流程分析(以非公平锁为例)</h2>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1675737423342.png" alt="" loading="lazy"></figure>
<p>从上图可以看出，针对阻塞线程的部分实现，和ReentrantLock基本一致，我们不做赘述，主要来看下前半部分的源码实现：</p>
<pre><code class="language-java"> // Semaphore.java
 public void acquire() throws InterruptedException {
     sync.acquireSharedInterruptibly(1);
 }
</code></pre>
<pre><code class="language-java"> // AbstractQueuedSynchronizer.java
 public final void acquireSharedInterruptibly(int arg)
         throws InterruptedException {
     // 如果线程是中断状态，抛出异常
     if (Thread.interrupted())
         throw new InterruptedException();
     // 尝试获取共享资源
     if (tryAcquireShared(arg) &lt; 0)
         doAcquireSharedInterruptibly(arg);
 }
</code></pre>
<p>从源码可以看出acquire主要依赖于tryAcquireShared和doAcquireSharedInterruptibly。</p>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1675740380818.png" alt="" loading="lazy"></figure>
<h3 id="41-tryacquireshared">4.1 tryAcquireShared</h3>
<pre><code class="language-java">static final class NonfairSync extends Sync {
    private static final long serialVersionUID = -2694183684443567898L;

    NonfairSync(int permits) {
        super(permits);
    }

    protected int tryAcquireShared(int acquires) {
        return nonfairTryAcquireShared(acquires);
    }
}
</code></pre>
<pre><code class="language-java">final int nonfairTryAcquireShared(int acquires) {
    for (;;) {
        int available = getState();
        //判断是否还有令牌
        int remaining = available - acquires;
        //无论是否还有令牌，都要返回
        if (remaining &lt; 0 ||
            compareAndSetState(available, remaining))
            return remaining;
    }
}
</code></pre>
<pre><code class="language-java"> // AbstractQueuedSynchronizer.java
 protected final boolean compareAndSetState(int expect, int update) {
     // See below for intrinsics setup to support this
     return unsafe.compareAndSwapInt(this, stateOffset, expect, update);
 }
</code></pre>
<p>从代码可以看出这里主要是根据申请的许可证数量，比较时否有许可证数量，如果可用许可证数量小于0，则直接返回，如果大于0，则通过CAS将state设置为可用许可证数量。</p>
<h3 id="42-doacquiresharedinterruptibly">4.2 doAcquireSharedInterruptibly</h3>
<p>当tryAcquireShared中返回的可用许可证数量小于0时，执行doAcquireSharedInterruptibly流程，代码如下：</p>
<pre><code class="language-java">private void doAcquireSharedInterruptibly(int arg)
    throws InterruptedException {
    //加入同步队列
    final Node node = addWaiter(Node.SHARED);
    boolean failed = true;
    try {
        //自旋获取锁
        for (;;) {
            final Node p = node.predecessor();
            //判断上一个节点是否是头节点
            if (p == head) {
                //如果是头节点则尝试获取锁
                int r = tryAcquireShared(arg);
                if (r &gt;= 0) {
                    //获取到锁，通知其他节点
                    setHeadAndPropagate(node, r);
                    p.next = null; // help GC
                    failed = false;
                    return;
                }
            }
            //判断是否需要阻塞线程，设置waitStatus并阻塞
            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;
                parkAndCheckInterrupt())
                throw new InterruptedException();
        }
    } finally {
        if (failed)
            cancelAcquire(node);
    }
}
</code></pre>
<pre><code class="language-java"> // AbstractQueuedSynchronizer.java
 // 在队尾新建Node对象并添加
 private Node addWaiter(Node mode) {
     Node node = new Node(Thread.currentThread(), mode);
     // Try the fast path of enq; backup to full enq on failure
     Node pred = tail;
     if (pred != null) {
         node.prev = pred;
         if (compareAndSetTail(pred, node)) {
             pred.next = node;
             return node;
         }
     }
     enq(node);
     return node;
 }
</code></pre>
<pre><code class="language-java">private void setHeadAndPropagate(Node node, int propagate) {
    Node h = head; // Record old head for check below
    setHead(node);

    if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 ||
        (h = head) == null || h.waitStatus &lt; 0) {
        Node s = node.next;
        if (s == null || s.isShared())
            doReleaseShared();
    }
}
</code></pre>
<p>执行setHeadAndPropagate的主要目的在于，这里能获取到说明在该线程自旋过程中有线程释放了许可证，释放的许可证数量有可能还有剩余，所以传递给其他节点的线程，唤醒其他阻塞状态的线程也尝试去获取许可证。</p>
<h2 id="5-semaphorerelease流程分析以非公平锁为例">5 Semaphore.release流程分析(以非公平锁为例)</h2>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1675738215229.png" alt="" loading="lazy"></figure>
<p>Semaphore.release流程相对而言，就比较简单，将release传递到AQS内部通过CAS更新许可证数量信息，更新完成后，遍历队列中Node节点，将Node waitStatus设置为0，并对对应线程执行unpark，相关代码如下：</p>
<pre><code class="language-java">@ReservedStackAccess
public final boolean releaseShared(int arg) {
    if (tryReleaseShared(arg)) {
        doReleaseShared();
        return true;
    }
    return false;
}
</code></pre>
<pre><code class="language-java">protected final boolean tryReleaseShared(int releases) {
    for (;;) {
        int current = getState();
        int next = current + releases;
        if (next &lt; current) // overflow
            throw new Error(&quot;Maximum permit count exceeded&quot;);
        // 通过CAS更新许可证数量
        if (compareAndSetState(current, next))
            return true;
    }
}
</code></pre>
<pre><code class="language-java">private void doReleaseShared() {
    for (;;) {
        Node h = head;
        if (h != null &amp;&amp; h != tail) {
            int ws = h.waitStatus;
            if (ws == Node.SIGNAL) {
                if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0))
                    continue;            // loop to recheck cases
                unparkSuccessor(h);
            }
            else if (ws == 0 &amp;&amp;
                        !compareAndSetWaitStatus(h, 0, Node.PROPAGATE))
                continue;                // loop on failed CAS
        }
        if (h == head)                   // loop if head changed
            break;
    }
}
</code></pre>
<pre><code class="language-java">private void unparkSuccessor(Node node) {
    int ws = node.waitStatus;
    if (ws &lt; 0)
        compareAndSetWaitStatus(node, ws, 0);

    Node s = node.next;
    if (s == null || s.waitStatus &gt; 0) {
        s = null;
        for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev)
            if (t.waitStatus &lt;= 0)
                s = t;
    }
    if (s != null)
        LockSupport.unpark(s.thread);
}

// 许可证数量更新完成后，调用该方法唤醒线程
private void doReleaseShared() {
    // 自旋
    for (;;) {
        Node h = head;
        if (h != null &amp;&amp; h != tail) {
            int ws = h.waitStatus;
            if (ws == Node.SIGNAL) {
                if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0))
                    continue;            // loop to recheck cases
                // 唤醒后继节点线程抢占许可证
                unparkSuccessor(h);
            }
            else if (ws == 0 &amp;&amp;
                     !compareAndSetWaitStatus(h, 0, Node.PROPAGATE))
                continue;                // loop on failed CAS
        }
        if (h == head)                   // loop if head changed
            break;
    }
}
</code></pre>
<h2 id="6-公平锁">6 公平锁</h2>
<p>我们分析了Smaphore非公平锁的实现，公平锁的实现其本质区别在于在tryAcquireShared中只有当等待队列为空时，才会去尝试更新剩余许可证数量。</p>
<pre><code class="language-java">protected int tryAcquireShared(int acquires) {
    for (;;) {
        //判断是否是头节点
        if (hasQueuedPredecessors())
            return -1;
        int available = getState();
        int remaining = available - acquires;
        if (remaining &lt; 0 ||
            compareAndSetState(available, remaining))
            return remaining;
    }
}

public final boolean hasQueuedPredecessors() {
    // The correctness of this depends on head being initialized
    // before tail and on head.next being accurate if the current
    // thread is first in queue.
    Node t = tail; // Read fields in reverse initialization order
    Node h = head;
    Node s;
    return h != t &amp;&amp;
        ((s = h.next) == null || s.thread != Thread.currentThread());
}
</code></pre>
<h2 id="7-semaphore和reentrantlock的区别">7 Semaphore和ReentrantLock的区别</h2>
<ol>
<li>可重入的性质<br>
Semaphores在本质上是非可重入的 ，这意味着我们不能在同一个线程中第二次获得Semaphore。试图这样做会导致死锁（一个线程与自己死锁）。<br>
另一方面， 可重入锁在本质上是可重入的，允许一个线程使用lock() 方法多次锁定一个特定的资源。</li>
<li>同步机制<br>
Semaphores很适合信号传递（信号机制），线程使用acquire()&amp;release() 方法来标记访问关键资源的开始和结束。<br>
ReentrantLock 使用锁定机制，使用lock() 方法锁定一个特定的资源 ，在对该资源进行特定操作后，使用unlock() 方法释放该锁。</li>
<li>死锁恢复<br>
Semaphores提供了一个强大的死锁恢复机制，因为它使用了一个非所有权的释放机制，因此任何线程都可以释放一个许可，以恢复一个卡住或等待的线程的死锁情况。<br>
在 ReentrantLock的情况下，死锁恢复是有点困难的，因为它使用线程对资源的所有权，通过物理锁定它，只有所有者线程可以解锁该资源。如果所有者Thread进入无限等待或睡眠状态，就不可能释放该特定资源的锁，从而导致死锁情况。</li>
<li>抛出IllegalMonitorStateException<br>
在Semaphores中，没有线程拥有获取或释放许可的所有权，所以任何线程都可以调用release() 方法来释放任何其他线程的许可，没有线程会引发 IllegalMonitorStateException。<br>
在可重入锁中，一个Thread 通过调用lock() 方法成为一个关键共享资源的所有者，如果其他Thread在没有拥有锁的情况下调用unlock() 方法，那么 它将会产生 IllegalMonitorStateException。</li>
<li>修改<br>
任何线程都可以使用Semaphore的acquire() 和release() 方法来修改它的可用许可。<br>
只有通过lock()方法拥有资源的当前所有者线程可以修改ReentrantLock，而其他线程不允许这样做。</li>
</ol>
<p>Semaphores可以用于非所有权-释放语义，即不止一个Thread 可以进入一个关键部分，并且不需要锁定机制来锁定一个共享资源。根据设计，Semaphore对哪个线程调用acquisition()和release()方法是盲目的，它所关心的是许可成为可用的。<br>
如果我们需要可重入互斥或一个简单的互斥 ，那么 ReentrantLock是最好的选择。可重入锁 提供了对锁机制更好的控制，并且允许每次只有一个线程访问关键部分，从而提供了同步性，并消除了在多线程应用程序中工作时的数据不一致问题。</p>
<h2 id="8-场景问题">8 场景问题</h2>
<p><code>semaphore初始化有10个令牌，11个线程同时各调用1次acquire方法，会发生什么?</code><br>
答案：拿不到令牌的线程阻塞，不会继续往下运行。</p>
<p><code>semaphore初始化有10个令牌，一个线程重复调用11次acquire方法，会发生什么?</code><br>
答案：线程阻塞，不会继续往下运行。可能你会考虑类似于锁的重入的问题，很好，但是，令牌没有重入的概念。你只要调用一次acquire方法，就需要有一个令牌才能继续运行。</p>
<p><code>semaphore初始化有1个令牌，1个线程调用一次acquire方法，然后调用两次release方法，之后另外一个线程调用acquire(2)方法，此线程能够获取到足够的令牌并继续运行吗?</code><br>
答案：能，原因是release方法会添加令牌，并不会以初始化的大小为准。</p>
<p><code>semaphore初始化有2个令牌，一个线程调用1次release方法，然后一次性获取3个令牌，会获取到吗?</code><br>
答案：能，原因是release会添加令牌，并不会以初始化的大小为准。Semaphore中release方法的调用并没有限制要在acquire后调用。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[面试题（一）]]></title>
        <id>https://q456qq520.github.io/post/mian-shi-ti-yi/</id>
        <link href="https://q456qq520.github.io/post/mian-shi-ti-yi/">
        </link>
        <updated>2023-02-01T06:50:06.000Z</updated>
        <content type="html"><![CDATA[<h2 id="java基础">JAVA基础</h2>
<h3 id="1-equals与hashcode的区别与联系">1.==、equals与HashCode的区别与联系</h3>
<p>1、equals用于判断两个对象是否相等  == 判断的是地址是否相等（两个实例），具备自反性、一致性、传递性<br>
2、hashCode 返回对象的哈希值int类型，用于再HashTable、HashSet计算存放下标使用（先通过计算hashCode获取下标，下标一致再根据equals判断是否相同的两个对象）</p>
<p>再使用HashCode的类散列表情况下 hashCode和equals具备以下关系：<br>
1)、如果两个对象相等，那么它们的hashCode()值一定相同。<br>
这里的相等是指，通过equals()比较两个对象时返回true。<br>
2)、如果两个对象hashCode()相等，它们并不一定相等。<br>
因为在散列表中，hashCode()相等，即两个键值对的哈希值相等。然而哈希值相等，并不一定能得出键值对相等。补充说一句：“两个不同的键值对，哈希值相等”，这就是哈希冲突。所以这时候 一般我们修改 equals方法时，也需要修改 hashCode方法，不然即使equals方法返回TRUE，但是再HashMap里面因为hashCode的不同，所以不会调用equals方法，导致结果不正确。</p>
<h3 id="2深克隆与浅克隆">2.深克隆与浅克隆</h3>
<p>1.浅克隆：只复制基本类型（包含String类型）的数据，引用类型的数据只复制了引用的地址，引用的对象并没有复制，在新的对象中修改引用类型的数据会影响原对象中的引用。<br>
2.深克隆：是在引用类型的类中也实现了clone，是clone的嵌套，复制后的对象与原对象之间完全不会影响。<br>
3.使用序列化也能完成深复制的功能：对象序列化后写入流中，此时也就不存在引用什么的概念了，再从流中读取，生成新的对象，新对象和原对象之间也是完全互不影响的。<br>
4.使用clone实现的深克隆其实是浅克隆中嵌套了浅克隆，与toString方法类似</p>
<h3 id="3hashmap数据结构hashtable数据结构">3.HashMap数据结构,HashTable数据结构</h3>
<p>哈希表是一种组合的数据结构，它通常的实现方式是数组加链表，或者数组加红黑树。</p>
<p><code>HashMap</code><br>
<a href="/post/java-ji-chu-hashmap">java基础-HashMap</a></p>
<p><code>HashTable</code><br>
HashTable类继承自Dictionary类， 实现了Map接口。 大部分的操作都是通过synchronized锁保护的，是线程安全的，key、value都不可以为null， 每次put方法不允许null值，如果发现是null，则直接抛出异常。它的数据结构：主要是<mark>数组+链表</mark>。</p>
<p>如果在非线程安全的情况下使用，建议使用HashMap替换，如果在线程安全的情况下使用，建议使用ConcurrentHashMap替换。</p>
<h3 id="4concurrenthashmap数据结构">4.ConcurrentHashMap数据结构</h3>
<p><a href="/post/java-bing-fa-san">ConcurrentHashMap数据结构</a></p>
<h3 id="5代理模式及动态代理详解">5.代理模式及动态代理详解</h3>
<p>代理模式的定义：为其他对象提供一种代理以控制对这个对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在客户端和目标对象之间起到中介的作用。</p>
<p><code>静态代理</code><br>
代理对象与目标对象一起实现相同的接口或者继承相同父类，由程序员创建或特定工具自动生成源代码，即在编译时就已经确定了接口，目标类，代理类等。在程序运行之前，代理类的 .class 文件就已经生成。<br>
静态代理优缺点：<br>
优点：在不修改目标对象的功能前提下，可以对目标功能扩展。<br>
缺点：假如又有一个目标类，也要做增强，则还需要新增相对应的代理类，导致我们要手动编写很多代理类。同时，一旦接口增加方法，目标对象与代理对象都要维护。</p>
<p><code>动态代理</code><br>
代理类在程序运行时才创建的代理方式被称为动态代理。</p>
<ol>
<li>
<p>基于JDK原生动态代理实现<br>
JDK动态代理是基于反射机制，生成一个实现代理接口的匿名类，然后重写方法进行方法增强。在调用具体方法前通过调用 InvokeHandler 的 invoke 方法来处理。通过JDK源码分析其实是 Proxy 类的 newProxyInstance方法在运行时动态生成字节码生成代理类（缓存在Java虚拟机内存中），从而创建了一个动态代理对象。<br>
代理类继承了 Proxy 类，因为在Java中是单继承的，所以这就是为什么JDK动态代理中，目标对象一定要实现接口。<br>
它的特点是生成代理类速度很快，但是运行时调用方法操作会比较慢，因为是基于反射机制的，而且只能针对接口编程，即目标对象要实现接口。</p>
</li>
<li>
<p>CGLIB动态代理<br>
Cglib（Code Generation Library）是一个强大的，高性能，高质量的Code生成类库，它是开源的。动态代理是利用 asm 开源包，将目标对象类的 class 文件加载进来，然后修改其字节码生成新的子类来进行扩展处理。即可以在运行期扩展Java类和实现Java接口。<br>
Cglib动态代理注意的2点：</p>
</li>
</ol>
<ul>
<li>被代理类不能是 final 修饰的。</li>
<li>需要扩展的方法不能有 final 或 static 关键字修饰，不然不会被拦截，即执行方法只会执行目标对象的方法，不会执行方法扩展的内容。</li>
</ul>
<p>两种动态代理区别</p>
<ol>
<li>JDK动态代理是基于反射机制，生成一个实现代理接口的匿名类。而Cglib动态代理是基于继承机制，继承被代理类，底层是基于asm第三方框架对代理对象类的class文件加载进来,通过修改其字节码生成子类来处理。</li>
<li>JDK动态代理是生成类的速度快，后续执行类的方法操作慢；Cglib动态代理是生成类的速度慢，后续执行类的方法操作快。</li>
<li>JDK只能针对接口编程，Cglib可以针对类和接口。在Springboot项目中，在配置文件中增加 spring.aop.proxy-target-class=true 即可强制使用Cglib动态代理实现AOP。</li>
<li>如果目标对象实现了接口，默认情况下是采用JDK动态实现AOP，如果目标对象没有实现接口，必须采用CGLIB库动态实现AOP。</li>
</ol>
<h2 id="线程">线程</h2>
<h3 id="6进程-线程-协程">6.进程、线程、协程</h3>
<p><code>进程与线程</code><br>
进程是操作系统进行资源分配的基本单位，每个进程都有自己的独立内存空间。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。</p>
<p>线程又叫做轻量级进程，是进程的一个实体，是处理器任务调度和执行的基本单位位。它是比进程更小的能独立运行的基本单位。线程只拥有一点在运行中必不可少的资源(如程序计数器，一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。</p>
<p>对于操作系统来说，一个任务就是一个进程（Process）。一个进程至少有一个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。</p>
<p>线程进程的区别体现在6个方面：<br>
根本区别：进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位。<br>
资源开销：每个进程都有独立的代码和数据空间，程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一进程的线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器，线程之间切换的开销小。<br>
包含关系：如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线（线程）共同完成的。<br>
内存分配：同一进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的。<br>
影响关系：一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃整个进程都死掉。所以多进程要比多线程健壮。<br>
执行过程：每个独立的进程有程序运行的入口、顺序执行序列和程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。两者均可并发执行。</p>
<p><code>协程</code><br>
协程，又称微线程，是一种用户态的轻量级线程，协程的调度完全由用户控制（也就是在用户态执行）。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到线程的堆区，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。<br>
协程最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和线程切换相比，线程数量越多，协程的性能优势就越明显。不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。此外，一个线程的内存在MB级别，而协程只需要KB级别。</p>
<p>协程与线程的区别</p>
<ol>
<li>一个线程可以有多个协程。</li>
<li>大多数业务场景下，线程进程可以看做是同步机制，而协程则是异步。</li>
<li>线程是抢占式，而协程是非抢占式的，所以需要用户代码释放使用权来切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力。</li>
<li>协程并不是取代线程，而且抽象于线程之上。线程是被分割的CPU资源, 协程是组织好的代码流程, 协程需要线程来承载运行。</li>
</ol>
<h3 id="7linux中java的线程模型">7.linux中，java的线程模型</h3>
<p>JVM 没有限定 Java 线程需要使用哪种线程模型来实现， JVM 只是封装了底层操作系统的差异，而不同的操作系统可能使用不同的线程模型，例如 Linux 和 windows 可能使用了一对一模型，solaris 和 unix 某些版本可能使用多对多模型。所以一谈到 Java 语言的多线程模型，需要针对具体 JVM 实现。</p>
<ol>
<li>使用用户线程实现（多对一模型 M:1）<br>
多个用户线程映射到一个内核线程，用户线程建立在用户空间的线程库上，用户线程的建立、同步、销毁和调度完全在用户态中完成，对内核透明。<br>
<img src="https://q456qq520.github.io/post-images/1675246200667.png" alt="" loading="lazy"><br>
优点：</li>
</ol>
<ol>
<li>线程的上下文切换都发生在用户空间，避免了模态切换（mode switch），减少了性能的开销。</li>
<li>用户线程的创建不受内核资源的限制，可以支持更大规模的线程数量。</li>
</ol>
<p>缺点：</p>
<ol>
<li>所有的线程基于一个内核调度实体即内核线程，这意味着只有一个处理器可以被利用，浪费了其它处理器资源，不支持并行，在多处理器环境下这是不能够被接受的，如果线程因为 I/O 操作陷入了内核态，内核态线程阻塞等待 I/O 数据，则所有的线程都将会被阻塞。</li>
<li>增加了复杂度，所有的线程操作都需要用户程序自己处理，而且在用户空间要想自己实现 “阻塞的时候把线程映射到其他处理器上” 异常困难</li>
</ol>
<ol start="2">
<li>使用内核线程实现（一对一模型 1:1）<br>
每个用户线程都映射到一个内核线程，每个线程都成为一个独立的调度单元，由内核调度器独立调度，一个线程的阻塞不会影响到其他线程，从而保障整个进程继续工作。<br>
<img src="https://q456qq520.github.io/post-images/1675246283087.png" alt="" loading="lazy"><br>
优点：</li>
</ol>
<ol>
<li>每个线程都成为一个独立的调度单元，使用内核提供的线程调度功能及处理器映射，可以完成线程的切换，并将线程的任务映射到其他处理器上，充分利用多核处理器的优势，实现真正的并行。</li>
</ol>
<p>缺点：</p>
<ol>
<li>每创建一个用户级线程都需要创建一个内核级线程与其对应，因此需要消耗一定的内核资源,而内核资源是有限的，所以能创建的线程数量也是有限的。</li>
<li>模态切换频繁，各种线程操作，如创建、析构及同步，都需要进行系统调用，需要频繁的在用户态和内核态之间切换，开销大。</li>
</ol>
<ol start="3">
<li>使用用户线程加轻量级进程混合实现（多对多模型 M:N）<br>
内核线程和用户线程的数量比为 M : N，这种模型需要内核线程调度器和用户空间线程调度器相互操作，本质上是多个线程被映射到了多个内核线程。<br>
<img src="https://q456qq520.github.io/post-images/1675246296511.png" alt="" loading="lazy"><br>
综合了前面两种模型的优点：</li>
</ol>
<ol>
<li>用户线程的创建、切换、析构及同步依然发生在用户空间，能创建数量更多的线程，支持更大规模的并发。</li>
<li>大部分的线程上下文切换都发生在用户空间，减少了模态切换带来的开销。</li>
<li>可以使用内核提供的线程调度功能及处理器映射，充分利用多核处理器的优势，实现真正的并行，并降低了整个进程被完全阻塞的风险。</li>
</ol>
<h3 id="8java线程状态-runnable-blockedtime_waitingwaiting">8.java线程状态, runnable、blocked，time_waiting，waiting</h3>
<p>1.NEW(创建)<br>
创建态：当一个已经被创建的线程处于未被启动时，即：还没有调用start方法时，就处于这个状态。</p>
<p>2.RUNNABLE(运行时)<br>
运行态：当线程已被占用，在Java虚拟机中正常执行时，就处于此状态。</p>
<p>3.BLOCKED(排队时)<br>
阻塞态：当一个线程试图获取一个对象锁，而该对象锁被其他的线程持有，则该线程进入Blocked状态。当该线程持有锁时，该线程将自动变成RUNNABLE状态。</p>
<p>4.WAITING(休眠)<br>
休眠态：一个线程在等待另一个线程执行一个(唤醒)动作时，该线程进入Waiting状态。进入这个状态后是不能自动唤醒的，必须等待另一个线程调用notify或者notifyAll方法才能够唤醒。</p>
<p>5.TIMED_WAITING (指定休眠时间)<br>
指定时间休眠态：基本同WAITING状态，多了个超时参数，调用对应方法时线程将进入TIMED_WAITING状态，这一状态将一直保持到超时期满或者接收到唤醒通知，带有超时参数的常用方法有Thread.sleep、锁对象.wait() 。</p>
<p>6.TERMINATED (结束)<br>
结束态：从RUNNABLE状态正常退出而死亡，或者因为没有捕获的异常终止了RUNNABLE状态而死亡。</p>
<h3 id="9线程池有哪些核心的参数">9.线程池有哪些核心的参数</h3>
<ol>
<li>
<p>核心线程数：corePoolSize<br>
线程池中活跃的线程数，即使它们是空闲的，除非设置了allowCoreThreadTimeOut为true。allowCoreThreadTimeOut的值是控制核心线程数是否在没有任务时是否停止活跃的线程，当它的值为true时，在线程池没有任务时，所有的工作线程都会停止。</p>
</li>
<li>
<p>最大线程数：maximumPoolSize<br>
线程池所允许存在的最大线程数。</p>
</li>
<li>
<p>多余线程存活时长：keepAliveTime<br>
线程池中除核心线程数之外的线程（多余线程）的最大存活时间，如果在这个时间范围内，多余线程没有任务需要执行，则多余线程就会停止。(注意：多余线程数 = 最大线程数 - 核心线程数)</p>
</li>
<li>
<p>时间单位：unit<br>
多余线程存活时间的单位，可以是分钟、秒、毫秒等。</p>
</li>
<li>
<p>任务队列：workQueue<br>
线程池的任务队列，使用线程池执行任务时，任务会先提交到这个队列中，然后工作线程取出任务进行执行，当这个队列满了，线程池就会执行拒绝策略。</p>
</li>
<li>
<p>线程工厂：threadFactory<br>
创建线程池的工厂，线程池将使用这个工厂来创建线程池，自定义线程工厂需要实现ThreadFactory接口。</p>
</li>
<li>
<p>拒绝执行处理器（也称拒绝策略）：handler<br>
当线程池无空闲线程，并且任务队列已满，此时将线程池将使用这个处理器来处理新提交的任务。</p>
</li>
</ol>
<h3 id="10线程池空闲的线程是如何回收">10.线程池空闲的线程是如何回收？</h3>
<p>超过corePoolSize的空闲线程由线程池回收，线程池Worker启动跑第一个任务之后就一直循环遍历线程池任务队列，超过指定超时时间获取不到任务就remove Worker，最后由垃圾回收器回收。</p>
<blockquote>
<p>Worker是线程池ThreadPoolExecutor的一个内部类，其有一个成员变量thread（线程），所以我们可以把一个Worker假以理解为一个线程。</p>
</blockquote>
<p>ThreadPoolExecutor回收工作线程，一条线程getTask()返回null，就会被回收。<br>
分两种场景。<br>
1、未调用shutdown() ，RUNNING状态下全部任务执行完成的场景<br>
线程数量大于corePoolSize，线程超时阻塞，超时唤醒后CAS减少工作线程数，如果CAS成功，返回null，线程回收。否则进入下一次循环。当工作者线程数量小于等于corePoolSize，就可以一直阻塞了。<br>
2、调用shutdown() ，全部任务执行完成的场景<br>
shutdown() 会向所有线程发出中断信号，这时有两种可能。<br>
2.1）所有线程都在阻塞<br>
中断唤醒，进入循环，都符合第一个if判断条件，都返回null，所有线程回收。<br>
2.2）任务还没有完全执行完<br>
至少会有一条线程被回收。在processWorkerExit(Worker w, boolean completedAbruptly)方法里会调用tryTerminate()，向任意空闲线程发出中断信号。所有被阻塞的线程，最终都会被一个个唤醒，回收。</p>
<h3 id="11java线程状态为-blocked-场景">11.java线程状态为 blocked 场景</h3>
<p>BLOCKED 状态跟 I/O 的阻塞是不同的，它不是一般意义上的阻塞，而是特指被 synchronized 块阻塞，即是跟线程同步有关的一个状态。</p>
<p>一旦一个线程获取锁进入同步块，在其出来之前，如果其它线程想进入，就会因为获取不到锁而阻塞在同步块之外，这时的状态就是 BLOCKED。</p>
<p>简单来说，大致有两种情况可以让线程处于这个状态。</p>
<ol>
<li>线程A想进入某个同步快，但是由于该同步锁被其他线程占用，所以自己只能等待该锁，此时线程A为BLOCKED状态。</li>
<li>线程A已经获取该锁，进入同步块，但调用了wait方法后释放了该锁，然后其他线程内执行了同一把锁对象的notify或者notifyAll后，此时线程A为BLOCKED状态。</li>
</ol>
<h3 id="12sleep-和-wait-区别">12.sleep 和 wait 区别</h3>
<p>sleep() 方法让当前线程停止运行一段时间，到期自动继续执行。<br>
wait() 方法让线程停止运行，在 notify() 或 notifyAll() 后继续执行。</p>
<p><mark>相同</mark></p>
<ol>
<li>sleep() 和 wait() 调用都会暂停当前线程并让出 CPU</li>
</ol>
<p><mark>不同</mark></p>
<ol>
<li>定义位置不同：sleep() 是线程类（Thread）的方法；wait() 是顶级类 Object 的方法；</li>
<li>调用地方不同：sleep 方法可以在任何地方使用；wait 方法则只能在同步方法或同步块中使用；</li>
<li>锁资源释放方式不同：sleep 方法只让出了CPU，没有释放同步资源锁！ wait方法则是指当前线程让自己暂时退让出同步资源锁，以便其他正在等待该资源的线程得到该资源进而运行，只有调用了notify方法，之前调用wait()的线程才会解除wait状态，可以去参与竞争同步资源锁，进而得到执行。</li>
<li>恢复方式不同：sleep调用后停止运行期间仍持有同步锁，所以到时间会继续执行；wait调用会放弃对象锁，进入等待队列，待调用notify()/notifyAll()唤醒指定的线程或者所有线程，才会进入锁池，再次获得对象锁后才会进入运行状态，在没有获取对象锁之前不会继续执行；</li>
<li>异常捕获：sleep需要捕获或者抛出异常，而wait/notify/notifyAll则不需要。</li>
</ol>
<h2 id="锁">锁</h2>
<h3 id="13reentrantlock-和-sychnozied-区别">13.reentrantLock 和 sychnozied 区别</h3>
<p>相似点：<br>
这两种同步方式有很多相似之处，它们都是加锁方式同步，而且都是阻塞式的同步，也就是说当如果一个线程获得了对象锁，进入了同步块，其他访问该同步块的线程都必须阻塞在同步块外面等待。</p>
<p>不同点：</p>
<ol>
<li>Synchronized是java语言的关键字，是原生语法层面的互斥，需要jvm实现。而ReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。</li>
<li>Synchronized等待不可中断，reentrantLock等待可中断。</li>
<li>synchronized的锁是非公平锁，ReentrantLock默认情况下也是非公平锁，但可以通过带布尔值的构造函数要求使用公平锁。</li>
<li>ReentrantLock可以同时绑定多个Condition对象，只需多次调用newCondition方法即可。<br>
synchronized中，锁对象的wait()和notify()或notifyAll()方法可以实现一个隐含的条件。但如果要和多于一个的条件关联的时候，就不得不额外添加一个锁。</li>
</ol>
<h3 id="14sychnozied-锁升级设计思想偏向锁轻量级重量级">14.sychnozied 锁升级设计思想，偏向锁，轻量级，重量级</h3>
<p>每个java对象都有一个对象头，对象头由类型指针和标记字段组成。在64位虚拟机中，未开启压缩指针，标记字段占64位，类型指针占64位，共计16个字节。markword是java对象数据结构中的一部分，markword数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，它的最后2bit是锁状态标志位，用来标记当前对象的状态，对象的所处的状态，决定了markword存储的内容，00表示轻量级锁，01表示无锁或偏向锁，10表示重量级锁。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1675325742033.png" alt="" loading="lazy"></figure>
<ol>
<li>检测Mark Word里面是不是当前线程的ID，如果是则表示当前线程处于偏向锁；</li>
<li>如果不是，则使用CAS将当前线程的ID替换Mard Word，如果成功则表示当前线程获得偏向锁，置偏向标志位1；</li>
<li>如果失败，则说明发生竞争，撤销偏向锁，进而升级为轻量级锁；</li>
<li>当前线程使用CAS将对象头的Mark Word替换为锁记录指针，如果成功，当前线程获得锁；</li>
<li>如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁；</li>
<li>如果自旋成功则依然处于轻量级状态；</li>
<li>如果自旋失败，则升级为重量级锁；</li>
</ol>
<p>偏向锁是在无锁争用的情况下使用的，也就是同步开在当前线程没有执行完之前，没有其它线程会执行该同步块，一旦有了第二个线程的争用，偏向锁就会升级为轻量级锁，如果轻量级锁自旋到达阈值后，没有获取到锁，就会升级为重量级锁。</p>
<h3 id="15sychnozied-偏向锁是怎么撤销的">15.sychnozied 偏向锁是怎么撤销的</h3>
<p>偏向锁的撤销，需要等待全局安全点（safe point，代表了一个状态，在该状态下所有线程都是暂停的，stop-the-world，在这个时间点上没有正在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的会被执行，并升级为轻量级锁，最后唤醒暂停的线程。</p>
<h3 id="16threadlocals-使用场景实现原理">16.threadLocals 使用场景，实现原理</h3>
<p><code>场景一：代替参数的显式传递</code><br>
当我们在写API接口的时候，通常Controller层会接受来自前端的入参，当这个接口功能比较复杂的时候，可能我们调用的Service层内部还调用了 很多其他的很多方法，通常情况下，我们会在每个调用的方法上加上需要传递的参数。<br>
但是如果我们将参数存入ThreadLocal中，那么就不用显式的传递参数了，而是只需要ThreadLocal中获取即可。<br>
<code>场景二：全局存储用户信息</code><br>
我们会选择在拦截器的业务中， 获取到保存的用户信息，然后存入ThreadLocal，那么当前线程在任何地方如果需要拿到用户信息都可以使用ThreadLocal的get()方法 (异步程序中ThreadLocal是不可靠的)<br>
<code>场景三：解决线程安全问题</code><br>
在Spring的Web项目中，我们通常会将业务分为Controller层，Service层，Dao层， 由于Dao层使用单例，那么负责数据库连接的Connection也只有一个， 如果每个请求线程都去连接数据库，那么就会造成线程不安全的问题，当每个请求线程使用Connection的时候， 都会从ThreadLocal获取一次，如果为null，说明没有进行过数据库连接，连接后存入ThreadLocal中，如此一来，每一个请求线程都保存有一份 自己的Connection。</p>
<p>每一个线程都有一个对应的Thread对象，而Thread类有一个成员变量，它是一个Map集合，这个Map集合的key就是ThreadLocal的引用，而value就是当前线程在key所对应的ThreadLocal中存储的值。当某个线程需要获取存储在ThreadLocal变量中的值时，ThreadLocal底层会获取当前线程的Thread对象中的Map集合，然后以ThreadLocal作为key，从Map集合中查找value值。</p>
<p>参考：<a href="https://www.cnblogs.com/tuyang1129/p/12713815.html" title="深入分析ThreadLocal的实现原理">深入分析ThreadLocal的实现原理</a></p>
<h3 id="17synchronize-使用场景实现原理">17.synchronize 使用场景，实现原理</h3>
<p>Synchronized进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>
<blockquote>
<p>monitorenter和monitorexit指令的底层是lock和unlock指令。</p>
</blockquote>
<h3 id="18wait-notify-使用场景实现原理">18.wait、notify 使用场景，实现原理</h3>
<p>Monitor（管程）的结构，其中有一块叫做waitSet的区域，里面存放的是状态为WAITING状态的线程。如下图虚线框中的内容：<br>
<img src="https://q456qq520.github.io/post-images/1675329321527.png" alt="" loading="lazy"></p>
<p>调用wait方法，首先会获取监视器锁，获得成功以后，会让当前线程进入等待状态进入等待队列并且释放锁；然后 当其他线程调用notify或者notifyall以后，会选择从等待队列中唤醒任意一个线程，而执行完notify方法以后，并不会立马唤醒线程，原因是当前的线程仍然持有这把锁，处于等待状态的线程无法获得锁。必须要等到当前的线程执行完按monitorexit指令以后，也就是锁被释放以后，处于等待队列中的线程就可以开始竞争锁了。</p>
<h3 id="19join-使用场景实现原理">19.join 使用场景，实现原理</h3>
<p>join()是Thread类的一个方法，等待该线程终止. 需要明确的是主线程等待子线程(假设有个子线程thread)的终止。即在主线程的代码块中，如果碰到了thread.join()方法，此时主线程需要等子线程thread结束了(Waits for this thread to die.),才能继续执行thread.join()之后的代码块。</p>
<pre><code class="language-java">public final void join() throws InterruptedException {
    join(0);
}

public final synchronized void join(long millis)
throws InterruptedException {
    long base = System.currentTimeMillis();
    long now = 0;

    if (millis &lt; 0) {
        throw new IllegalArgumentException(&quot;timeout value is negative&quot;);
    }

    if (millis == 0) {
        while (isAlive()) {
            wait(0);
        }
    } else {
        while (isAlive()) {
            long delay = millis - now;
            if (delay &lt;= 0) {
                break;
            }
            wait(delay);
            now = System.currentTimeMillis() - base;
        }
    }
}
</code></pre>
<p>当子线程执行结束的时候，jvm会自动唤醒阻塞主线程。</p>
<h3 id="20interrupted-使用场景实现原理">20.interrupted 使用场景，实现原理</h3>
<p>java interrupt中断机制是当主线程向目标线程发起interrupt中断命令后，目标线程的中断标志位被置为true，目标线程通过查询中断标志位自行决定是否停止当前线程的执行。</p>
<pre><code class="language-java">public void interrupt() {
    if (this != Thread.currentThread())
        checkAccess();

    synchronized (blockerLock) {
        Interruptible b = blocker;
        if (b != null) {
            //打断的主要方法，该方法的主要作用是设置一个打断标记
            interrupt0();
            b.interrupt(this);
            return;
        }
    }
    interrupt0();
}
</code></pre>
<p>interrupted()是静态方法而isInterrupted()是实例方法，他们的实现都是调用同一个native方法。主要的区别是他们的形参ClearInterrupted传的不一样。interrupted()在返回中断标志位后会清除标志位，isInterrupted()则不清除中断标志位。</p>
<pre><code class="language-java">public static boolean interrupted() {
    return currentThread().isInterrupted(true);
}

public boolean isInterrupted() {
    return isInterrupted(false);
}

private native boolean isInterrupted(boolean ClearInterrupted);
</code></pre>
<p><code>使用场景</code></p>
<ol>
<li>ThreadPoolExecutor中的 shutdownNow 方法会遍历线程池中的工作线程并调用线程的 interrupt 方法来中断线程。</li>
<li>FutureTask 中的 cancel 方法，如果传入的参数为 true，它将会在正在运行异步任务的线程上调用 interrupt 方法，如果正在执行的异步任务中的代码没有对中断做出响应，那么 cancel 方法中的参数将不会起到什么效果。</li>
</ol>
<h3 id="21volatile-使用场景实现原理">21.volatile 使用场景，实现原理</h3>
<p>volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令，lock前缀指令实际上相当于一个内存屏障（也成内存栅栏) ，内存屏障会提供3个功能：</p>
<ul>
<li>它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成；</li>
<li>它会强制将对缓存的修改操作立即写入主存；（每个线程都有自己的工作内存）</li>
<li>如果是写操作，它会导致其他CPU中对应的缓存行无效。</li>
</ul>
<h3 id="22指令重排序可以解决什么问题">22.指令重排序可以解决什么问题</h3>
<p>为了使处理器内部的运算单元能尽量被充分利用，处理器可能会对输入的代码进行乱序执行优化，处理器会在计算之后将乱序执行的结果重组，并确保这一结果和顺序执行结果是一致的，但是这个过程并不保证各个语句计算的先后顺序和输入代码中的顺序一致。这就是指令重排序。</p>
<ol>
<li>
<p>编译器优化<br>
编译器（包括 JVM、JIT 编译器等）出于优化的目的，例如当前有了数据 a，把对 a 的操作放到一起效率会更高，避免读取 b 后又返回来重新读取 a 的时间开销，此时在编译的过程中会进行一定程度的重排。不过重排序并不意味着可以任意排序，它需要需要保证重排序后，不改变单线程内的语义，否则如果能任意排序的话，程序早就逻辑混乱了。</p>
</li>
<li>
<p>CPU 重排序<br>
CPU 同样会有优化行为，这里的优化和编译器优化类似，都是通过乱序执行的技术来提高整体的执行效率。所以即使之前编译器不发生重排，CPU 也可能进行重排，我们在开发中，一定要考虑到重排序带来的后果。</p>
</li>
<li>
<p>内存的“重排序”<br>
内存系统内不存在真正的重排序，但是内存会带来看上去和重排序一样的效果，所以这里的“重排序”打了双引号。由于内存有缓存的存在，在 JMM 里表现为主存和本地内存，而主存和本地内存的内容可能不一致，所以这也会导致程序表现出乱序的行为。</p>
</li>
</ol>
<p>重排序通过减少执行指令，从而提高整体的运行速度。</p>
<h3 id="23volatile-指令重排序可见性">23.volatile 指令重排序，可见性</h3>
<p>可见性： volatile的功能就是被修饰的变量在被修改后可以立即同步到主内存，被修饰的变量在每次是用之前都从主内存刷新。本质也是通过内存屏障来实现可见性 写内存屏障（Store Memory Barrier）可以促使处理器将当前store buffer（存储缓存）的值写回主存。读内存屏障（Load Memory Barrier）可以促使处理器处理invalidate queue（失效队列）。进而避免由于Store Buffer和Invalidate Queue的非实时性带来的问题。</p>
<p>禁止指令重排序： volatile是通过内存屏障来禁止指令重排序</p>
<pre><code class="language-java">public class Singleton {
	//volatile是防止指令重排
    private static volatile Singleton singleton;
    // 无参构造
    private Singleton() {}
    public static Singleton getInstance() {
        //第一层判断singleton是不是为null
        //如果不为null直接返回，这样就不必加锁了
        if (singleton == null) {
            //现在再加锁
            synchronized (Singleton.class){
                //第二层判断
                //如果A,B两个线程都在synchronized等待
                //A创建完对象之后，B还会再进入，如果不再检查一遍，B又会创建一个对象
                if (singleton == null) {
                    /*volatile主要是防止这里：
                    下面字节码会生成三个操作：
                    一是为Singleton对象在堆中分配空间
                    二是执行Singleton的构造函数
                    三是将新生成的Singleton对象的引用赋给singleton字段
                    而在重排序之后，上面的顺序有可能变成 一、三、二，那么这对象是残缺不全的--半对象
                    于是，在多线程情况下，别的线程可能会访问到一个singleton不为null却没有执行完构造函数的无效引用
                    */
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
}
</code></pre>
<h3 id="24cas-使用场景aba的问题如何解决">24.cas 使用场景，aba的问题如何解决</h3>
<h3 id="25aqs-是什么数据结构volatilecaslocksupportunparkunpark">25.aqs 是什么数据结构，volatile，cas，LockSupport.unpark/unpark</h3>
<h3 id="26伪共享问题是如何发生的">26.伪共享问题是如何发生的</h3>
<p>缓存系统中的缓存是以缓存行（cache line）为单位存储的，Cache Line 是 CPU 和主存之间数据传输的最小单位，缓存行通常是 64 字节。当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，因为都会导致同一个缓存行失效而会无意中影响彼此的性能，这就是伪共享（false sharing）。</p>
<p>为了避免由于 false sharing 导致 Cache Line 从 L1,L2,L3 到主存之间重复载入，我们可以使用数据填充追加字节的方式来避免，即单个数据填充满一个CacheLine，该方法本质上是一种空间换时间的做法。<br>
JDK 8开始,提供了一个sun.misc.Contended 注解，用来解决伪共享问题，加上这个注解的类会自动补齐缓存行。</p>
<h3 id="27disruptor-使用场景数据结构优势">27.Disruptor 使用场景，数据结构，优势</h3>
<h2 id="计算机网络">计算机网络</h2>
<h3 id="1-网络分层思想链路层网络层传输层应用层">1. 网络分层思想，链路层，网络层，传输层，应用层</h3>
<p>1）物理层<br>
该层负责比特流在节点间的传输，即负责物理传输。该层的协议既与链路有关，业余传输介质有关。通俗来讲就是把计算机连接起来的物理手段。<br>
2）数据链路层<br>
该层控制网络层与物理层之间的通信，其主要功能是如何在不可靠的物理线路上进行数据的可靠传递。为了保证传输，从网络层接收到的数据被分割成特定的可被物理层传输的帧。帧是用来移动数据的结构包，它不仅包括原始数据，还包括发送方和接收方的物理地址以及纠错和控制信息。其中的地址确定了帧将发送到何处，而纠错和控制信息则确保帧无差错到达。如果在传送数据时，接收点检测到所传数据中有差错，就要通知发送方重发这一帧。<br>
3）网络层<br>
该层决定如何将数据从发送方路由到接收方。网络层通过综合考虑发送优先权、网络拥塞程度、服务质量以及可选路由的花费来决定从一个网络中的节点 A 到另一个网络中节点 B 的最佳路径。<br>
4）传输层<br>
该层为两台主机上的应用程序提供端到端的通信。相比之下，网络层的功能是建立主机到主机的通<br>
信。传输层有两个传输协议：TCP（传输控制协议）和UDP（用户数据报协议）。其中，TCP是一个可靠的面向连接的协议，UDP是不可靠的或者说无连接的协议。<br>
5）应用层<br>
应用程序收到传输层的数据后，接下来就要进行解读。解读必须事先规定好格式，而应用层就是规定应用程序的数据格式的。它的主要协议有HTTP、FTP、Telnet、SMTP、POP3等。</p>
<h3 id="2-交换机是在链路层工作mac层协议中mtu-最大传输单元">2. 交换机是在链路层工作，MAC层协议中，MTU 最大传输单元</h3>
<h3 id="3-路由器是在网络层工作ip层协议中ttlip分片">3. 路由器是在网络层工作，IP层协议中，TTL，IP分片</h3>
<h3 id="4-udp是传输层协议有哪些使用场景">4. UDP是传输层协议，有哪些使用场景</h3>
<p>UDP是无连接的，不可靠传输，尽最大努力交付数据，协议简单、资源要求少、传输速度快、实时性高的特点，适用于对传输效率要求高，但准确率要求低的应用场景，比如域名转换(DNS)、远程文件服务器(NFS)等。</p>
<h3 id="5-udp需要三次握手吗">5. UDP需要三次握手吗？</h3>
<h3 id="6三次握手原理为什么不是两次挥手为什么是4次">6.三次握手原理，为什么不是两次，挥手为什么是4次</h3>
<p>答：建立连接的过程是利用客户服务器模式，假设主机A为客户端，主机B为服务器端。</p>
<p>（1）TCP的三次握手过程：主机A向B发送连接请求；主机B对收到的主机A的报文段进行确认；主机A再次对主机B的确认进行确认。<br>
（2）采用三次握手是为了防止失效的连接请求报文段突然又传送到主机B，因而产生错误。失效的连接请求报文段是指：主机A发出的连接请求没有收到主机B的确认，于是经过一段时间后，主机A又重新向主机B发送连接请求，且建立成功，顺序完成数据传输。考虑这样一种特殊情况，主机A第一次发送的连接请求并没有丢失，而是因为网络节点导致延迟达到主机B，主机B以为是主机A又发起的新连接，于是主机B同意连接，并向主机A发回确认，但是此时主机A根本不会理会，主机B就一直在等待主机A发送数据，导致主机B的资源浪费。<br>
（3）如果不采用三次握手，我们考虑以下场景：<br>
采用一次握手：<br>
首先A发送一个(SYN)到B，意思是A要和B建立连接进行通信；<br>
如果是只有一次握手的话，这样肯定是不行的，A压根都不知道B是不是收到了这个请求。<br>
采用二次握手：<br>
B收到A要建立连接的请求之后，发送一个确认(SYN+ACK)给A，意思是收到A的消息了，B这里也是通的，表示可以建立连接；<br>
如果只有两次通信的话，这时候B不确定A是否收到了确认消息，有可能这个确认消息由于某些原因丢了。<br>
采用三次握手：<br>
A如果收到了B的确认消息之后，再发出一个确认(ACK)消息，意思是告诉B，这边是通的，然后A和B就可以建立连接相互通信了；<br>
这个时候经过了三次握手，A和B双方确认了两边都是通的，可以相互通信了，已经可以建立一个可靠的连接，并且可以相互发送数据。</p>
<p>因为TCP有个半关闭状态，假设A.B要释放连接，那么A发送一个释放连接报文给B，B收到后发送确认，这个时候A不发数据，但是B如果发数据A还是要接收，这叫半关闭。然后B还要发给A连接释放报文，然后A发确认，所以是4次。</p>
<h3 id="7tcp数据结构关键字段序号滑动窗口">7.TCP数据结构关键字段：序号，滑动窗口</h3>
<p><code>Sequence Number（序列号）</code>：占 4 个字节，范围是[0， 232 - 1]，序号增加到 232 - 1 后，下一个序号就又回到 0。在 TCP 连接中传送的字节流中的每一个字节都要按顺序编号，起始序号在连接建立时就完成设置。因此序列号可以用来解决网络包乱序（reordering）问题。</p>
<p>例如，一个报文段的序号是 301，而携带的数据共有 100 个字节。这就表明：本报文段的数据的第一个字节的序号是 301，最后一个字节的序号是 400。显然下一个报文段的数据序号要从 401 开始。</p>
<p><code>Window（窗口）</code>：占 2 个字节，窗口值是一个 [0， 216 - 1] 之间的整数。窗口指的是发送本报文段的一方的接收窗口（而不是自己的发送窗口）。窗口值用于告诉对方：从本报文段首部中的确认号算起，接受方目前允许对方发送的数据量（以字节为单位）。之所以要有这个限制，是因为接受方的数据空间是有限的。</p>
<p>例如：发送了一个报文段，其确认号是 701，窗口字段值为 1000。这就告诉对方：“从 701 序号开始算起，我（发送此报文段的一方）的接收缓存空间还可以接收 1000 个字节数据，字节序号是 701 - 1700，你在给我发送数据时，必须要考虑到这一点”。窗口字段值明确的指出了现在允许对方发送的数据量，窗口值通常是在不断的动态变化着。</p>
<h3 id="8tcp相对udp有哪些优点顺序发送超时重试流量控制拥塞控制">8.TCP相对UDP有哪些优点：顺序发送，超时重试，流量控制，拥塞控制</h3>
<h3 id="9tcp顺序发送如何解决顺序发送问题-自增序号三次握手确定序号">9.TCP顺序发送：如何解决顺序发送问题? 自增序号，三次握手确定序号</h3>
<h3 id="10tcp超时重试有哪些重试的方法-快速重传接受地图">10.TCP超时重试：有哪些重试的方法? 快速重传，接受地图</h3>
<h3 id="11tcp拥塞控制如何解决拥塞问题">11.TCP拥塞控制：如何解决拥塞问题</h3>
<h3 id="12tcp粘包拆包是如何发生的">12.TCP：粘包/拆包是如何发生的</h3>
<p>粘包：两个包较小，间隔时间短，发生粘包，合并成一个包发送；<br>
拆包：一个包过大，超过缓存区大小，拆分成两个或多个包发送；<br>
拆包和粘包：Packet1过大，进行了拆包处理，而拆出去的一部分又与Packet2进行粘包处理。</p>
<p>对于粘包和拆包问题，常见的解决方案有四种：</p>
<ol>
<li>发送端将每个包都封装成固定的长度，比如100字节大小。如果不足100字节可通过补0或空等进行填充到指定长度；</li>
<li>发送端在每个包的末尾使用固定的分隔符，例如\r\n。如果发生拆包需等待多个包发送过来之后再找到其中的\r\n进行合并；例如，FTP协议；</li>
<li>将消息分为头部和消息体，头部中保存整个消息的长度，只有读取到足够长度的消息之后才算是读到了一个完整的消息；</li>
<li>通过自定义协议进行粘包和拆包的处理。</li>
</ol>
<h3 id="13dns是如何工作的本地电脑发起一个百度的请求">13.DNS是如何工作的，本地电脑发起一个百度的请求</h3>
<h3 id="14linux操作系统的用户态和内核态">14.Linux操作系统的，用户态和内核态</h3>
<h3 id="15零拷贝怎么理解mmap-sendfile-虚拟内存">15.零拷贝怎么理解，mmap、sendfile。虚拟内存</h3>
<h3 id="16dma是什么">16.DMA是什么</h3>
<h3 id="17io多路复用主要解决什么问题">17.IO多路复用主要解决什么问题？</h3>
<p><code>同步阻塞（BIO）：</code><br>
服务端采用单线程，当 accept 一个请求后，在 recv 或 send 调用阻塞时，将无法 accept 其他请求（必须等上一个请求处理 recv 或 send 完 ）（无法处理并发）<br>
服务端采用多线程，当 accept 一个请求后，开启线程进行 recv，可以完成并发处理，但随着请求数增加需要增加系统线程，大量的线程占用很大的内存空间，并且线程切换会带来很大的开销，10000个线程真正发生读写实际的线程数不会超过20%，每次accept都开一个线程也是一种资源浪费。</p>
<p><code>同步非阻塞（NIO）：</code><br>
服务器端当 accept 一个请求后，加入 fds 集合，每次轮询一遍 fds 集合 recv (非阻塞)数据，没有数据则立即返回错误，每次轮询所有 fd （包括没有发生读写实际的 fd）会很浪费 CPU。</p>
<p><code>IO多路复用：</code><br>
服务器端采用单线程通过 select/poll/epoll 等系统调用获取 fd 列表，遍历有事件的 fd 进行 accept/recv/send ，使其能支持更多的并发连接请求。</p>
<h3 id="18io多路复用epoll原理等待队列红黑树就绪队列">18.IO多路复用，EPOLL原理，等待队列，红黑树，就绪队列</h3>
<p>epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是**事件驱动（每个事件关联上fd）**的，此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)）。</p>
<p><code>epoll函数接口</code><br>
当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关。eventpoll结构体如下所示：</p>
<pre><code class="language-c">#include &lt;sys/epoll.h&gt;

// 数据结构
// 每一个epoll对象都有一个独立的eventpoll结构体
// 用于存放通过epoll_ctl方法向epoll对象中添加进来的事件
// epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可
struct eventpoll {
    /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
    struct rb_root  rbr;
    /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
    struct list_head rdlist;
};

// API
int epoll_create(int size); // 内核中间加一个 ep 对象，把所有需要监听的 socket 都放到 ep 对象中
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // epoll_ctl 负责把 socket 增加、删除到内核红黑树
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);// epoll_wait 负责检测可读队列，没有可读 socket 则阻塞进程
</code></pre>
<p>每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgn，其中n为红黑树元素个数)。<br>
而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中。<br>
在epoll中，对于每一个事件，都会建立一个epitem结构体，如下所示：</p>
<pre><code class="language-c">struct epitem{
    struct rb_node  rbn;//红黑树节点
    struct list_head    rdllink;//双向链表节点
    struct epoll_filefd  ffd;  //事件句柄信息
    struct eventpoll *ep;    //指向其所属的eventpoll对象
    struct epoll_event event; //期待发生的事件类型
}
</code></pre>
<p>当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。</p>
<p>从上面的讲解可知：通过红黑树和双链表数据结构，并结合回调机制，造就了epoll的高效。<br>
讲解完了Epoll的机理，我们便能很容易掌握epoll的用法了。一句话描述就是：三步曲。</p>
<p>第一步：epoll_create()系统调用。此调用返回一个句柄，之后所有的使用都依靠这个句柄来标识。<br>
第二步：epoll_ctl()系统调用。通过此调用向epoll对象中添加、删除、修改感兴趣的事件，返回0标识成功，返回-1表示失败。<br>
第三部：epoll_wait()系统调用。通过此调用收集收集在epoll监控中已经发生的事件。</p>
<h3 id="19https是如何工作的">19.HTTPS是如何工作的</h3>
<h3 id="20http20有哪些升级合并请求头多路复用主动推送">20.HTTP2.0有哪些升级？合并请求头，多路复用，主动推送</h3>
<h3 id="21-rpc和http的区别使用用场景">21. rpc和http的区别，使⽤用场景?</h3>
<ol>
<li>区别: 传输协议<br>
RPC，可以基于TCP协议，也可以基于HTTP协议<br>
HTTP，基于HTTP协议</li>
<li>传输效率<br>
RPC，使⽤用⾃自定义的TCP协议，可以让请求报⽂文体积更更⼩小，或者使⽤用HTTP2协议，也可以很好的减少报⽂文的体积，提⾼传输效率<br>
HTTP，如果是基于HTTP1.1的协议，请求中会包含很多⽆无⽤用的内容，如果是基于HTTP2.0，那么简单的封装以下是可以作为⼀个RPC来使⽤用的，这时标准RPC框架更更多的是服务治理理</li>
<li>性能消耗，主要在于序列列化和反序列列化的耗时<br>
RPC，可以基于thrift实现⾼高效的⼆进制传输<br>
HTTP，⼤大部分是通过json来实现的，字节⼤大⼩小和序列列化耗时都⽐比thrift要更消耗性能</li>
<li>负载均衡<br>
RPC，基本都⾃自带了了负载均衡策略略<br>
HTTP，需要配置Nginx，HAProxy来实现</li>
<li>服务治理理(下游服务新增，重启，下线时如何不不影响上游调⽤用者)</li>
<li>RPC，能做到⾃自动通知，不不影响上游</li>
<li>HTTP，需要事先通知，修改Nginx/HAProxy配置</li>
</ol>
<p>总结:RPC主要⽤用于公司内部的服务调⽤用，性能消耗低，传输效率⾼高，服务治理理⽅方便便。HTTP主要⽤用于对外的异构环境，浏览器器接⼝口调⽤用，APP接⼝口调⽤用，第三⽅方接⼝口调⽤用等。</p>
<h2 id="jvm">JVM</h2>
<h3 id="1class二进制文件前面几位魔术是什么">1.class二进制文件，前面几位魔术是什么？</h3>
<p>每个Class文件的头4个字节称为魔数（Magic Number），它的唯一作用是确定这个文件是否为一个能被虚拟机接受的Class文件。之所以使用魔数而不是文件后缀名来进行识别主要是基于安全性的考虑，因为文件后缀名是可以随意更改的（当然魔术也可以改，只不过比起改后缀名来说更复杂）。<br>
Class 文件的魔数值固定为「0xCAFEBABE」。</p>
<h3 id="2多态是如何实现的invokevirtualinvokeinterface">2.多态是如何实现的，invokevirtual，invokeInterface</h3>
<p>C++中只有直接调用、间接调用，而JVM通过不同的invoke指令来实现不同属性的方法调用。<br>
那什么是多态呢，满足下面这几个条件就可以称为多态：<br>
1、继承了某个类、实现了某个接口<br>
2、重写父类的方法、实现接口中的方法<br>
3、父类引用指向子类对象</p>
<p>C++中的间接调用与直接调用，JVM抽象成了4个指令来完成：<br>
1、invokevirtual：invokevirtual指令用于调用声明为类的方法；这个指令用于调用public、protected修饰，且不被static、final修饰的方法。跟多态机制有关。<br>
2、invokeinterface：invokeinterface指令用于调用声明为接口的方法；跟invokevirtual差不多。区别是多态调用时，如果父类引用是对象，就用invokevirtual。如果父类引用是接口，就用这个。<br>
3、invokespecial：只用于调用私有方法，构造方法。跟多态无关<br>
4、invokestatic：调用静态方法；</p>
<p>以 invokevirtual 指令为例，在执行时，大致可以分为以下几步：<br>
1、先从操作栈中找到对象的实际类型 class；<br>
2、找到 class 中与被调用方法签名相同的方法，如果有访问权限就返回这个方法的直接引用，如果没有访问权限就报错 java.lang.IllegalAccessError ；<br>
3、如果第 2 步找不到相符的方法，就去搜索 class 的父类，按照继承关系自下而上依次执行第 2 步的操作；<br>
4、如果第 3 步找不到相符的方法，就报错 java.lang.AbstractMethodError ；<br>
可以看到，如果子类覆盖了父类的方法，则在多态调用中，动态绑定过程会首先确定实际类型是子类，从而先搜索到子类中的方法。这个过程便是方法覆盖的本质。</p>
<h3 id="3类加载器有哪些类型">3.类加载器有哪些类型？</h3>
<p>VM支持两种类型的类加载器 。分别为引导类加载器（Bootstrap ClassLoader）和自定义类加载器（User-Defined ClassLoader）。从概念上来讲，自定义类加载器一般指的是程序中由开发人员自定义的一类类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器</p>
<ol>
<li>启动类加载器（引导类加载器，Bootstrap ClassLoader）</li>
</ol>
<ul>
<li>这个类加载使用C/C++语言实现的，嵌套在JVM内部。</li>
<li>它用来加载Java的核心库（JAVA_HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容），用于提供JVM自身需要的类。</li>
<li>并不继承自java.lang.ClassLoader，没有父加载器。</li>
<li>加载扩展类和应用程序类加载器，并作为他们的父类加载器（当他俩的爹）。</li>
<li>出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类。</li>
</ul>
<ol start="2">
<li>扩展类加载器（Extension ClassLoader）</li>
</ol>
<ul>
<li>java语言编写，由sun.misc.Launcher$ExtClassLoader实现。</li>
<li>派生于ClassLoader类。</li>
<li>父类加载器为启动类加载器。</li>
<li>从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录（扩展目录）下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载。</li>
</ul>
<ol start="3">
<li>应用程序类加载器（系统类加载器，AppClassLoader）</li>
</ol>
<ul>
<li>Java语言编写，由sun.misc.LaunchersAppClassLoader实现。</li>
<li>派生于ClassLoader类。</li>
<li>父类加载器为扩展类加载器。</li>
<li>它负责加载环境变量classpath或系统属性java.class.path指定路径下的类库。</li>
<li>该类加载是程序中默认的类加载器，一般来说，Java应用的类都是由它来完成加载。</li>
<li>通过classLoader.getSystemclassLoader()方法可以获取到该类加载器。</li>
</ul>
<ol start="4">
<li>用户自定义加载器</li>
</ol>
<h3 id="4类的加载过程">4.类的加载过程</h3>
<p>类加载的过程主要分为三个部分：加载、链接、初始化。<br>
而链接又可以细分为三个小部分：验证、准备、解析。<br>
<code>加载</code><br>
简单来说，加载指的是把class字节码文件从各个来源通过类加载器装载入内存中。这里有两个重点：</p>
<ol>
<li>字节码来源。一般的加载来源包括从本地路径下编译生成的.class文件，从jar包中的.class文件，从远程网络，以及动态代理实时编译。</li>
<li>类加载器。一般包括启动类加载器，扩展类加载器，应用类加载器以及用户的自定义类加载器。</li>
</ol>
<p><strong>加载的3个阶段</strong></p>
<ol>
<li>通过类的全限定名获取二进制字节流（将 .class 文件读进内存）；</li>
<li>将字节流的静态存储结构转化为运行时的数据结构；</li>
<li>在内存中生成该类的 Class 对象；HotSpot 虚拟机把这个对象放在方法区，非 Java 堆。</li>
</ol>
<p><code>验证</code><br>
主要是为了保证加载进来的字节流符合虚拟机规范，不会造成安全错误。<br>
包括对于文件格式的验证，比如常量中是否有不被支持的常量？是否符合 Class 文件格式规范，验证文件开头 4 个字节是不是 “魔数” 0xCAFEBABE？文件中是否有不规范的或者附加的其他信息？<br>
对于元数据的验证，比如该类是否继承了被final修饰的类？类中的字段，方法是否与父类冲突？是否出现了不合理的重载？<br>
对于字节码的验证，保证程序语义的合理性，比如要保证类型转换的合理性。<br>
对于符号引用的验证，比如校验符号引用中通过全限定名是否能够找到对应的类？校验符号引用中的访问性（private，public等）是否可被当前类访问？</p>
<p><code>准备</code><br>
主要是为类变量（注意，不是实例变量）分配内存，并且赋予初值。<br>
特别需要注意，初值，不是代码中具体写的初始化的值，而是 Java 虚拟机根据不同变量类型的默认初始值。<br>
比如 8 种基本类型的初值，默认为 0；引用类型的初值则为null；常量的初值即为代码中设置的值，例如final static tmp = 456， 那么该阶段 456 就是tmp的初值。</p>
<p><code>解析</code><br>
将常量池内的符号引用替换为直接引用的过程。两个重点：</p>
<ol>
<li>符号引用。即一个字符串，但是这个字符串给出了一些能够唯一性识别一个方法，一个变量，一个类的相关信息。</li>
<li>直接引用。可以理解为一个内存地址，或者一个偏移量。比如类方法，类变量的直接引用是指向方法区的指针；而实例方法，实例变量的直接引用则是从实例的头指针开始算起到这个实例变量位置的偏移量。<br>
举个例子来说，现在调用方法hello()，这个方法的地址是1234567，那么hello就是符号引用，1234567就是直接引用。在解析阶段，虚拟机会把所有的类名，方法名，字段名这些符号引用替换为具体的内存地址或偏移量，也就是直接引用。</li>
</ol>
<p><code>初始化</code><br>
这个阶段主要是对类变量初始化，是执行类构造器的过程。换句话说，只对static修饰的变量或语句进行初始化。如果初始化一个类的时候，其父类尚未初始化，则优先初始化其父类。如果同时包含多个静态变量和静态代码块，则按照自上而下的顺序依次执行。</p>
<h3 id="5双亲委派机制作用是什么为什么又要打破">5.双亲委派机制，作用是什么，为什么又要打破</h3>
<ol>
<li>如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行。</li>
<li>如果父类的加载器还存在其父类加载器，则进一步向上委托，依次递归请求最终达到顶层的启动类加载器。</li>
<li>如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派机制。</li>
</ol>
<p><mark>优点</mark></p>
<ul>
<li>避免类的重复加载</li>
<li>保护程序安全，防止核心API被随意篡改</li>
</ul>
<p>为什么要打破呢？因为类加载器受到加载范围的限制，在某些情况下父类加载器无法加载到需要的文件，这时候就需要委托子类加载器去加载class文件。自定义类加载器加载一个类需要：继承ClassLoader，重写findClass，如果不想打破双亲委派模型，那么只需要重写findClass；如果想打破双亲委派模型，那么就重写整个loadClass方法，设定自己的类加载逻辑。</p>
<h3 id="6jvm内存是如何划分的堆栈方法区直接内存">6.JVM内存是如何划分的，堆，栈，方法区，直接内存</h3>
<ol>
<li><code>程序计数器</code><br>
程序计数器（Program Counter Register），也有称作为PC寄存器。虽然JVM中的程序计数器并不像汇编语言中的程序计数器一样是物理概念上的CPU寄存器，但是JVM中的程序计数器的功能跟汇编语言中的程序计数器的功能在逻辑上是等同的，也就是说是用来指示执行哪条指令的。</li>
</ol>
<p>由于在JVM中，多线程是通过线程轮流切换来获得CPU执行时间的，因此，在任一具体时刻，一个CPU的内核只会执行一条线程中的指令，因此，为了能够使得每个线程都在线程切换后能够恢复在切换之前的程序执行位置，每个线程都需要有自己独立的程序计数器，并且不能互相**扰，否则就会影响到程序的正常执行次序。因此，可以这么说，程序计数器是每个线程所私有的。</p>
<p>在JVM规范中规定，如果线程执行的是非native方法，则程序计数器中保存的是当前需要执行的指令的地址；如果线程执行的是native方法，则程序计数器中的值是undefined。</p>
<p>由于程序计数器中存储的数据所占空间的大小不会随程序的执行而发生改变，因此，对于程序计数器是不会发生内存溢出现象(OutOfMemory)的。</p>
<ol start="2">
<li><code>Java栈</code><br>
Java栈也称作虚拟机栈（Java Vitual Machine Stack），也就是我们常常所说的栈，跟C语言的数据段中的栈类似。事实上，Java栈是Java方法执行的内存模型。为什么这么说呢？下面就来解释一下其中的原因。</li>
</ol>
<p>Java栈中存放的是一个个的栈帧，每个栈帧对应一个被调用的方法，在栈帧中包括局部变量表(Local Variables)、操作数栈(Operand Stack)、指向当前方法所属的类的运行时常量池（的引用(Reference to runtime constant pool)、方法返回地址(Return Address)和一些额外的附加信息。当线程执行一个方法时，就会随之创建一个对应的栈帧，并将建立的栈帧压栈。当方法执行完毕之后，便会将栈帧出栈。因此可知，线程当前执行的方法所对应的栈帧必定位于Java栈的顶部。讲到这里，大家就应该会明白为什么 在 使用 递归方法的时候容易导致栈内存溢出的现象了，这部分空间的分配和释放都是由系统自动实施的。</p>
<p><mark>局部变量表</mark>就是用来存储方法中的局部变量（包括在方法中声明的非静态变量以及函数形参）。对于基本数据类型的变量，则直接存储它的值，对于引用类型的变量，则存的是指向对象的引用。局部变量表的大小在编译器就可以确定其大小了，因此在程序执行期间局部变量表的大小是不会改变的。<br>
<mark>操作数栈</mark>，一个线程执行方法的过程中，实际上就是不断执行语句的过程，而归根到底就是进行计算的过程。因此可以这么说，程序中的所有计算过程都是在借助于操作数栈来完成的。主要用于保存计算过程的中间结果，同时作为计算过程中变量的临时存储空间。<br>
<mark>动态链接</mark>，指向运行时常量池的引用，因为在方法执行的过程中有可能需要用到类中的常量，所以必须要有一个引用指向运行时常量。<br>
<mark>方法返回地址</mark>，当一个方法执行完毕之后，要返回之前调用它的地方，因此在栈帧中必须保存一个方法返回地址。</p>
<p>由于每个线程正在执行的方法可能不同，因此每个线程都会有一个自己的Java栈，互不干扰。</p>
<ol start="3">
<li>
<p><code>本地方法栈</code><br>
本地方法栈与Java栈的作用和原理非常相似。区别只不过是Java栈是为执行Java方法服务的，而本地方法栈则是为执行本地方法（Native Method）服务的。在JVM规范中，并没有对本地方发展的具体实现方法以及数据结构作强制规定，虚拟机可以自由实现它。在HotSopt虚拟机中直接就把本地方法栈和Java栈合二为一。</p>
</li>
<li>
<p><code>堆</code><br>
Java中的堆是用来存储对象本身的以及数组（当然，数组引用是存放在Java栈中的）。这部分空间也是Java垃圾收集器管理的主要区域。另外，堆是被所有线程共享的，在JVM中只有一个堆。</p>
</li>
<li>
<p><code>方法区</code><br>
方法区在JVM中也是一个非常重要的区域，它与堆一样，是被线程共享的区域。在方法区中，存储了每个类的信息（包括类的名称、方法信息、字段信息）、静态变量、常量以及编译器编译后的代码等。</p>
</li>
</ol>
<p>在Class文件中除了类的字段、方法、接口等描述信息外，还有一项信息是常量池，用来存储编译期间生成的字面量和符号引用。</p>
<p>在方法区中有一个非常重要的部分就是运行时常量池，它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载到JVM后，对应的运行时常量池就被创建出来。当然并非Class文件常量池中的内容才能进入运行时常量池，在运行期间也可将新的常量放入运行时常量池中，比如String的intern方法。</p>
<h3 id="7栈数据结构栈帧结构局部变量表操作数栈动态链接方法返回地址">7.栈数据结构：栈帧结构，局部变量表，操作数栈，动态链接，方法返回地址</h3>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1675822649183.png" alt="" loading="lazy"></figure>
<h3 id="8分代设计思想堆是如何划分">8.分代设计思想，堆是如何划分</h3>
<p>一般商业的虚拟机，大多数都遵循了分代收集的设计思想，分代收集理论主要有两条假说。<br>
第一个是强分代假说，强分代假说指的是 JVM 认为绝大多数对象的生存周期都是朝生夕灭的；<br>
第二个是弱分代假说，弱分代假说指的是只要熬过越多次垃圾收集过程的对象就越难以回收（看来对象也会长心眼）。<br>
就是基于这两个假说理论，JVM 将堆区划分为不同的区域，再将需要回收的对象根据其熬过垃圾回收的次数分配到不同的区域中存储。</p>
<p>JVM 根据这两条分代收集理论，把堆区划分为新生代(Young Generation)和老年代(Old Generation)这两个区域。其中，新生代又被划分为 Eden 区，以及两个大小相同的 Survivor 区（From Survivor 和 To Survivor）。在新生代中，每次垃圾收集时都发现有大批对象死去，剩下没有死去的对象会直接晋升到老年代中。<br>
上面这两个假说没有考虑对象的引用关系，而事实情况是，对象之间会存在引用关系，基于此又诞生了第三个假说，即跨代引用假说(Intergeneration Reference Hypothesis)，跨代引用相比较同代引用来说仅占少数。正常来说存在相互引用的两个对象应该是同生共死的，不过也会存在特例，如果一个新生代对象跨代引用了一个老年代的对象，那么垃圾回收的时候就不会回收这个新生代对象，更不会回收老年代对象，然后这个新生代对象熬过一次垃圾回收进入到老年代中，这时候跨代引用才会消除。</p>
<p>依据这条假说，我们就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用，只需在新生代上建立一个全局的数据结构（该结构被称为“记忆集”，Remembered Set），这个结构把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。此后当发生Minor GC时（指新生代的垃圾收集），只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描。虽然这种方法需要在对象改变引用关系（如将自己或者某个属性赋值）时维护记录数据的正确性，会增加一些运行时的开销，但比起收集时扫描整个老年代来说仍然是划算的。</p>
<p>JVM中大家是否还记得对象在Suvivor中每熬过一次MinorGC，年龄就增加1，当它的年龄增加到一定程度后就会被晋升到老年代中，这个次数默认是15岁，有想过为什么是15吗？在Mark Word中可以发现标记对象分代年龄的分配的空间是4bit，而4bit能表示的最大数就是2^4-1 = 15。</p>
<h3 id="9分类强-软-弱-虚引用">9.分类，强、软、弱、虚引用</h3>
<ol>
<li>强引用(StrongReference)<br>
强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。如下：</li>
</ol>
<pre><code class="language-java">Object strongReference = new Object();
</code></pre>
<p>如果强引用对象不使用时，需要弱化从而使GC能够回收，如显式地设置strongReference对象为null，或让其超出对象的生命周期范围，则gc认为该对象不存在引用，这时就可以回收这个对象。<br>
在一个方法的内部有一个强引用，这个引用保存在Java栈中，而真正的引用内容(Object)保存在Java堆中。 当这个方法运行完成后，就会退出方法栈，则引用对象的引用数为0，这个对象会被回收。</p>
<ol start="2">
<li>软引用(SoftReference)<br>
如果一个对象只具有软引用，则内存空间充足时，垃圾回收器就不会回收它；如果内存空间不足了，JVM首先将软引用中的对象引用置为null，然后等待垃圾回收器回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。</li>
</ol>
<pre><code class="language-java">// 软引用
String str = new String(&quot;abc&quot;);
SoftReference&lt;String&gt; softReference = new SoftReference&lt;String&gt;(str);
</code></pre>
<p>也就是说，垃圾收集线程会在虚拟机抛出OutOfMemoryError之前回收软引用对象，而且虚拟机会尽可能优先回收长时间闲置不用的软引用对象。对那些刚构建的或刚使用过的<strong>较新的软对象会被虚拟机尽可能保留</strong>，这就是引入引用队列ReferenceQueue的原因。</p>
<p>软引用可以和一个引用队列(ReferenceQueue)联合使用。如果软引用所引用对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。</p>
<ol start="3">
<li>弱引用(WeakReference)<br>
弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。</li>
</ol>
<pre><code class="language-java">    String str = new String(&quot;abc&quot;);
    WeakReference&lt;String&gt; weakReference = new WeakReference&lt;&gt;(str);
    str = null;
</code></pre>
<p>JVM首先将软引用中的对象引用置为null，然后通知垃圾回收器进行回收。<br>
同样，弱引用可以和一个引用队列(ReferenceQueue)联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。</p>
<ol start="4">
<li>虚引用(PhantomReference)<br>
虚引用顾名思义，就是形同虚设。与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。</li>
</ol>
<pre><code class="language-java">String str = new String(&quot;abc&quot;);
ReferenceQueue queue = new ReferenceQueue();
// 创建虚引用，要求必须与一个引用队列关联
PhantomReference pr = new PhantomReference(str, queue);
</code></pre>
<p>程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要进行垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。</p>
<blockquote>
<p>Java中4种引用的级别和强度由高到低依次为：强引用 -&gt; 软引用 -&gt; 弱引用 -&gt; 虚引用</p>
</blockquote>
<table>
<thead>
<tr>
<th>引用类型</th>
<th>被垃圾回收时间</th>
<th>用途</th>
<th>生存时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>强引用</td>
<td>从来不会</td>
<td>对象的一般状态</td>
<td>JVM停止运行时终止</td>
</tr>
<tr>
<td>软引用</td>
<td>当内存不足时</td>
<td>对象缓存</td>
<td>内存不足时终止</td>
</tr>
<tr>
<td>弱引用</td>
<td>正常垃圾回收时</td>
<td>对象缓存</td>
<td>垃圾回收后终止</td>
</tr>
<tr>
<td>虚引用</td>
<td>正常垃圾回收时</td>
<td>跟踪对象的垃圾回收</td>
<td>垃圾回收后终止</td>
</tr>
</tbody>
</table>
<h3 id="10堆外内存如何回收">10.堆外内存如何回收？</h3>
<p><code>堆外内存的申请和释放</code><br>
JDK的ByteBuffer类提供了一个接口allocateDirect(int capacity)进行堆外内存的申请，底层通过unsafe.allocateMemory(size)实现。<br>
最底层是通过malloc方法申请的，但是这块内存需要进行手动释放，JVM并不会进行回收，幸好Unsafe提供了另一个接口freeMemory可以对申请的堆外内存进行释放。</p>
<p><code>堆外内存的回收机制</code><br>
JDK中使用DirectByteBuffer对象来表示堆外内存，每个DirectByteBuffer对象在初始化时，都会创建一个对用的Cleaner对象，这个Cleaner对象会在合适的时候执行unsafe.freeMemory(address)，从而回收这块堆外内存。</p>
<h3 id="11对象内存中数据接口对象头hash锁信息生代年龄kclass-实例数据-对齐填充">11.对象内存中数据接口：对象头（hash，锁信息，生代年龄，kclass）、实例数据、对齐填充</h3>
<p>在 JVM 中，Java对象保存在堆中时，由以下三部分组成：</p>
<p><code>对象头（object header）</code>：包括了关于堆对象的布局、类型、GC状态、同步状态和标识哈希码的基本信息。Java对象和vm内部对象都有一个共同的对象头格式。<br>
Mark Word：用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等等。Mark Word在32位JVM中的长度是32bit，在64位JVM中长度是64bit。虽然它们在不同位数的JVM中长度不一样，但是基本组成内容是一致的。</p>
<ol>
<li>锁标志位（lock）：区分锁状态，11时表示对象待GC回收状态, 只有最后2位锁标识(11)有效。</li>
<li>biased_lock：是否偏向锁，由于无锁和偏向锁的锁标识都是 01，没办法区分，这里引入一位的偏向锁标识位。</li>
<li>分代年龄（age）：表示对象被GC的次数，当该次数到达阈值的时候，对象就会转移到老年代。</li>
<li>对象的hashcode（hash）：运行期间调用System.identityHashCode()来计算，延迟计算，并把结果赋值到这里。当对象加锁后，计算的结果31位不够表示，在偏向锁，轻量锁，重量锁，hashcode会被转移到Monitor中。</li>
<li>偏向锁的线程ID（JavaThread）：偏向模式的时候，当某个线程持有对象的时候，对象这里就会被置为该线程的ID。 在后面的操作中，就无需再进行尝试获取锁的动作。</li>
<li>epoch：偏向锁在CAS锁操作过程中，偏向性标识，表示对象更偏向哪个锁。</li>
<li>ptr_to_lock_record：轻量级锁状态下，指向栈中锁记录的指针。当锁获取是无竞争的时，JVM使用原子操作而不是OS互斥。这种技术称为轻量级锁定。在轻量级锁定的情况下，JVM通过CAS操作在对象的标题字中设置指向锁记录的指针。</li>
<li>ptr_to_heavyweight_monitor：重量级锁状态下，指向对象监视器Monitor的指针。如果两个不同的线程同时在同一个对象上竞争，则必须将轻量级锁定升级到Monitor以管理等待的线程。在重量级锁定的情况下，JVM在对象的ptr_to_heavyweight_monitor设置指向Monitor的指针</li>
</ol>
<blockquote>
<p>32位JVM中Mark Word存储格式<br>
<img src="https://q456qq520.github.io/post-images/1675825680792.png" alt="" loading="lazy"><br>
64位JVM中Mark Word存储格式<br>
<img src="https://q456qq520.github.io/post-images/1675825727788.png" alt="" loading="lazy"></p>
</blockquote>
<p>Klass Pointer：即类型指针，是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。</p>
<p><code>实例数据（Instance Data）</code>：主要是存放类的数据信息，父类的信息，对象字段属性信息。<br>
<code>对齐填充（Padding）</code>：为了字节对齐，填充的数据，不是必须的。默认情况下，Java虚拟机堆中对象的起始地址需要对齐至8的倍数。</p>
<p>为什么要对齐数据？字段内存对齐的其中一个原因，是让字段只出现在同一CPU的缓存行中。如果字段不是对齐的，那么就有可能出现跨缓存行的字段。也就是说，该字段的读取可能需要替换两个缓存行，而该字段的存储也会同时污染两个缓存行。这两种情况对程序的执行效率而言都是不利的。其实对其填充的最终目的是为了计算机高效寻址。</p>
<h3 id="12对象在内存中生命周期从new到灭亡">12.对象在内存中生命周期，从new到灭亡</h3>
<p>Java对象在JVM中的运行周期大致上分为七个阶段，创建阶段（Creation）、应用阶段（Using）、不可视阶段（Invisible）、不可到达阶段（Unreachable）、可收集阶段（Collected）、终结阶段（Finalized）与释放阶段（Free）</p>
<h3 id="13逃逸分析是什么栈上分配锁消除标量替换">13.逃逸分析是什么？栈上分配，锁消除，标量替换</h3>
<p><code>逃逸分析</code><br>
逃逸分析是一种确定指针动态范围的静态分析，它可以分析在程序的哪些地方可以访问到指针。逃逸分析不是直接优化代码的手段，而是为其它优化手段提供依据的分析技术。逃逸分析的基本行为就是分析对象的动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其它方法中，称为方法逃逸。甚至还有可能被外部线程访问到，例如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸。简单的说，逃逸分析指的是分析变量能不能逃出它的作用域。<br>
如果能证明一个对象不会逃逸到方法或者线程之外，也就是别的方法和线程无法通过任何途径访问到这个方法，则可能为这个变量进行一些高效优化。<br>
 <br>
逃逸分析可以细分为四种场景：<br>
第一：全局变量赋值逃逸。<br>
第二：方法返回至逃逸。<br>
第三：实例引用逃逸。<br>
第四：线程逃逸。当赋值给类变量或者赋值给其他线程里面可以访问的实例变量就会发生线程逃逸。</p>
<pre><code class="language-java">public class SomeClass {
    public void printClassName(EscapeDemo1 escapeDemo1){
        System.out.println(escapeDemo1.getClass().getName());
    }
}

public static  SomeClass someClass;
//全局变量赋值逃逸
public void globalVariablePointerEscape(){
    someClass=new SomeClass();
}

//方法返回值逃逸
public void someMethod(){
    SomeClass someClass=methodPointerEscape();
}
public SomeClass methodPointerEscape(){
    return new SomeClass();
}

    //方法返回值逃逸
public void someMethod(){
    SomeClass someClass=methodPointerEscape();
}
public SomeClass methodPointerEscape(){
    return new SomeClass();
}
</code></pre>
<p><code>标量替换</code><br>
所谓的标量指的是不能进一步分解的量。像 Java 的基础数据类型（int、long等数值类型以及 reference 类型等）以及对象的地址引用都是标量，因为它们是没有办法继续分解的。与标量对应的是聚合量，聚合量指的是可以进一步分解的量，比如字符串就是一个聚合量，因为字符串是用字节数组实现的，可以分解。又比如我们自己定义的变量也都是聚合量。<br>
那么什么是标量替换呢？根据程序访问的情况，将其使用到的成员变量恢复原始类型来访问就叫做标量替换。如果逃逸分析证明一个对象不会被外部访问，并且这个对象可以被拆散的话，那程序真正执行的时候将可能不创建这个对象，而改为直接创建它的若干个被这个方法使用到的成员变量来代替。将对象拆分后，除了可以让对象的成员变量在栈上（栈上存储的数据，很大机会会被虚拟机分配至物理机器的高速寄存器中存储）分配和读写之外，还可以为后续进一步的优化手段创建条件。<br>
那么标量替换有什么好处呢？就是可以大大减少堆内存的占用。因为一旦不需要创建对象了，那么就不再需要分配堆内存了。</p>
<p><code>栈上分配</code><br>
Java 虚拟机中，绝大多数对象都是存放在堆里面的，Java 堆中的对象对于各个线程都是共享和可见的，只要持有这个对象的引用，就可以访问堆中存储的对象数据。虚拟机的垃圾收集系统可以回收掉堆中不再使用的对象，但回收动作无论是筛选可回收对象，还是回收和整理内存都需要耗费时间。<br>
 <br>
但是，有一种特殊情况，那就是如果经过逃逸分析后发现，一个对象并没有逃逸出方法的话，那么就可能被优化成栈上分配,这样就无需在堆上分配内存，也无须进行垃圾回收了。</p>
<p>那么什么是栈上分配呢？指的是如果通过逃逸分析确认对象不会被外部访问到的话。那么就直接在栈上分配对象，那么在栈上分配对象的话，这个对象占用的空间就会在栈帧出站的时候被销毁了，所以通过栈上分配可以降低垃圾回收的压力。</p>
<p><code>同步消除</code><br>
如果逃逸分析能确定一个变量不会逃逸出线程，无法被其它线程访问，那这个变量的读写就不会有多线程竞争的问题，因而变量的同步措施也就可以消除了。</p>
<h3 id="14对象存活算法-引用计数器与可达性分析">14.对象存活算法。引用计数器与可达性分析</h3>
<p><code>引用计数算法</code><br>
引用计数器就是: 给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加1；当引用失效时，计数器值就减一;任何时刻计数器为 0 的对象就是不可能再被使用的，可以此时进行回收。<br>
但是引用计数法有一个很大的缺陷，就是它很难解决对象之间相互循环引用的问题。</p>
<p><code>可达性分析算法</code><br>
可达性分析算法的基本思路就是通过一系列名为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。<br>
这个算法的基本思想是通过一系列称为“GC Roots”的对象作为起始点，从这些节点向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链（即GC Roots到对象不可达）时，则证明此对象是不可用的。</p>
<p>在Java语言中,可作为GCRoots对象包含为以下几种:</p>
<ol>
<li>虚拟机栈(栈帧中的本地变量表)中引用的对象。(可以理解为:引用栈帧中的本地变量表的所有对象)</li>
<li>方法区中静态属性引用的对象(可以理解为:引用方法区该静态属性的所有对象)</li>
<li>方法区中常量引用的对象(可以理解为:引用方法区中常量的所有对象)</li>
<li>本地方法栈中(Native方法)引用的对象(可以理解为:引用Native方法的所有对象)</li>
</ol>
<p>(1)首先第一种是虚拟机栈中的引用的对象，我们在程序中正常创建一个对象，对象会在堆上开辟一块空间，同时会将这块空间的地址作为引用保存到虚拟机栈中，如果对象生命周期结束了，那么引用就会从虚拟机栈中出栈，因此如果在虚拟机栈中有引用，就说明这个对象还是有用的，这种情况是最常见的。<br>
(2)第二种是我们在类中定义了全局的静态的对象，也就是使用了static关键字，由于虚拟机栈是线程私有的，所以这种对象的引用会保存在共有的方法区中，显然将方法区中的静态引用作为GC Roots是必须的。<br>
(3)第三种便是常量引用，就是使用了static final关键字，由于这种引用初始化之后不会修改，所以方法区常量池里的引用的对象也应该作为GC Roots。<br>
(4)最后一种是在使用JNI技术时，有时候单纯的Java代码并不能满足我们的需求，我们可能需要在Java中调用C或C++的代码，因此会使用native方法，JVM内存中专门有一块本地方法栈，用来保存这些对象的引用，所以本地方法栈中引用的对象也会被作为GC Roots。</p>
<h3 id="15为什么需要stw引用关系不发生变化gc-中断-取消偏向锁">15.为什么需要stw？引用关系不发生变化，GC、中断、取消偏向锁</h3>
<p>在发生GC时会停下所有的用户线程，从而导致Java程序出现全局停顿的无响应情况，而这种情况则被称为STW（Stop The World）世界暂停。在发生STW之后，所有的Java代码会停止运行，不过native代码是可以继续执行的，但也不能和JVM交互。一般发生STW都是由于GC引起的，但在某几种少数情况下，也会导致STW出现，如线程Dump、死锁检查、堆日志Dump等</p>
<p>GC发生时为什么都必须要STW呢？</p>
<p>一个是尽量为了避免浮动垃圾产生，就是刚刚标记完成一块区域中的对象，但转眼用户线程又在该区域中产生了新的“垃圾”。<br>
第二个则是为了确保一致性，分析工作必须在一个能确保一致性的快照中进行，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果的准确性无法得到保证。</p>
<p>JVM在发生GC时，主要作用的区域有三个：新生代、年老代以及元数据空间，当然，程序运行期间，绝对多数GC都是在回收新生代。一般而言，GC可以分为四种类型，如下：</p>
<p>①新生代收集：只针对新生代的GC，当Eden区满了时触发，Survivor满了并不会触发。<br>
②年老代收集：针对年老代空间的GC，不过目前只有CMS存在单独回收年老代的行为。<br>
③混合收集：指收集范围覆盖整个新生代空间及部分年老代空间的GC，目前只有G1存在该行为。<br>
④全面收集：覆盖新生代、年老代以及元数据空间的GC，会对于所有可发生GC的内存进行收集。</p>
<p><code>偏向锁的撤销</code><br>
偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态；如果线程仍然活着，拥有偏向锁的会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。</p>
<h3 id="16安全点是什么安全区域是什么-引用关系不发生变化">16.安全点是什么？安全区域是什么。引用关系不发生变化</h3>
<p>当GC发生时，必然会出现程序停顿，也就是需要停止所有用户线程。但问题在于：用户线程停止的时机必须合理，不然在恢复线程后，有可能会导致最终的执行结果出现不一致，因此用户线程必然需要在一个安全的位置暂停。<br>
而在JVM中，存在两个概念：<code>安全点</code>和<code>安全区域</code>，当用户线程执行到安全点或安全区域的代码处，此时发生停止是安全的，后续再次唤醒线程工作时，执行结果也不会因为线程暂停而受到任何影响。</p>
<p><code>安全点(SafePoint)</code><br>
无论是在GC中还是并发编程中，都会经常出现安全点这个概念，因为当我们需要阻塞停止一条线程时，都需要在安全点停止，简单说安全点就是指当线程运行到这类位置时，堆对象状态是确定一致的，线程停止后，JVM可以安全地进行操作，如GC、偏向锁撒销等。</p>
<p>而JVM中对于安全点的定义主要有如下几种：<br>
①循环结束的末尾段<br>
②方法调用之后<br>
③抛出异常的位置<br>
④方法返回之前</p>
<p>当JVM需要发生GC、偏向锁撤销等操作时，如何才能让所有线程到达安全点阻塞或停止？<br>
①主动式中断(JVM采用的方式)：不中断线程，而是设置一个标志，而后让每条线程执行时主动轮询这个标志，当一个线程到达安全点后，发现中断标志为true时就自己中断挂起。<br>
②抢断式中断：先中断所有线程，如果发现线程未执行到安全点则恢复线程让其运行到安全点位置。</p>
<p><code>安全区域(SafeRegion)</code><br>
当Java程序需要停下所有用户线程时，某些线程可能处于中断或者休眠状态，从而无法响应JVM的中断请求走到安全点位置挂起了，所以出现了安全区域的概念。</p>
<p>安全区域是指一条线程执行到一段代码时，该区域的代码不会改变堆中对象的引用。在这区域内JVM可以安全地进行操作。当线程进入到该区域时需要先标识自己进入了，这样GC线程则不会管这些已标识的线程，当线程要离开这个区域时需要先判断可达性分析是否完成，如果完成了则往下执行，如果没有则需要原地等待到GC线程发出安全离开信息为止。</p>
<h3 id="17gc回收算法复制标记清除标记整理">17.GC回收算法：复制，标记清除，标记整理</h3>
<p><code>标记-清除算法</code><br>
标记清除算法是现代GC算法的基础，标-清算法会将回收工作分为标记和清除两个阶段。在标记阶段会根据可达性分析算法，通过根节点标记堆中所有的可达对象，而这些对象则被称为堆中存活对象，反之，未被标记的则为垃圾对象。然后在清除阶段，会对于所有未标记的对象进行清除。</p>
<p>初始GC标志位都为0，也就是未标记状态，假设此时系统堆内存出现不足，那么最终会触发GC机制。GC开始时，在标记阶段首先会停下整个程序，然后GC线程开始遍历所有GC Roots节点，根据可达性分析算法找出所有的存活对象并标记为1，标记阶段完成后，会找出并标记所有存活对象，接下来就会执行清除阶段，清楚所有未被标记的对象,在清除操作完成之后，会将前面存活对象的GC标志位复位，也就是会将标记从1为还原成未标记的0。</p>
<blockquote>
<p>GC标记到底在哪儿？在对象头中存在一个markword字段，而GC标志位就存在其内部。同时，清除阶段并不是简单的置空内存，而是把需要清除的对象地址保存在空闲的地址列表里，下次有新对象需要加载时，判断垃圾的位置空间是否够，如果够就存放。</p>
</blockquote>
<p>标记-清除算法是最初的GC算法，因为在标记阶段需要停下所有用户线程，也就是发生STW，而标记的时候又需要遍历整个堆空间中的所有GcRoots，所以耗时比较长，对于客户端而言，可能会导致GC发生时，造成很长一段时间内无响应。同时，因为堆空间中的垃圾对象是会分散在内存的各个角落，所以一次GC之后，会造成大量的内存碎片，也就是通过标-清算法清理出来的内存是不连续的，为了解决这个问题，JVM就不得不再额外维持一个内存的空闲列表，这又是一种开销。而且在分配数组对象或大对象时，连续的内存空间资源又会变得很匮乏。</p>
<p><code>复制算法</code><br>
复制算法会将JVM中原有的堆内存分为两块，在同一时刻只会使用一块内存用于对象分配。在发生GC时，首先会将使用的那块内存区域中的存活对象复制到未使用的这块内存中。等复制完成之后，对当前使用的这块内存进行全面清除回收，清除完成之后，交换两块内存之间的角色，最后GC结束。</p>
<p>复制算法带来的好处是显而易见的，因为每次GC都是直接对半边区域进行回收，所以回收之后不需要考虑内存碎片的复杂情况，在内存分配时直接可以使用简单高效的 指针碰撞 方式分配对象。</p>
<p>但这种算法最大的问题在于对内存的浪费，因为在实际内存分配时只会使用一块内存，所以在实际分配时，内存直接缩水一半，这是比较头疼的事情。同时，存活的对象在GC发生时，还需要复制到另一块内存区域，因此对象移动的开销也需要考虑在内，所以想要使用这种算法，最起码对象的存活率要非常低才行。</p>
<blockquote>
<p>一般都采用复制算法来收集新生代空间，因为新生代中95%左右的对象都是朝生夕死的。在HotSpot中，新生代会被划分为Eden<em>1、Survivor</em>2三个区域，但比例并非1:1，因为经过一次GC后能够依旧活着的对象是十不存一的，所以需要转移的对象并不多，所以在HotSpotVM中，三个区域的比例默认为8:1:1。当每次新生代发生GC时，就将Eden区和一块Survivor区的存活对象移动到另外一块Survivor区中，最后对Eden区和原本那块Survivor区进行全面回收。所以也就是说，HotSpot中新生代的内存最多浪费10%，最大容量为80%+10%=90%。</p>
</blockquote>
<p>但凡事没有绝对，因为在运行时，谁也不能保证每次存活的对象总量都小于新生代空间的10%，所以有时候可能会出现：另外一块Survivor区10%的空间放不下新生代的存活对象这种情况，所以此时就需要<mark>空间分配担保机制</mark>介入了。</p>
<blockquote>
<p>空间分配担保机制机制是指：当Survivor空间不够用时，需要依赖于年老代进行分配担保，对Survivor空间空间中的存活对象进行动态晋升判定，把一些符合条件的对象提前转入到年老代空间中存储，以确保新生代能够空出足够的空间，确保新生代GC的正常工作。</p>
</blockquote>
<p><code>标记-整理算法</code><br>
标记-整理算法也被称为标记-压缩算法，标-整算法适用于存活率较高的场景，它是建立在标-清算法的基础上做了优化。标-整算法也会分为两个阶段，分别为标记阶段、整理阶段：</p>
<p>①标记阶段：和标-清算法一样。在标记阶段时也会基于GcRoots节点遍历整个内存中的所有对象，然后对所有存活对象做一次标记。<br>
②整理阶段：在整理阶段该算法并不会和标-清算法一样简单的清理内存，而是会将所有存活对象移动（压缩）到内存的一端，然后对于存活对象边界之外的内存进行统一回收。</p>
<blockquote>
<p>经过标-整算法之后的堆空间会变成整齐的内存，因为被标记为存活的对象都会被压缩到内存的一端。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，也就是保留一根指针指向已用内存和空闲内存的分割点，也就是可以直接采用指针碰撞的方式进行内存分配，这比维护一个空闲列表显然少了许多开销。</p>
</blockquote>
<p>标-整算法唯一的美中不足在于：它的整体收集效率并不高。因为标-整算法不仅仅要标记对象，同时还要移动存活对象，所以整个GC过程下来，它所需要耗费的时间资源开销必然是不小的。</p>
<p>不过一般年老代空间都是采用标-整算法，因为一方面年老代GC次数方面远没有新生代频繁，同时，晋升年老代的对象一般来说体积不会很小，所以在晋升时需要足够的内存大小分配，如果采用标-清算法会导致大对象无法进行分配，如若采用复制算法则没有新的空间为年老代提供。</p>
<p>如上三种GC算法则是JVM虚拟机的基础GC算法，综合对比来看：</p>
<p>收集速度：复制算法 &gt; 标-清算法 &gt; 标-整算法<br>
内存整齐度：复制算法 = 标-整算法 &gt; 标-清算法<br>
内存利用率：标-整算法 &gt; 标-清算法 &gt; 复制算法</p>
<h3 id="18cms-g1垃圾收集器的对比">18.CMS、G1垃圾收集器的对比</h3>
<p>G1 在压缩空间方面有优势。<br>
G1 通过将内存空间分成区域（Region）的方式避免内存碎片问题。Eden, Survivor, Old 区不再固定、在内存使用效率上来说更灵活。<br>
G1 可以通过设置预期停顿时间（Pause Time）来控制垃圾收集时间避免应用雪崩现象。<br>
G1 在回收内存后会马上同时做合并空闲内存的工作、而 CMS 默认是在 STW（stop the world）的时候做。<br>
G1 会在 Young GC 中使用、而 CMS 只能在 O 区使用。<br>
吞吐量优先：G1<br>
响应优先：CMS<br>
CMS 的缺点是对 cpu 的要求比较高。G1 是将内存化成了多块，所有对内段的大小有很大的要求。<br>
CMS 是清除，所以会存在很多的内存碎片。G1 是整理，所以碎片空间较小。</p>
<h3 id="19gc类型yonggc-oldgc-mixedgc-fullgc">19.GC类型：YongGC、OldGC、MixedGC、FullGC</h3>
<p><code>Mixed GC</code><br>
Mixed GC 是 G1 中特有的概念，其实说白了，主要就是说在 G1 中，一旦老年代占据堆内存的 45%（-XX:InitiatingHeapOccupancyPercent：设置触发标记周期的 Java 堆占用率阈值，默认值是 45%。这里的Java 堆占比指的是 non_young_capacity_bytes，包括 old + humongous），就要触发 Mixed GC，此时对年轻代和老年代都会进行回收。Mixed GC 只有 G1 中才会出现。</p>
<h3 id="20cms垃圾收集器的特点">20.CMS垃圾收集器的特点</h3>
<p>CMS收集器是一种以获取最短回收停顿时间为目标的收集器。基于“标记-清除”算法实现，它的运作过程如下：<br>
1）初始标记<br>
2）并发标记<br>
3）重新标记<br>
4）并发清除<br>
初始标记、重新标记这两个步骤仍然需要“stop the world”，初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing，而重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生表动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长点，但远比并发标记的时间短。</p>
<p>CMS是一款优秀的收集器，主要优点：并发收集、低停顿。</p>
<p>缺点：</p>
<p>1）CMS收集器对CPU资源非常敏感。在并发阶段，它虽然不会导致用户线程停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。<br>
2）CMS收集器无法处理浮动垃圾，可能会出现“Concurrent Mode Failure（并发模式故障）”失败而导致Full GC产生。<br>
浮动垃圾：由于CMS并发清理阶段用户线程还在运行着，伴随着程序运行自然就会有新的垃圾不断产生，这部分垃圾出现的标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC中再清理。这些垃圾就是“浮动垃圾”。<br>
3）CMS是一款“标记--清除”算法实现的收集器，容易出现大量空间碎片。当空间碎片过多，将会给大对象分配带来很大的麻烦，往往会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发一次Full GC。</p>
<h3 id="21cms垃圾收集器收集过称初始标记并发标记重新标记并发清理">21.CMS垃圾收集器收集过称，初始标记，并发标记，重新标记，并发清理</h3>
<p>CMS 处理过程有七个步骤：</p>
<ol>
<li>初始标记(CMS-initial-mark) ,会导致stw;</li>
<li>并发标记(CMS-concurrent-mark)，与用户线程同时运行；</li>
<li>预清理（CMS-concurrent-preclean），与用户线程同时运行；</li>
<li>可被终止的预清理（CMS-concurrent-abortable-preclean） 与用户线程同时运行；</li>
<li>重新标记(CMS-remark) ，会导致swt；</li>
<li>并发清除(CMS-concurrent-sweep)，与用户线程同时运行；</li>
<li>并发重置状态等待下次CMS的触发(CMS-concurrent-reset)，与用户线程同时运行；</li>
</ol>
<p>CMS是老年代垃圾收集器，在收集过程中可以与用户线程并发操作。它可以与Serial收集器和Parallel New收集器搭配使用。CMS牺牲了系统的吞吐量来追求收集速度，适合追求垃圾收集速度的服务器上。可以通过JVM启动参数：-XX:+UseConcMarkSweepGC来开启CMS。</p>
<p><code>初始标记</code><br>
这一步的作用是标记存活的对象，有两部分：</p>
<ol>
<li>标记老年代中所有的GC Roots对象</li>
<li>标记年轻代中活着的对象引用到的老年代的对象</li>
</ol>
<p><code>并发标记</code><br>
从“初始标记”阶段标记的对象开始找出所有存活的对象;因为是并发运行的，在运行期间会发生新生代的对象晋升到老年代、或者是直接在老年代分配对象、或者更新老年代对象的引用关系等等，对于这些对象，都是需要进行重新标记的，否则有些对象就会被遗漏，发生漏标的情况。为了提高重新标记的效率，该阶段会把上述对象所在的Card标识为Dirty，后续只需扫描这些Dirty Card的对象，避免扫描整个老年代；并发标记阶段只负责将引用发生改变的Card标记为Dirty状态，不负责处理；</p>
<blockquote>
<p>JVM会通过Card(卡片)的方式将发生改变的老年代区域标记为“脏”区，这就是所谓的卡片标记（Card Marking）</p>
</blockquote>
<p>并发标记的特点是和应用程序线程同时运行。并不是老年代的所有存活对象都会被标记，因为标记的同时应用程序会改变一些对象的引用等。 由于这个阶段是和用户线程并发的，可能会导致concurrent mode failure。</p>
<p><code>重新标记</code><br>
最终标记是此阶段GC事件中的第二次（也是最后一次）STW停顿。目标： 重新扫描堆中的对象，因为之前的预清理阶段是并发执行的，有可能GC线程跟不上应用程序的修改速度。扫描范围： 新生代对象+GC Roots+被标记为“脏”区的对象。如果预清理阶段没有做好，这一步扫描新生代的时候就会花很多时间。</p>
<p><code>并发清理</code><br>
此阶段与应用程序并发执行，不需要STW停顿。JVM在此阶段删除不再使用的对象，并回收他们占用的内存空间。因为重新标记已经把所有还在使用的对象进行了标记，因此此阶段可以与应用线程并发的执行。</p>
<h3 id="22cms垃圾收集器会产生内存碎片吗如何处理">22.CMS垃圾收集器会产生内存碎片吗？如何处理？</h3>
<p>由于CMS采用的是&quot;标记-清除&quot;算法，那么在运行到一定时间后，会产生一些内存碎片。当有新的对象要进入老年代时，可能会造成内存不够分配的情况。这个时候可以通过参数<code>-XX:CMSFullGCsBeforeCompaction</code>进行内存整理。比如配置-XX:CMSFullGCsBeforeCompaction=5，那么每执行5次Full GC就会对老年代进行内存空间整理。</p>
<h3 id="23并发标记三色标记法-浮动垃圾-漏标">23.并发标记：三色标记法。浮动垃圾、漏标</h3>
<p><code>三色标记法</code><br>
并发标记过程中允许用户线程正常执行，采用三色标记算法从初始标记的对象开始遍历整个老年代，进行存活对象标记，这个过程相对过长，但对于用户来说是几乎无感知的。</p>
<p>三色标记是CMS采用的标记对象算法，可以缩短STW时间并达到标记存活对象的效果。按照对象是否被垃圾收集器访问过这个条件，标记的颜色有下面三种：</p>
<p>白色：表示该对象还没有被访问过。在可达性分析开始阶段，除GC Root对象外，所有的对象节点都是白色的。如果在可达性分析执行完后，还有白色状态的对象，即对象不可达，那么这些就可以被认定为垃圾对象。<br>
黑色：表示该对象已经被访问过，并且该对象的引用对象也全部被访问过，该对象可达，为存活对象。<br>
灰色：表示该对象已经被访问过，但是存在引用对象还没有被访问过。比如在访问A对象时，A对象内部引用了B和C对象，当访问B对象时，发现内部没有引用其他对象，那么此时B对象已经被GC访问过了。但对于A对象来说，尽管B对象已经被访问，C对象还没有被访问，所以A的对象标记为灰色。</p>
<p><code>浮动垃圾</code><br>
在并发清除过程中，由于用户线程也在不断的运行，如果出现漏标情况，就会产生一些垃圾对象，这些垃圾对象叫做浮动垃圾。这些垃圾对象只能等待下次GC时才能被回收，在没被回收之前仍然占用内存空间。</p>
<p><code>错标</code><br>
在并发标记过程中不会触发STW，会导致对象引用关系的变更。那么就会产生一些问题。比如A对象被垃圾回收器访问后被标记成了黑色，但是用户线程的执行A对象和H对象产生了引用关系，但是A已经被标记为白色了，不会在被重新访问，那就意味着H对象被误认为垃圾对象了，当H对现象被回收后，那将会有严重的错标问题了。</p>
<p><code>漏标</code><br>
除错标问题外，还有另外一种情况。当扫描到C对象时，由于C对象还有引用对象没有被扫描，此时C对象会被标为灰色。但是由于用户线程的运行，A对象和C对象的引用关系被取消，垃圾收集器会继续由C对象向下进行可达性分析。原则上C对象将会是垃圾对象，但是实际上这些对象仍然会被标记为存活对象，这种情况称为漏标。</p>
<h3 id="24g1垃圾收集器的特点大内存友好可预计的暂停时间">24.G1垃圾收集器的特点，大内存友好，可预计的暂停时间</h3>
<p><code>特点</code></p>
<ol>
<li>并行和并发<br>
并行性: G1在回收期间,可以有多个GC线程同时工作,有效利用多核计算能力。此时用户线程STW<br>
并发性: G1拥有与应用程序交替执行的能力,部分工作可以和应用程序同时执行,因此,一般来说,不会在整个回收阶段发生完全阻塞应用程序的情况</li>
<li>分代收集<br>
从分代上看,G1依然属于分代型垃圾回收器,它会区分年轻代和老年代,年轻代依然有Eden区和Survivor区。但从堆的结构上看,它不要求整个Eden区、年轻代或者老年代都是连续的,也不再坚持固定大小和固定数量。将堆空间分为若干个<code>区域(Region)</code>,这些区域中包含了逻辑上的年轻代和老年代。和之前的各类回收器不同,它同时兼顾年轻代和老年代。</li>
<li>空间整合<br>
G1将内存划分为一个个的region。 内存的回收是以region作为基本单位的。Region之间是复制算法,但整体上实际可看作是标记一压缩(Mark一Compact)算法,两种算法都可以避免内存碎片。这种特性有利于程序长时间运行,分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。</li>
<li>可预测的停顿时间模型</li>
</ol>
<p><code>可预计暂停时间</code><br>
可以通过-XX:MaxGCPauseMillis参数指定预期的停顿时间，G1 GC的停顿预测模型是以衰减均值（Decaying Average）为理论基础来实现的，在垃圾收集过程中，G1收集器会记录每个Region的回收耗时、每个Region记忆集里的脏卡数量等各个可测量的步骤花费的成本，并分析得出平均值、标准偏差、置信度等统计信息。“衰减平均值”是指它会比普通的平均值更容易受到新数据的影响，平均值代表整体平均状态，但衰减平均值更准确地代表“最近的”平均状态。也就是说，Region的统计状态越新越能决定其回收的价值。然后通过这些信息预测现在开始回收的话，由哪些Region组成回收集才可以在不超过期望停顿时间的约束下获得最高的收益。</p>
<h3 id="25如何达到可预测的暂定时间remembered-set">25.如何达到可预测的暂定时间？Remembered Set</h3>
<p>见24。</p>
<p><code>已记忆集合</code><br>
<img src="https://q456qq520.github.io/post-images/1675848326032.png" alt="" loading="lazy"><br>
在串行和并行收集器中，GC通过整堆扫描，来确定对象是否处于可达路径中。然而G1为了避免STW式的整堆扫描，在每个分区记录了一个已记忆集合(Remembered Set)，内部类似一个反向指针，记录引用分区内对象的卡片索引。当要回收该分区时，通过扫描分区的RSet，来确定引用本分区内的对象是否存活，进而确定本分区内的对象存活情况。<br>
事实上，并非所有的引用都需要记录在RSet中，如果一个分区确定需要扫描，那么无需RSet也可以无遗漏的得到引用关系。那么引用源自本分区的对象，当然不用落入RSet中；同时，G1 GC每次都会对年轻代进行整体收集，因此引用源自年轻代的对象，也不需要在RSet中记录。最后只有老年代的分区可能会有RSet记录，这些分区称为拥有RSet分区。</p>
<p>对于年轻代的Region，它的RSet 只保存了来自老年代的引用（因为年轻代的没必要存储啊，自己都要做Minor GC了）而对于老年代的 Region 来说，它的 RSet 也只会保存老年代对它的引用（在G1垃圾收集器，老年代回收之前，都会先对年轻代进行回收，所以没必要保存年轻代的引用</p>
<h3 id="26g1垃圾收集器发生fullgc正常吗">26.G1垃圾收集器发生fullGC正常吗？</h3>
<p>G1的初衷就是要避免Fu1l GC的出现。但是如果上述方式不能正常工作，G1会停止应用程序的执行(Stop-The-World) ，使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。<br>
要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢? 比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用，则会回退到full gc， 这种情况可以通过增大内存解决。<br>
导致G1Full GC的原因可能有两个: .</p>
<ol>
<li>回收的时候没有足够的to-space来存放晋升的对象</li>
<li>并发处理过程没完成空间就耗尽了</li>
</ol>
<h3 id="27g1垃圾收集器收集过称年轻代gc老年代并发标记过程45混合回收fullgc">27.G1垃圾收集器收集过称，年轻代GC，老年代并发标记过程（45%），混合回收，fullGC</h3>
<p><code>设计理念</code></p>
<ol>
<li>区域划分
<ul>
<li>Region区域：将Java堆划分为多个大小相等的Region，每个Region都可以是新生代、老年代。G1收集器根据角色的不同采用不同的策略去处理。在每个分区内部又被分成了若干个大小为512 Byte卡片(Card)，标识堆内存最小可用粒度所有分区的卡片将会记录在全局卡片表(Global Card Table)中，分配的对象会占用物理上连续的若干个卡片，当查找对分区内对象的引用时便可通过记录卡片来查找该引用对象(见RSet)。每次对内存的回收，都是对指定分区的卡片进行处理。</li>
<li>Humongous区域（Region中的一部分）：专门用来存储大对象（超过Region容量一半的对象即为大对象），超过整个Region区域的会放在多个连续的Humongous区。 G1把Humongous当做老年代的一部分。</li>
</ul>
</li>
<li>垃圾收集<br>
G1的Collector一侧就是两个大部分，并且这两个部分可以相对独立执行。全局并发标记（global concurrent marking）和拷贝存活对象（evacuation）。</li>
</ol>
<p><code>垃圾回收过程</code><br>
<strong>年轻代GC (Young GC)</strong><br>
回收时机<br>
(1). 当Eden空间耗尽时,G1会启动一次年轻代垃圾回收过程<br>
(2). 年轻代垃圾回收只会回收Eden区和Survivor区</p>
<p>回收过程</p>
<ol>
<li>根扫描:一定要考虑remembered Set,看是否有老年代中的对象引用了新生代对象<br>
根是指static变量指向的对象,正在执行的方法调用链条上的局部变量等。根引用连同RSet记录的外部引用作为扫描存活对象的入口)</li>
<li>更新RSet:处理dirty card queue中的card,更新RSet。 此阶段完成后,RSet可以准确的反映老年代对所在的内存分段中对象的引用</li>
<li>处理RSet:识别被老年代对象指向的Eden中的对象,这些被指向的Eden中的对象被认为是存活的对象</li>
<li>复制对象:此阶段,对象树被遍历,Eden区 内存段中存活的对象会被复制到Survivor区中空的内存分段,Survivor区内存段中存活的对象如果年龄未达阈值,年龄会加1,达到阀值会被会被复制到old区中空的内存分段。如果Survivor空间不够,Eden空间的部分数据会直接晋升到老年代空间</li>
<li>处理引用:处理Soft,Weak, Phantom, Final, JNI Weak等引用。最终Eden空间的数据为空,GC停止工作,而目标内存中的对象都是连续存储的,没有碎片,所以复制过程可以达到内存整理的效果,减少碎片。</li>
</ol>
<p><strong>老年代并发标记过程 (Concurrent Marking)</strong></p>
<ol>
<li>初始标记阶段:<br>
标记从根节点直接可达的对象。这个阶段是STW的,并且会触发一次年轻代GC</li>
<li>根区域扫描(Root Region Scanning):<br>
G1 GC扫描Survivor区**直接可达的老年代区域对象,**并标记被引用的对象。这一过程必须在young GC之前完成(YoungGC时,会动Survivor区,所以这一过程必须在young GC之前完成)</li>
<li>并发标记(Concurrent Marking):<br>
在整个堆中进行并发标记(和应用程序并发执行),此过程可能被young GC中断。在并发标记阶段,若发现区域对象中的所有对象都是垃圾,那这个区域会被立即回收。同时,并发标记过程中,会计算每个区域的对象活性(区域中存活对象的比例)。</li>
<li>再次标记(Remark):<br>
由于应用程序持续进行,需要修正上一次的标记结果。是STW的。G1中采用了比CMS更快的初始快照算法:snapshot一at一the一beginning (SATB).</li>
<li>独占清理(cleanup,STW):<br>
计算各个区域的存活对象和GC回收比例,并进行排序,识别可以混合回收的区域。为下阶段做铺垫。是STW的。(这个阶段并不会实际上去做垃圾的收集)</li>
<li>并发清理阶段:识别并清理完全空闲的区域</li>
</ol>
<p><strong>混合回收(Mixed GC)</strong><br>
Mixed GC并不是FullGC,老年代的堆占有率达到参数(-XX:InitiatingHeapOccupancyPercent)设定的值则触发,回收所有的Young和部分Old(根据期望的GC停顿时间确定old区垃圾收集的优先顺序)以及大对象区,正常情况G1的垃圾收集是先做MixedGC,主要使用复制算法,需要把各个region中存活的对象拷贝到别的region里去,拷贝过程中如果发现没有足够的空region能够承载拷贝对象就会触发一次Full GC。</p>
<p>由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1MixedGCLiveThresholdPercent，默认为65%，意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。</p>
<p>混合回收并不一定 要进行8次。有一个阈值**-XX :G1HeapWastePercent**,默认值为10%，意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%，则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。</p>
<h3 id="28工作经验性能有问题如何分析oom怎么处理mat工具arthas工具">28.工作经验，性能有问题如何分析，OOM怎么处理？MAT工具，Arthas工具</h3>
<h2 id="框架">框架</h2>
<h3 id="1-spring-ioc体系结构设计">1. Spring - IOC体系结构设计</h3>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1675753715033.png" alt="" loading="lazy"></figure>
<h3 id="2-spring-ioc初始化流程">2. Spring IOC初始化流程</h3>
<figure data-type="image" tabindex="4"><img src="https://q456qq520.github.io/post-images/1675753914567.png" alt="" loading="lazy"></figure>
<h3 id="3-spring-ioc中bean的生命周期">3. spring ioc中bean的生命周期</h3>
<p>Spring如何实现将资源配置（以xml配置为例）通过加载，解析，生成BeanDefination并注册到IoC容器中的；容器中存放的是Bean的定义即BeanDefinition放到beanDefinitionMap中，本质上是一个ConcurrentHashMap&lt;String, Object&gt;；并且BeanDefinition接口中包含了这个类的Class信息以及是否是单例等。<br>
<img src="https://q456qq520.github.io/post-images/1675753903296.png" alt="" loading="lazy"></p>
<h3 id="4-beanfactory-和-factorybean-的区别">4. BeanFactory 和 FactoryBean 的区别？</h3>
<p>BeanFactory是接口，提供了IOC容器最基本的形式，给具体的IOC容器的实现提供了规范，它定义了getBean()、containsBean()等管理Bean的通用方法，并不是IOC容器的具体实现，但是Spring容器给出了很多种实现，Spring的容器都是它的具体实现如：</p>
<ul>
<li>DefaultListableBeanFactory</li>
<li>XmlBeanFactory</li>
<li>ApplicationContext</li>
</ul>
<pre><code class="language-java">public interface BeanFactory {

	//对FactoryBean的转义定义，因为如果使用bean的名字检索FactoryBean得到的对象是工厂生成的对象，
	//如果需要得到工厂本身，需要转义
	String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;;

	//根据bean的名字，获取在IOC容器中得到bean实例
	Object getBean(String name) throws BeansException;

	//根据bean的名字和Class类型来得到bean实例，增加了类型安全验证机制。
	&lt;T&gt; T getBean(String name, @Nullable Class&lt;T&gt; requiredType) throws BeansException;

	Object getBean(String name, Object... args) throws BeansException;

	&lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException;

	&lt;T&gt; T getBean(Class&lt;T&gt; requiredType, Object... args) throws BeansException;

	//提供对bean的检索，看看是否在IOC容器有这个名字的bean
	boolean containsBean(String name);

	//根据bean名字得到bean实例，并同时判断这个bean是不是单例
	boolean isSingleton(String name) throws NoSuchBeanDefinitionException;

	boolean isPrototype(String name) throws NoSuchBeanDefinitionException;

	boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException;

	boolean isTypeMatch(String name, @Nullable Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException;

	//得到bean实例的Class类型
	@Nullable
	Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException;

	//得到bean的别名，如果根据别名检索，那么其原名也会被检索出来
	String[] getAliases(String name);
}
</code></pre>
<p>FactoryBean也是接口，为IOC容器中Bean的实现提供了更加灵活的方式，FactoryBean在IOC容器的基础上给Bean的实现加上了一个简单工厂模式和装饰模式，我们可以在getObject()方法中灵活配置。一般情况下，Spring通过反射机制利用<bean>的class属性指定实现类实例化Bean，在某些情况下，实例化Bean过程比较复杂，如果按照传统的方式，则需要在<bean>中提供大量的配置信息。配置方式的灵活性是受限的，这时采用编码的方式可能会得到一个简单的方案。Spring为此提供了一个org.springframework.bean.factory.FactoryBean的工厂类接口，用户可以通过实现该接口定制实例化Bean的逻辑。</p>
<p>BeanFactory是个Factory，也就是IOC容器或对象工厂，FactoryBean是个Bean。在Spring中，所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean。</p>
<pre><code class="language-java">public interface FactoryBean&lt;T&gt; {

	//从工厂中获取bean
	@Nullable
	T getObject() throws Exception;

	//获取Bean工厂创建的对象的类型
	@Nullable
	Class&lt;?&gt; getObjectType();

	//Bean工厂创建的对象是否是单例模式
	default boolean isSingleton() {
		return true;
	}
}
</code></pre>
<p>不同于普通Bean的是：它是实现了FactoryBean<T>接口的Bean，根据该Bean的ID从BeanFactory中获取的实际上是FactoryBean的getObject()返回的对象，而不是FactoryBean本身，如果要获取FactoryBean对象，请在id前面加一个&amp;符号来获取。</p>
<p>他们两个都是个工厂，但FactoryBean本质上还是一个Bean，也归BeanFactory管理。BeanFactory是Spring容器的顶层接口，FactoryBean更类似于用户自定义的工厂接口。</p>
<h3 id="5-springboot应用启动流程有哪些扩展点">5. springboot应用启动流程，有哪些扩展点</h3>
<p>链接:<a href="/post/springboot">《SpringBoot启动流程</a></p>
<h3 id="6-value之类的标签是如何实现的">6. @Value之类的标签是如何实现的</h3>
<p>Spring中@Autowire，@Value 注解实现原理基本一致。</p>
<p>在spring中是由<code>AutowiredAnnotationBeanPostProcessor</code>解析处理@Value注解。AutowiredAnnotationBeanPostProcessor是一个<code>BeanPostProcessor</code>，所以每个类的实例化都过经过AutowiredAnnotationBeanPostProcessor类。当<code>post-processor</code>处理bean时，会解析<code>bean Class</code>的所有属性，在解析时会判断属性上是否标有@Value注解，有就解析这个@Value的属性值，将解析后结果放入<code>AutowiredFieldElement</code>类型<code>InjectionMetaData.checkedElements</code>中，当给属性赋值时会使用<code>checkedElements</code>，从而得到@Value注解的<code>Filed</code>属性，调用<code>AutowiredFieldElement.inject()</code>方法进行解析，解析时会使用<code>DefaultListableBeanFactory</code>(用于解析<code>${}</code>)和<code>TypeConverter</code>(用于类型转换)，从而得到属性的值，最后调用<code>field.set(bean, value)</code>，从而获取的值赋给bean的field。</p>
<p>具体步骤为：将class的标注@Value的所有信息转存InjectionMetadata.InjectedElement集合中。入口为<code>AutowiredAnnotationBeanPostProcessor.buildAutowiringMetadata(Class clazz)</code>。这个方法用于遍历和解析clazz的所有filed和method，解析其上的<code>@Value、@Autowired、@Inject</code>注解，然后放入类型为<code>InjectionMetadata.InjectedElement</code>的<code>elements</code>中，elements再放入<code>metadata(=new InjectionMetadata(clazz, elements))</code>中。再将metadata放入缓存<code>injectionMetadataCache</code>中，后面会从缓存中取值</p>
<h3 id="7-spring的动态代理实现有哪些方式从源码来看是如何实现的">7. spring的动态代理实现有哪些方式？从源码来看是如何实现的？</h3>
<ol>
<li>
<p>JDK动态代理<br>
JDK是通过代理类跟目标类实现同一个接口来实现代理的</p>
</li>
<li>
<p>CGLIB代理<br>
CGlib是通过继承目标类来实现代理的</p>
</li>
</ol>
<p>在spring中的实现具体为：</p>
<pre><code class="language-java">public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException {
    //这里初步判断代理的创建方式，如果不满足则直接使用 JDK 动态代理，如果满足条件，则进一步在判断是否使用 JKD 动态代理还是 CGLIB 代理
    if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) {
        Class&lt;?&gt; targetClass = config.getTargetClass();
        if (targetClass == null) {
            throw new AopConfigException(&quot;......&quot;);
        }
        // 如果代理的是接口或者设置代理的类就是当前类，则使用 JDK 动态代理
        if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) {
            return new JdkDynamicAopProxy(config);
        }
        // 否则使用 CGLIB 代理
        return new ObjenesisCglibAopProxy(config);
    }
    // 条件不满足CGBLIB的方式直接使用JDK动态代理
    else {
        return new JdkDynamicAopProxy(config);
    }
}
</code></pre>
<p>这里的 if 条件有三个：</p>
<ol>
<li>config.isOptimize() : 用来控制通过 CGLIB 创建的代理是否使用激进的优化策略，目前仅用于 CGLIB 代理</li>
<li>config.isProxyTargetClass() :  在 Spring AOP 注解方式源码解析 中了解到，我们可以强制 Spring 完全使用 CGLIB 进行代理，只要在配置文件配置 proxy-target-class 属性为true即可，如：&lt;aop:aspectj-autoproxy expose-proxy=&quot;true&quot; proxy-target-class=&quot;true&quot;/&gt;，如果配置个该属性，则会使用 CGLIB 来创建代理</li>
<li>hasNoUserSuppliedProxyInterfaces(config) : 是否存在代理接口，如果不存在代理接口，则使用 CGLIB 进行代理</li>
</ol>
<p>如果这三个条件有一个满足，则会再进一次判断，需要代理的类是否是接口或者是否设置的就是代理当前类，如果是，则还是会使用 JDK 动态代理，否则的话才会使用 CGLIB 代理。</p>
<p>接下来看下 JDK 动态代理和 CGLIB 代理的创建过程：</p>
<p><strong>JDK 动态代理</strong></p>
<pre><code class="language-java">return new JdkDynamicAopProxy(config);
// JdkDynamicAopProxy.java
public JdkDynamicAopProxy(AdvisedSupport config) throws AopConfigException {
    this.advised = config;
}
</code></pre>
<p>通过 JDK 动态代理来获取代理的方法 getProxy():</p>
<pre><code class="language-java">public Object getProxy(ClassLoader classLoader) {
    // 获取代理类的接口
    Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true);
    // 处理 equals , hashcode 方法
    findDefinedEqualsAndHashCodeMethods(proxiedInterfaces);
    // 创建代理
    return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);
}
</code></pre>
<p>Spring 使用 JDK 创建代理和我们使用的 JDK 来创建代理是没有区别的，都是使用 <code>Proxy.newProxyInstance</code>的方式来创建；我们知道 JDK 动态代理有个<code>invoke</code>方法，用来执行目标方法，而 JdkDynamicAopProxy 实现了 <code>InvocationHandler</code>接口，所有它也会重写该方法，在该方法中植入增强：</p>
<pre><code class="language-java">public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
    MethodInvocation invocation;
    Object oldProxy = null;
    boolean setProxyContext = false;
    // 目标类
    TargetSource targetSource = this.advised.targetSource;
    Object target = null;

    try {
        // 如果接口没有定义 equals 方法且当前方法是 equals 方法，则不会增强，直接返回
        if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) {
            return equals(args[0]);
        }
        else if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) {
            // 如果接口没有定义 hashCode方法且当前方法是 hashCode方法，则不会增强，直接返回
            return hashCode();
        }
        else if (method.getDeclaringClass() == DecoratingProxy.class) {
            return AopProxyUtils.ultimateTargetClass(this.advised);
        }
        else if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp;
                method.getDeclaringClass().isAssignableFrom(Advised.class)) {
            // 如果方法所在的类和Advised是同一个类或者是父类子类关系，则直接执行
            return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args);
        }
        // 返回值
        Object retVal;

        // 这里对应的是expose-proxy属性的应用，把代理暴露处理
        // 目标方法内部的自我调用将无法实施切面中的增强，所以在这里需要把代理暴露出去
        if (this.advised.exposeProxy) {
            oldProxy = AopContext.setCurrentProxy(proxy);
            setProxyContext = true;
        }

        target = targetSource.getTarget();
        Class&lt;?&gt; targetClass = (target != null ? target.getClass() : null);

        // 获取该方法的拦截器
        List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);

        //如果方法的拦截器为空，则直接执行目标方法，避免创建 MethodInvocation 对象
        if (chain.isEmpty()) {
            Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args);
            // 执行目标方法：method.invoke(target, args)
            retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse);
        }
        else {
            // 把所有的拦截器封装在ReflectiveMethodInvocation中，以便于链式调用 
            invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain);
            // 执行拦截器链
            retVal = invocation.proceed();
        }
        // ..........
        return retVal;
    }
    finally {
      // .            
    }
}
</code></pre>
<p>在执行拦截器方法 proceed 中执行增强方法，比如前置增强在方法之前执行，后置增强在方法之后执行，proceed 方法如下：</p>
<pre><code class="language-java">public Object proceed() throws Throwable {
    //当执行完所有增强方法后执行目标方法
    if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) {
        // method.invoke(target, args)
        return invokeJoinpoint();
    }
     // 获取下一个要执行的拦截器
    Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex);

    if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) {
        // 动态匹配
        InterceptorAndDynamicMethodMatcher dm = interceptorOrInterceptionAdvice;
        Class&lt;?&gt; targetClass = (this.targetClass != null ? this.targetClass : this.method.getDeclaringClass());
        // 如果能够匹配，则执行拦截器的方法，
        if (dm.methodMatcher.matches(this.method, targetClass, this.arguments)) {
            // 比如 @After @Before 对应的增强器（拦截器）的方法
            // 比如 @After 对应的增强器 AspectJAfterAdvice 的invoke方法为：MethodInvocation.proceed();
            return dm.interceptor.invoke(this);
        }
        else {
            // 如果动态匹配失败，则跳过该拦截器，执行下一个拦截器
            return proceed();
        }
    }
    else {
        // 普通拦截器，直接调用
        return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this);
    }
}
</code></pre>
<p>在该方法中完成了增强的植入，主要逻辑就是，每个方法都会有一个拦截器链，在 AOP 中我们称之为增强，然后循环执行每个拦截器链，当执行完所有的拦截器后，才会执行目标方法。比如 <code>@After</code>对应的增强器<code>AspectJAfterAdvice</code>， <code>@Around</code>对应的增强器<code>AspectJAroundAdvice</code>等。</p>
<p>以上就是 Spring 通过 JDK 动态代理来实现 AOP 的一个过程。</p>
<p><strong>CGLIB 代理</strong><br>
<code>ObjenesisCglibAopProxy</code>继承于<code>CglibAopProxy</code></p>
<pre><code class="language-java">return new ObjenesisCglibAopProxy(config)

public CglibAopProxy(AdvisedSupport config) throws AopConfigException {
    this.advised = config;
    this.advisedDispatcher = new AdvisedDispatcher(this.advised);
}
</code></pre>
<pre><code class="language-java">public Object getProxy(ClassLoader classLoader) {
    // 代理的目标类
    Class&lt;?&gt; rootClass = this.advised.getTargetClass();
    Class&lt;?&gt; proxySuperClass = rootClass;
    if (ClassUtils.isCglibProxyClass(rootClass)) {
        proxySuperClass = rootClass.getSuperclass();
        Class&lt;?&gt;[] additionalInterfaces = rootClass.getInterfaces();
        for (Class&lt;?&gt; additionalInterface : additionalInterfaces) {
            this.advised.addInterface(additionalInterface);
        }
    }
    // 创建并配置 CGLIB Enhancer
    Enhancer enhancer = createEnhancer();
    if (classLoader != null) {
        enhancer.setClassLoader(classLoader);
        if (classLoader instanceof SmartClassLoader &amp;&amp;((SmartClassLoader) classLoader).isClassReloadable(proxySuperClass)) {
            enhancer.setUseCache(false);
        }
    }
    enhancer.setSuperclass(proxySuperClass);
    enhancer.setInterfaces(AopProxyUtils.completeProxiedInterfaces(this.advised));
    enhancer.setNamingPolicy(SpringNamingPolicy.INSTANCE);
    enhancer.setStrategy(new ClassLoaderAwareUndeclaredThrowableStrategy(classLoader));

    // 设置拦截器
    Callback[] callbacks = getCallbacks(rootClass);
    Class&lt;?&gt;[] types = new Class&lt;?&gt;[callbacks.length];
    for (int x = 0; x &lt; types.length; x++) {
        types[x] = callbacks[x].getClass();
    }
    enhancer.setCallbackFilter(new ProxyCallbackFilter(this.advised.getConfigurationOnlyCopy(), this.fixedInterceptorMap, this.fixedInterceptorOffset));
    enhancer.setCallbackTypes(types);

    //生成代理类和创建代理
    return createProxyClassAndInstance(enhancer, callbacks);
}

// 生成代理类和创建代理
protected Object createProxyClassAndInstance(Enhancer enhancer, Callback[] callbacks) {
    enhancer.setInterceptDuringConstruction(false);
    enhancer.setCallbacks(callbacks);
    return (this.constructorArgs != null &amp;&amp; this.constructorArgTypes != null ?
            enhancer.create(this.constructorArgTypes, this.constructorArgs) :
            enhancer.create());
}
</code></pre>
<p>从上述的方法可知，Sping 使用 CGLIB 来创建代理类和代理对象和我们使用的一样，都是使用 Enhancer.create() 来创建，这里主要的是设置拦截器，通过 getCallbacks () 方法来实现的，如下：</p>
<pre><code class="language-java">private Callback[] getCallbacks(Class&lt;?&gt; rootClass) throws Exception {
    //expose-proxy 属性
    boolean exposeProxy = this.advised.isExposeProxy();
    boolean isFrozen = this.advised.isFrozen();
    boolean isStatic = this.advised.getTargetSource().isStatic();

    // 将拦截器封装在 DynamicAdvisedInterceptor 中
    Callback aopInterceptor = new DynamicAdvisedInterceptor(this.advised);

    //暴露代理
    Callback targetInterceptor;
    if (exposeProxy) {
        targetInterceptor = (isStatic ?
                new StaticUnadvisedExposedInterceptor(this.advised.getTargetSource().getTarget()) :
                new DynamicUnadvisedExposedInterceptor(this.advised.getTargetSource()));
    }
    else {
        targetInterceptor = (isStatic ?
                new StaticUnadvisedInterceptor(this.advised.getTargetSource().getTarget()) :
                new DynamicUnadvisedInterceptor(this.advised.getTargetSource()));
    }
    // 将拦截器 aopInterceptor 进入到 Callback 中 
    Callback[] mainCallbacks = new Callback[] {
            aopInterceptor,  // for normal advice
            targetInterceptor,  // invoke target without considering advice, if optimized
            new SerializableNoOp(),  // no override for methods mapped to this
            targetDispatcher, this.advisedDispatcher,
            new EqualsInterceptor(this.advised),
            new HashCodeInterceptor(this.advised)
    };
    // ..............
    return callbacks;
}
</code></pre>
<p>使用 CGLIB 来实现代理功能的时候，当代理执行的时候，会调用<code>intercept</code>方法，和 JKD 动态代理的 invoke 方法类似；Spring 中 CGLIB 的 intercept 方法如下，该方法在 <code>DynamicAdvisedInterceptor</code>中，从上面的代理知道，使用它来封装拦截器，它是 <code>CglibAopProxy</code>的一个子类：</p>
<pre><code class="language-java">public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy){
    Object oldProxy = null;
    boolean setProxyContext = false;
    Object target = null;
    // 目标类
    TargetSource targetSource = this.advised.getTargetSource();
    // 处理 expose-proxy 属性，暴露代理
    if (this.advised.exposeProxy) {
        oldProxy = AopContext.setCurrentProxy(proxy);
        setProxyContext = true;
    }
    target = targetSource.getTarget();
    Class&lt;?&gt; targetClass = (target != null ? target.getClass() : null);

    // 获取拦截器链
    List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass);

    // 返回值
    Object retVal;

    if (chain.isEmpty() &amp;&amp; Modifier.isPublic(method.getModifiers())) {
        Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args);
        // 如果拦截器为空则直接执行目标方法
        retVal = methodProxy.invoke(target, argsToUse);
    }
    else {
        //封装拦截器链并执行
        retVal = new CglibMethodInvocation(proxy, target, method, args, targetClass, chain, methodProxy).proceed();
    }
    // 处理返回值类型
    retVal = processReturnType(proxy, target, method, retVal);
    return retVal;
    // .....................
}
</code></pre>
<p>CGLIB 使用<code>CglibMethodInvocation</code>来封装拦截器链，它是 CglibAopProxy 的一个内部类：</p>
<pre><code class="language-java">private static class CglibMethodInvocation extends ReflectiveMethodInvocation {

    @Nullable
    private final MethodProxy methodProxy;

    public CglibMethodInvocation(Object proxy, Object target, Method method,Object[] arguments, Class&lt;?&gt; targetClass, List&lt;Object&gt; interceptorsAndDynamicMethodMatchers, MethodProxy methodProxy) {

        super(proxy, target, method, arguments, targetClass, interceptorsAndDynamicMethodMatchers);

        this.methodProxy = (Modifier.isPublic(method.getModifiers()) &amp;&amp;
                method.getDeclaringClass() != Object.class &amp;&amp; !AopUtils.isEqualsMethod(method) &amp;&amp;
                !AopUtils.isHashCodeMethod(method) &amp;&amp; !AopUtils.isToStringMethod(method) ?
                methodProxy : null);
    }

    // proceed 方法会调用该方法来执行
    @Override
    protected Object invokeJoinpoint() throws Throwable {
        if (this.methodProxy != null) {
            return this.methodProxy.invoke(this.target, this.arguments);
        }
        else {
            return super.invokeJoinpoint();
        }
    }
}
</code></pre>
<p>当调用<code>proceed</code> 方法时，和 JDK 的处理是一样的，只不过当执行完所有的拦截器后，执行目标方法调用的是 CglibMethodInvocation  的 <code>invokeJoinpoint</code>来执行而已；</p>
<p>因为 CglibMethodInvocation 继承于 ReflectiveMethodInvocation ，而 JDK 使用的就是 ReflectiveMethodInvocation 来执行的，ReflectiveMethodInvocation 的 invokeJoinpoint 方法为 ： method.invoke(target, args)。</p>
<h3 id="8-mybatis一级缓存是如何实现的sqlsession级别">8. Mybatis一级缓存是如何实现的？SqlSession级别</h3>
<p>一级缓存是 SqlSession 级别的缓存。在操作数据库时需要构造 SqlSession 对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的是 SqlSession 之间的缓存数据区（HashMap）是互相不影响。</p>
<figure data-type="image" tabindex="5"><img src="https://q456qq520.github.io/post-images/1676176678813.png" alt="" loading="lazy"></figure>
<p>SqlSession 是一个接口，提供了一些 CRUD 的方法，而 SqlSession 的默认实现类是 DefaultSqlSession，DefaultSqlSession 类持有 Executor 接口对象，而 Executor 的默认实现是 BaseExecutor 对象，每个 BaseExecutor 对象都有一个 PerpetualCache 缓存，也就是上图的  Local Cache。<br>
当用户发起查询时，MyBatis 根据当前执行的语句生成 MappedStatement，在 Local Cache 进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入 Local Cache，最后返回结果给用户。一级缓存的底层数据结构就是 HashMap，每一个 SqlSession 都会存放一个 map 对象的引用。每次执行 update 前都会清空 localCache。</p>
<p>二级缓存是 Mapper 级别的缓存，多个 SqlSession 去操作同一个 Mapper 的 sql 语句，多个 SqlSession 可以共用二级缓存，二级缓存是跨 SqlSession 的。<br>
<img src="https://q456qq520.github.io/post-images/1676176896433.png" alt="" loading="lazy"><br>
MyBatis 的二级缓存相对于一级缓存来说，实现了 SqlSession 之间缓存数据的共享，同时粒度更加的细，能够到 namespace 级别，通过 Cache 接口实现类不同的组合，对 Cache 的可控性也更强。<br>
MyBatis 在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。<br>
在分布式环境下，由于默认的 MyBatis Cache 实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将 MyBatis 的 Cache 接口实现，有一定的开发成本，直接使用 Redis、Memcached 等分布式缓存可能成本更低，安全性也更高。</p>
<h3 id="9-mybatis声明interface加上注解即可被service注入使用是怎么实现的">9. Mybatis声明interface加上注解即可被service注入使用，是怎么实现的</h3>
<ol>
<li>使用@MapperScan注解扫描Mapper接口，这个注解由mybatis-spring提供，配合MapperScannerRegistrar和MapperScannerConfigurer，可以实现编码方式为Mapper接口创建代理，并注册到Spring容器。</li>
<li>导入MapperScannerRegistrar类，MapperScannerRegistrar类实现了ImportBeanDefinitionRegistrar接口，在registerBeanDefinitions方法中，他将MapperScannerConfigurer类注册到了Spring容器中，而MapperScannerConfigurer类实现了BeanDefinitionRegistryPostProcessor接口，而MapperScannerConfigurer就是在这个postProcessBeanDefinitionRegistry方法中扫描所有的Mapper接口并且注册FactoryBean bean definition的。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[《从根儿上理解MySQL》读书笔记(三)]]></title>
        <id>https://q456qq520.github.io/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-san/</id>
        <link href="https://q456qq520.github.io/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-san/">
        </link>
        <updated>2023-01-13T10:14:23.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="第10章-条条大路通罗马-单表访问方法">第10章 条条大路通罗马-单表访问方法</h2>
<p>MySQL Server有一个称为查询优化器的模块，一条查询语句进行语法解析之后就会被交给查询优化器来进行优化，优化的结果就是生成一个所谓的执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是什么样的，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。不过查询优化这个主题有点儿大，在学会跑之前还得先学会走，所以本章先来看看MySQL怎么执行单表查询（就是FROM子句后边只有一个表，最简单的那种查询～）。</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="第10章-条条大路通罗马-单表访问方法">第10章 条条大路通罗马-单表访问方法</h2>
<p>MySQL Server有一个称为查询优化器的模块，一条查询语句进行语法解析之后就会被交给查询优化器来进行优化，优化的结果就是生成一个所谓的执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是什么样的，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。不过查询优化这个主题有点儿大，在学会跑之前还得先学会走，所以本章先来看看MySQL怎么执行单表查询（就是FROM子句后边只有一个表，最简单的那种查询～）。</p>
<!-- more -->
<pre><code class="language-mysql">CREATE TABLE single_table (
    id INT NOT NULL AUTO_INCREMENT,
    key1 VARCHAR(100),
    key2 INT,
    key3 VARCHAR(100),
    key_part1 VARCHAR(100),
    key_part2 VARCHAR(100),
    key_part3 VARCHAR(100),
    common_field VARCHAR(100),
    PRIMARY KEY (id),
    KEY idx_key1 (key1),
    UNIQUE KEY idx_key2 (key2),
    KEY idx_key3 (key3),
    KEY idx_key_part(key_part1, key_part2, key_part3)
) Engine=InnoDB CHARSET=utf8;
</code></pre>
<p>我们为这个single_table表建立了1个聚簇索引和4个二级索引。然后我们需要为这个表插入10000行记录。</p>
<h3 id="101-访问方法access-method的概念">10.1 访问方法（access method）的概念</h3>
<p>MySQL把查询的执行方式大致分为下面两种：</p>
<ol>
<li>
<p>使用全表扫描进行查询<br>
  这种执行方式很好理解，就是把表的每一行记录都扫一遍嘛，把符合搜索条件的记录加入到结果集就完了。不管是什么查询都可以使用这种方式执行，当然，这种也是最笨的执行方式。</p>
</li>
<li>
<p>使用索引进行查询<br>
  因为直接使用全表扫描的方式执行查询要遍历好多记录，所以代价可能太大了。如果查询语句中的搜索条件可以使用到某个索引，那直接使用索引来执行查询可能会加快查询执行的时间。使用索引来执行查询的方式五花八门，又可以细分为许多种类：</p>
<ul>
<li>针对主键或唯一二级索引的等值查询</li>
<li>针对普通二级索引的等值查询</li>
<li>针对索引列的范围查询</li>
<li>直接扫描整个索引</li>
</ul>
</li>
</ol>
<p>把MySQL执行查询语句的方式称之为<code>访问方法</code>或者<code>访问类型</code>。同一个查询语句可能可以使用多种不同的访问方法来执行，虽然最后的查询结果都是一样的，但是执行的时间可能差老远了，下面细细道来各种访问方法的具体内容。</p>
<ol>
<li>const<br>
  有的时候我们可以通过主键列来定位一条记录，比方说这个查询：</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE id = 1438;
</code></pre>
<p>MySQL会直接利用主键值在聚簇索引中定位对应的用户记录，就像这样：<br>
<img src="https://q456qq520.github.io/post-images/1673839553874.png" alt="" loading="lazy"></p>
<p>我们忽略掉了页的结构，直接把所有的叶子节点的记录都放在一起展示，而且记录中只展示我们关心的索引列，对于single_table表的聚簇索引来说，展示的就是id列。我们想突出的重点就是：B+树叶子节点中的记录是按照索引列排序的，对于的聚簇索引来说，它对应的B+树叶子节点中的记录就是按照id列排序的。所以这样根据主键值定位一条记录的速度贼快。类似的，我们根据唯一二级索引列来定位一条记录的速度也是贼快的，比如下面这个查询：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 = 3841;
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1673839657085.png" alt="" loading="lazy"></figure>
<p>可以看到这个查询的执行分两步，第一步先从idx_key2对应的B+树索引中根据key2列与常数的等值比较条件定位到一条二级索引记录，然后再根据该记录的id值到聚簇索引中获取到完整的用户记录。</p>
<p>把这种通过主键或者唯一二级索引列来定位一条记录的访问方法定义为：const，意思是常数级别的，代价是可以忽略不计的。不过这种const访问方法只能在主键列或者唯一二级索引列和一个常数进行等值比较时才有效，如果主键或者唯一二级索引是由多个列构成的话，索引中的每一个列都需要与常数进行等值比较，这个const访问方法才有效。</p>
<p>对于唯一二级索引来说，查询该列为NULL值的情况比较特殊，比如这样：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 IS NULL;
</code></pre>
<p>因为唯一二级索引列并不限制 NULL 值的数量，所以上述语句可能访问到多条记录，也就是说 上面这个语句不可以使用const访问方法来执行。</p>
<ol start="2">
<li>ref<br>
 有时候我们对某个普通的二级索引列与常数进行等值比较，比如这样：</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 = 'abc';
</code></pre>
<p>对于这个查询，我们当然可以选择全表扫描来逐一对比搜索条件是否满足要求，我们也可以先使用二级索引找到对应记录的id值，然后再回表到聚簇索引中查找完整的用户记录。由于普通二级索引并不限制索引列值的唯一性，所以可能找到多条对应的记录，也就是说使用二级索引来执行查询的代价取决于等值匹配到的二级索引记录条数。如果匹配的记录较少，则回表的代价还是比较低的，所以MySQL可能选择使用索引而不是全表扫描的方式来执行查询。设计MySQL的大佬就把这种搜索条件为二级索引列与常数等值比较，采用二级索引来执行查询的访问方法称为：<code>ref</code>。我们看一下采用ref访问方法执行查询的图示：</p>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1673841047507.png" alt="" loading="lazy"></figure>
<p>从图示中可以看出，对于普通的二级索引来说，通过索引列进行等值比较后可能匹配到多条连续的记录，而不是像主键或者唯一二级索引那样最多只能匹配1条记录，所以这种ref访问方法比const差了那么一丢丢，但是在二级索引等值比较时匹配的记录数较少时的效率还是很高的（如果匹配的二级索引记录太多那么回表的成本就太大了），不过需要注意下面两种情况：</p>
<pre><code>-  二级索引列值为NULL的情况
</code></pre>
<p>不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含NULL值的数量并不限制，所以我们采用key IS NULL这种形式的搜索条件最多只能使用ref的访问方法，而不是const的访问方法。<br>
- 对于某个包含多个索引列的二级索引来说，只要是最左边的连续索引列是与常数的等值比较就可能采用ref的访问方法，比方说下面这几个查询：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key_part1 = 'god like';

SELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 = 'legendary';

SELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 = 'legendary' AND key_part3 = 'penta kill';
</code></pre>
<p>但是如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为ref了，比方说这样：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key_part1 = 'god like' AND key_part2 &gt; 'legendary';
</code></pre>
<ol start="3">
<li>ref_or_null<br>
有时候我们不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为NULL的记录也找出来，就像下面这个查询：</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM single_demo WHERE key1 = 'abc' OR key1 IS NULL;
</code></pre>
<p>当使用二级索引而不是全表扫描的方式执行该查询时，这种类型的查询使用的访问方法就称为ref_or_null，这个ref_or_null访问方法的执行过程同上面基本类似。相当于先分别从idx_key1索引对应的B+树中找出key1 IS NULL和key1 = 'abc'的两个连续的记录范围，然后根据这些二级索引记录中的id值再回表查找完整的用户记录。</p>
<ol start="4">
<li>range</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 &gt;= 38 AND key2 &lt;= 79);
</code></pre>
<p>我们当然还可以使用全表扫描的方式来执行这个查询，不过也可以使用二级索引 + 回表的方式执行，如果采用二级索引 + 回表的方式来执行的话，那么此时的搜索条件就不只是要求索引列与常数的等值匹配了，而是索引列需要匹配某个或某些范围的值，在本查询中key2列的值只要匹配下列3个范围中的任何一个就算是匹配成功了：<br>
- key2的值是1438<br>
- key2的值是6328<br>
- key2的值在38和79之间。<br>
把这种利用索引进行范围匹配的访问方法称之为：<code>range</code>。</p>
<ol start="5">
<li>index</li>
</ol>
<pre><code class="language-mysql">SELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = 'abc';
</code></pre>
<p>由于key_part2并不是联合索引idx_key_part最左索引列，所以我们无法使用ref或者range访问方法来执行这个语句。但是这个查询符合下面这两个条件：</p>
<pre><code>- 它的查询列表只有3个列：key_part1, key_part2, key_part3，而索引idx_key_part又包含这三个列。
- 搜索条件中只有key_part2列。这个列也包含在索引idx_key_part中。
</code></pre>
<p>也就是说我们可以直接通过遍历idx_key_part索引的叶子节点的记录来比较key_part2 = 'abc'这个条件是否成立，把匹配成功的二级索引记录的key_part1, key_part2, key_part3列的值直接加到结果集中就行了。由于二级索引记录比聚簇索记录小的多（聚簇索引记录要存储所有用户定义的列以及所谓的隐藏列，而二级索引记录只需要存放索引列和主键），而且这个过程也不用进行回表操作，所以直接遍历二级索引比直接遍历聚簇索引的成本要小很多，MySQL就把这种采用遍历二级索引记录的执行方式称之为：<code>index</code>。</p>
<ol start="6">
<li>all<br>
最直接的查询执行方式就是我们已经提了无数遍的全表扫描，对于InnoDB表来说也就是直接扫描聚簇索引，MySQL把这种使用全表扫描执行查询的方式称之为：<code>all</code>。</li>
</ol>
<h3 id="102-注意事项">10.2 注意事项</h3>
<h4 id="1021-重温-二级索引-回表">10.2.1 重温 二级索引 + 回表</h4>
<p>一般情况下只能利用单个二级索引执行查询，比方说下面的这个查询：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 = 'abc' AND key2 &gt; 1000;
</code></pre>
<p>查询优化器会识别到这个查询中的两个搜索条件：<br>
- key1 = 'abc'<br>
- key2 &gt; 1000</p>
<p>优化器一般会根据single_table表的统计数据来判断到底使用哪个条件到对应的二级索引中查询扫描的行数会更少，选择那个扫描行数较少的条件到对应的二级索引中查询。然后将从该二级索引中查询到的结果经过回表得到完整的用户记录后再根据其余的WHERE条件过滤记录。</p>
<p>一般来说，等值查找比范围查找需要扫描的行数更少（也就是ref的访问方法一般比range好，但这也不总是一定的，也可能采用ref访问方法的那个索引列的值为特定值的行数特别多），所以这里假设优化器决定使用idx_key1索引进行查询，那么整个查询过程可以分为两个步骤：</p>
<pre><code>- 步骤1：使用二级索引定位记录的阶段，也就是根据条件key1 = 'abc'从idx_key1索引代表的B+树中找到对应的二级索引记录。
- 步骤2：回表阶段，也就是根据上一步骤中找到的记录的主键值进行回表操作，也就是到聚簇索引中找到对应的完整的用户记录，再根据条件key2 &gt; 1000到完整的用户记录继续过滤。将最终符合过滤条件的记录返回给用户。
</code></pre>
<p>这里需要特别提醒大家的一点是，因为二级索引的节点中的记录只包含索引列和主键，所以在步骤1中使用idx_key1索引进行查询时只会用到与key1列有关的搜索条件，其余条件，比如key2 &gt; 1000这个条件在步骤1中是用不到的，只有在步骤2完成回表操作后才能继续针对完整的用户记录中继续过滤。</p>
<h4 id="1022-明确range访问方法使用的范围区间">10.2.2 明确range访问方法使用的范围区间</h4>
<p>其实对于B+树索引来说，只要索引列和常数使用<code>=、&lt;=&gt;、IN、NOT IN、IS NULL、IS NOT NULL、&gt;、&lt;、&gt;=、&lt;=、BETWEEN、!=（不等于也可以写成&lt;&gt;）</code>或者<code>LIKE</code>操作符连接起来，就可以产生一个所谓的区间。</p>
<p>当我们想使用range访问方法来执行一个查询语句时，重点就是找出该查询可用的索引以及这些索引对应的范围区间。下面分两种情况看一下怎么从由AND或OR组成的复杂搜索条件中提取出正确的范围区间。</p>
<h5 id="所有搜索条件都可以使用某个索引的情况">所有搜索条件都可以使用某个索引的情况</h5>
<p>有时候每个搜索条件都可以使用到某个索引，比如下面这个查询语句：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 &gt; 100 AND key2 &gt; 200;
</code></pre>
<p>这个查询中的搜索条件都可以使用到key2，也就是说每个搜索条件都对应着一个idx_key2的范围区间。这两个小的搜索条件使用AND连接起来，也就是要取两个范围区间的交集，在我们使用range访问方法执行查询时，使用的idx_key2索引的范围区间的确定过程就如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1673857794078.png" alt="" loading="lazy"></p>
<p>key2 &gt; 100和key2 &gt; 200交集当然就是key2 &gt; 200了，也就是说上面这个查询使用idx_key2的范围区间就是(200, +∞)。</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 &gt; 100 OR key2 &gt; 200;
</code></pre>
<p>上面这个查询使用idx_key2的范围区间就是(100， +∞)。</p>
<h5 id="有的搜索条件无法使用索引的情况">有的搜索条件无法使用索引的情况</h5>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 &gt; 100 AND common_field = 'abc';
</code></pre>
<p>这个查询语句中能利用的索引只有idx_key2一个，而idx_key2这个二级索引的记录中又不包含common_field这个字段，所以在使用二级索引idx_key2定位记录的阶段用不到common_field = 'abc'这个条件，这个条件是在回表获取了完整的用户记录后才使用的，而范围区间是为了到索引中取记录中提出的概念，所以在确定范围区间的时候不需要考虑common_field = 'abc'这个条件，我们在为某个索引确定范围区间的时候只需要把用不到相关索引的搜索条件替换为TRUE就好了。</p>
<p>也就是说上面那个查询使用idx_key2的范围区间就是：(100, +∞)。</p>
<p>再来看一下使用OR的情况：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 &gt; 100 OR common_field = 'abc';
</code></pre>
<p>化简后：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key2 &gt; 100 OR TRUE;
SELECT * FROM single_table WHERE TRUE;
</code></pre>
<p>这也就说说明如果我们强制使用idx_key2执行查询的话，对应的范围区间就是(-∞, +∞)，也就是需要将全部二级索引的记录进行回表，这个代价肯定比直接全表扫描都大了。也就是说一个使用到索引的搜索条件和没有使用该索引的搜索条件使用OR连接起来后是无法使用该索引的。</p>
<blockquote>
<p>小贴士：之所以把用不到索引的搜索条件替换为TRUE，是因为我们不打算使用这些条件进行在该索引上进行过滤，所以不管索引的记录满不满足这些条件，我们都把它们选取出来，待到之后回表的时候再使用它们过滤。</p>
</blockquote>
<h5 id="复杂搜索条件下找出范围匹配的区间">复杂搜索条件下找出范围匹配的区间</h5>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE 
        (key1 &gt; 'xyz' AND key2 = 748 ) OR
        (key1 &lt; 'abc' AND key1 &gt; 'lmn') OR
        (key1 LIKE '%suf' AND key1 &gt; 'zzz' AND (key2 &lt; 8000 OR common_field = 'abc')) ;
</code></pre>
<ol>
<li>
<p>首先查看WHERE子句中的搜索条件都涉及到了哪些列，哪些列可能使用到索引。<br>
  这个查询的搜索条件涉及到了key1、key2、common_field这3个列，然后key1列有普通的二级索引idx_key1，key2列有唯一二级索引idx_key2。</p>
</li>
<li>
<p>对于那些可能用到的索引，分析它们的范围区间。<br>
假设我们使用idx_key1执行查询，我们需要把那些用不到该索引的搜索条件暂时移除掉，移除方法也简单，直接把它们替换为TRUE就好了。上面的查询中除了有关key2和common_field列不能使用到idx_key1索引外，key1 LIKE '%suf'也使用不到索引，所以把这些搜索条件替换为TRUE之后的样子就是这样：</p>
</li>
</ol>
<pre><code class="language-mysql">(key1 &gt; 'xyz' AND TRUE ) OR
(key1 &lt; 'abc' AND key1 &gt; 'lmn') OR
(TRUE AND key1 &gt; 'zzz' AND (TRUE OR TRUE))

(key1 &gt; 'xyz') OR
(key1 &lt; 'abc' AND key1 &gt; 'lmn') OR
(key1 &gt; 'zzz')

# 替换掉永远为TRUE或FALSE的条件,因为符合key1 &lt; 'abc' AND key1 &gt; 'lmn'永远为FALSE
(key1 &gt; 'xyz') OR (key1 &gt; 'zzz')
</code></pre>
<p>key1 &gt; 'xyz'和key1 &gt; 'zzz'之间使用OR操作符连接起来的，意味着要取并集，所以最终的结果化简的到的区间就是：key1 &gt; xyz。也就是说：上面那个有一坨搜索条件的查询语句如果使用 idx_key1 索引执行查询的话，需要把满足key1 &gt; xyz的二级索引记录都取出来，然后拿着这些记录的id再进行回表，得到完整的用户记录之后再使用其他的搜索条件进行过滤。</p>
<p>假设我们使用idx_key2执行查询同理，我们需要把那些用不到该索引的搜索条件暂时使用TRUE条件替换掉，其中有关key1和common_field的搜索条件都需要被替换掉。</p>
<h3 id="103-索引合并">10.3 索引合并</h3>
<p>MySQL在一般情况下执行一个查询时最多只会用到单个二级索引，但不是还有特殊情况么，在这些特殊情况下也可能在一个查询中使用到多个二级索引，设计MySQL的大佬把这种使用到多个索引来完成一次查询的执行方法称之为：<code>index merge</code>，具体的索引合并算法有下面三种。</p>
<h4 id="1031-intersection合并">10.3.1 Intersection合并</h4>
<p>Intersection翻译过来的意思是交集。这里是说某个查询可以使用多个二级索引，将从多个二级索引中查询到的结果取交集，比方说下面这个查询：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b';
</code></pre>
<p>假设这个查询使用Intersection合并的方式执行的话，那这个过程就是这样的：</p>
<ol>
<li>从idx_key1二级索引对应的B+树中取出key1 = 'a'的相关记录。</li>
<li>从idx_key3二级索引对应的B+树中取出key3 = 'b'的相关记录。</li>
<li>二级索引的记录都是由索引列 + 主键构成的，所以我们可以计算出这两个结果集中id值的交集。</li>
<li>按照上一步生成的id值列表进行回表操作，也就是从聚簇索引中把指定id值的完整用户记录取出来，返回给用户</li>
</ol>
<blockquote>
<p>虽然读取多个二级索引比读取一个二级索引消耗性能，但是读取二级索引的操作是顺序I/O，而回表操作是随机I/O，所以如果只读取一个二级索引时需要回表的记录数特别多，而读取多个二级索引之后取交集的记录数非常少，当节省的因为回表而造成的性能损耗比访问多个二级索引带来的性能损耗更高时，读取多个二级索引后取交集比只读取一个二级索引的成本更低。</p>
</blockquote>
<p>MySQL在某些特定的情况下才可能会使用到Intersection索引合并：<br>
- 情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只匹配部分列的情况。<br>
- 情况二：主键列可以是范围匹配</p>
<p>对于InnoDB的二级索引来说，记录先是按照索引列进行排序，如果该二级索引是一个联合索引，那么会按照联合索引中的各个列依次排序。而二级索引的用户记录是由索引列 + 主键构成的，二级索引列的值相同的记录可能会有好多条，这些索引列的值相同的记录又是按照主键的值进行排序的。所以重点来了，之所以在二级索引列都是等值匹配的情况下才可能使用Intersection索引合并，是因为<mark>只有在这种情况下根据二级索引查询出的结果集是按照主键值排序的。</mark></p>
<p>另外，不仅是多个二级索引之间可以采用Intersection索引合并，索引合并也可以有聚簇索引参加，也就是我们上面写的情况二：在搜索条件中有主键的范围匹配的情况下也可以使用Intersection索引合并索引合并。为什么主键这就可以范围匹配了？还是得回到应用场景里，比如看下面这个查询：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 = 'a' AND id &gt; 100;
</code></pre>
<p>假设这个查询可以采用Intersection索引合并，我们理所当然的以为这个查询会分别按照id &gt; 100这个条件从聚簇索引中获取一些记录，在通过key1 = 'a'这个条件从idx_key1二级索引中获取一些记录，然后再求交集，其实这样就把问题复杂化了，没必要从聚簇索引中获取一次记录。别忘了二级索引的记录中都带有主键值的，所以可以在从idx_key1中获取到的主键值上直接运用条件id &gt; 100过滤就行了，这样多简单。所以涉及主键的搜索条件只不过是为了从别的二级索引得到的结果集中过滤记录罢了，是不是等值匹配不重要。</p>
<p>当然，上面说的情况一和情况二只是发生Intersection索引合并的必要条件，不是充分条件。也就是说即使情况一、情况二成立，也不一定发生Intersection索引合并，这得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，而通过Intersection索引合并后需要回表的记录数大大减少时才会使用Intersection索引合并。</p>
<h4 id="1032-union合并">10.3.2 Union合并</h4>
<p>我们在写查询语句时经常想把既符合某个搜索条件的记录取出来，也把符合另外的某个搜索条件的记录取出来，我们说这些不同的搜索条件之间是OR关系。有时候OR关系的不同搜索条件会使用到不同的索引，比方说这样：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 = 'a' OR key3 = 'b'
</code></pre>
<p>Intersection是交集的意思，这适用于使用不同索引的搜索条件之间使用AND连接起来的情况；Union是并集的意思，适用于使用不同索引的搜索条件之间使用OR连接起来的情况。与Intersection索引合并类似，MySQL在某些特定的情况下才可能会使用到Union索引合并：</p>
<ol>
<li>情况一：二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只出现匹配部分列的情况。<br>
  比方说下面这个查询可能用到idx_key1和idx_key_part这两个二级索引进行Union索引合并的操作：</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 = 'a' OR ( key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c');
</code></pre>
<p>而下面这两个查询就不能进行Union索引合并：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 &gt; 'a' OR (key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c');

SELECT * FROM single_table WHERE key1 = 'a' OR key_part1 = 'a';
</code></pre>
<p>第一个查询是因为对key1进行了范围匹配，第二个查询是因为联合索引idx_key_part中的key_part2列并没有出现在搜索条件中，所以这两个查询不能进行Union索引合并。</p>
<ol start="2">
<li>
<p>情况二：主键列可以是范围匹配</p>
</li>
<li>
<p>情况三：使用Intersection索引合并的搜索条件<br>
  这种情况其实也挺好理解，就是搜索条件的某些部分使用Intersection索引合并的方式得到的主键集合和其他方式得到的主键集合取交集，比方说这个查询：</p>
</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c' OR (key1 = 'a' AND key3 = 'b');
</code></pre>
<p>优化器可能采用这样的方式来执行这个查询：</p>
<ol>
<li>先按照搜索条件key1 = 'a' AND key3 = 'b'从索引idx_key1和idx_key3中使用Intersection索引合并的方式得到一个主键集合。</li>
<li>再按照搜索条件key_part1 = 'a' AND key_part2 = 'b' AND key_part3 = 'c'从联合索引idx_key_part中得到另一个主键集合。</li>
<li>采用Union索引合并的方式把上述两个主键集合取并集，然后进行回表操作，将结果返回给用户。</li>
</ol>
<p>当然，查询条件符合了这些情况也不一定就会采用Union索引合并，也得看优化器的心情。优化器只有在单独根据搜索条件从某个二级索引中获取的记录数比较少，通过Union索引合并后进行访问的代价比全表扫描更小时才会使用Union索引合并。</p>
<h4 id="1033-sort-union合并">10.3.3 Sort-Union合并</h4>
<p>Union索引合并的使用条件太苛刻，必须保证各个二级索引列在进行等值匹配的条件下才可能被用到，比方说下面这个查询就无法使用到Union索引合并：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 &lt; 'a' OR key3 &gt; 'z'
</code></pre>
<p>这是因为根据key1 &lt; 'a'从idx_key1索引中获取的二级索引记录的主键值不是排好序的，根据key3 &gt; 'z'从idx_key3索引中获取的二级索引记录的主键值也不是排好序的，但是key1 &lt; 'a'和key3 &gt; 'z'这两个条件又特别让我们动心，所以我们可以这样：</p>
<ol>
<li>先根据key1 &lt; 'a'条件从idx_key1二级索引总获取记录，并按照记录的主键值进行排序</li>
<li>再根据key3 &gt; 'z'条件从idx_key3二级索引总获取记录，并按照记录的主键值进行排序</li>
<li>因为上述的两个二级索引主键值都是排好序的，剩下的操作和Union索引合并方式就一样了。</li>
</ol>
<p>我们把上述这种先按照二级索引记录的主键值进行排序，之后按照Union索引合并方式执行的方式称之为<code>Sort-Union索引合并</code>，很显然，这种Sort-Union索引合并比单纯的Union索引合并多了一步对二级索引记录的主键值排序的过程。</p>
<blockquote>
<p>小贴士：为什么有Sort-Union索引合并，就没有Sort-Intersection索引合并么？是的，的确没有Sort-Intersection索引合并这么一说，Sort-Union的适用场景是单独根据搜索条件从某个二级索引中获取的记录数比较少，这样即使对这些二级索引记录按照主键值进行排序的成本也不会太高，而Intersection索引合并的适用场景是单独根据搜索条件从某个二级索引中获取的记录数太多，导致回表开销太大，合并后可以明显降低回表开销，但是如果加入Sort-Intersection后，就需要为大量的二级索引记录按照主键值进行排序，这个成本可能比回表查询都高了，所以也就没有引入Sort-Intersection这个玩意儿。</p>
</blockquote>
<h4 id="1034-索引合并注意事项">10.3.4 索引合并注意事项</h4>
<h5 id="联合索引替代intersection索引合并">联合索引替代Intersection索引合并</h5>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b';
</code></pre>
<p>这个查询之所以可能使用Intersection索引合并的方式执行，还不是因为idx_key1和idx_key3是两个单独的B+树索引，你要是把这两个列搞一个联合索引，那直接使用这个联合索引就把事情搞定了，何必用什么索引合并呢。</p>
<h2 id="第十一章-两个表的亲密接触-连接的原理">第十一章 两个表的亲密接触-连接的原理</h2>
<h3 id="111-连接简介">11.1 连接简介</h3>
<h4 id="1111-连接的本质">11.1.1 连接的本质</h4>
<pre><code class="language-mysql">mysql&gt; CREATE TABLE t1 (m1 int, n1 char(1));
Query OK, 0 rows affected (0.02 sec)

mysql&gt; CREATE TABLE t2 (m2 int, n2 char(1));
Query OK, 0 rows affected (0.02 sec)

mysql&gt; INSERT INTO t1 VALUES(1, 'a'), (2, 'b'), (3, 'c');
Query OK, 3 rows affected (0.00 sec)
Records: 3  Duplicates: 0  Warnings: 0

mysql&gt; INSERT INTO t2 VALUES(2, 'b'), (3, 'c'), (4, 'd');
Query OK, 3 rows affected (0.00 sec)
Records: 3  Duplicates: 0  Warnings: 0
</code></pre>
<p>我们成功建立了t1、t2两个表，这两个表都有两个列，一个是INT类型的，一个是CHAR(1)类型的，填充好数据的两个表长这样：</p>
<pre><code class="language-mysql">mysql&gt; SELECT * FROM t1;
+------+------+
| m1   | n1   |
+------+------+
|    1 | a    |
|    2 | b    |
|    3 | c    |
+------+------+
3 rows in set (0.00 sec)

mysql&gt; SELECT * FROM t2;
+------+------+
| m2   | n2   |
+------+------+
|    2 | b    |
|    3 | c    |
|    4 | d    |
+------+------+
3 rows in set (0.00 sec)
</code></pre>
<p>连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。所以我们把t1和t2两个表连接起来的过程如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1673920033388.png" alt="" loading="lazy"><br>
这个过程看起来就是把t1表的记录和t2的记录连起来组成新的更大的记录，所以这个查询过程称之为连接查询。连接查询的结果集中包含一个表中的每一条记录与另一个表中的每一条记录相互匹配的组合，像这样的结果集就可以称之为<code>笛卡尔积</code>。因为表t1中有3条记录，表t2中也有3条记录，所以这两个表连接之后的笛卡尔积就有3×3=9行记录。</p>
<h4 id="1112-连接过程简介">11.1.2 连接过程简介</h4>
<p>我们可以连接任意数量张表，但是如果没有任何限制条件的话，这些表连接起来产生的笛卡尔积可能是非常巨大的。比方说3个100行记录的表连接起来产生的笛卡尔积就有100×100×100=1000000行数据！所以在连接的时候过滤掉特定记录组合是有必要的，在连接查询中的过滤条件可以分成两种：</p>
<ol>
<li>涉及单表的条件<br>
  这种只设计单表的过滤条件我们之前都提到过一万遍了，我们之前也一直称为搜索条件，比如t1.m1 &gt; 1是只针对t1表的过滤条件，t2.n2 &lt; 'd'是只针对t2表的过滤条件。</li>
<li>涉及两表的条件<br>
  这种过滤条件我们之前没见过，比如t1.m1 = t2.m2、t1.n1 &gt; t2.n2等，这些条件中涉及到了两个表。</li>
</ol>
<p>下面我们就要看一下携带过滤条件的连接查询的大致执行过程了，比方说下面这个查询语句：</p>
<pre><code class="language-mysql">SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; 'd';
</code></pre>
<p>那么这个连接查询的大致执行过程如下：</p>
<ol>
<li>首先确定第一个需要查询的表，这个表称之为<code>驱动表</code>。只需要选取代价最小的那种访问方法去执行单表查询语句就好了（就是说从const、ref、ref_or_null、range、index、all这些执行方法中选取代价最小的去执行查询）。<br>
  此处假设使用t1作为驱动表，那么就需要到t1表中找满足t1.m1  &gt; 1的记录，因为表中的数据太少，我们也没在表上建立二级索引，所以此处查询t1表的访问方法就设定为all吧，也就是采用全表扫描的方式执行单表查询。</li>
<li>针对上一步骤中从驱动表产生的结果集中的每一条记录，分别需要到t2表中查找匹配的记录，所谓匹配的记录，指的是符合过滤条件的记录。因为是根据t1表中的记录去找t2表中的记录，所以t2表也可以被称之为被驱动表。上一步骤从驱动表中得到了2条记录，所以需要查询2次t2表。此时涉及两个表的列的过滤条件t1.m1 = t2.m2就派上用场了：<br>
  - 当t1.m1 = 2时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 2，所以此时t2表相当于有了t2.m2 = 2、t2.n2 &lt; 'd'这两个过滤条件，然后到t2表中执行单表查询。<br>
  - 当t1.m1 = 3时，过滤条件t1.m1 = t2.m2就相当于t2.m2 = 3，所以此时t2表相当于有了t2.m2 = 3、t2.n2 &lt; 'd'这两个过滤条件，然后到t2表中执行单表查询。</li>
</ol>
<p>所以整个连接查询的执行过程就如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1673920484438.png" alt="" loading="lazy"></p>
<blockquote>
<p>从上面两个步骤可以看出来，我们上面介绍的这个两表连接查询共需要查询1次<code>t1</code>表，2次<code>t2</code>表。当然这是在特定的过滤条件下的结果，如果我们把<code>t1.m1 &gt; 1</code>这个条件去掉，那么从<code>t1</code>表中查出的记录就有3条，就需要查询3次<code>t2</code>表了。也就是说在两表连接查询中，驱动表只需要访问一次，被驱动表可能被访问多次。</p>
</blockquote>
<h3 id="112-内连接和外连接">11.2 内连接和外连接</h3>
<pre><code class="language-mysql">CREATE TABLE student (
    number INT NOT NULL AUTO_INCREMENT COMMENT '学号',
    name VARCHAR(5) COMMENT '姓名',
    major VARCHAR(30) COMMENT '专业',
    PRIMARY KEY (number)
) Engine=InnoDB CHARSET=utf8 COMMENT '学生信息表';

CREATE TABLE score (
    number INT COMMENT '学号',
    subject VARCHAR(30) COMMENT '科目',
    score TINYINT COMMENT '成绩',
    PRIMARY KEY (number, score)
) Engine=InnoDB CHARSET=utf8 COMMENT '学生成绩表';
</code></pre>
<p>我们新建了一个学生信息表，一个学生成绩表，然后我们向上述两个表中插入一些数据:</p>
<pre><code class="language-mysql">mysql&gt; SELECT * FROM student;
+----------+-----------+--------------------------+
| number   | name      | major                    |
+----------+-----------+--------------------------+
| 20180101 | 杜子腾    | 软件学院                 |
| 20180102 | 范统      | 计算机科学与工程         |
| 20180103 | 史珍香    | 计算机科学与工程         |
+----------+-----------+--------------------------+
3 rows in set (0.00 sec)

mysql&gt; SELECT * FROM score;
+----------+-----------------------------+-------+
| number   | subject                     | score |
+----------+-----------------------------+-------+
| 20180101 | 母猪的产后护理              |    78 |
| 20180101 | 论萨达姆的战争准备          |    88 |
| 20180102 | 论萨达姆的战争准备          |    98 |
| 20180102 | 母猪的产后护理              |   100 |
+----------+-----------------------------+-------+
4 rows in set (0.00 sec)
</code></pre>
<p>现在我们想把每个学生的考试成绩都查询出来就需要进行两表连接了（因为score中没有姓名信息，所以不能单纯只查询score表）。连接过程就是从student表中取出记录，在score表中查找number相同的成绩记录，所以过滤条件就是student.number = socre.number，整个查询语句就是这样：</p>
<pre><code class="language-mysql">mysql&gt; SELECT s1.number, s1.name, s2.subject, s2.score FROM student AS s1, score AS s2 WHERE s1.number = s2.number;
+----------+-----------+-----------------------------+-------+
| number   | name      | subject                     | score |
+----------+-----------+-----------------------------+-------+
| 20180101 | 杜子腾    | 母猪的产后护理              |    78 |
| 20180101 | 杜子腾    | 论萨达姆的战争准备          |    88 |
| 20180102 | 范统      | 论萨达姆的战争准备          |    98 |
| 20180102 | 范统      | 母猪的产后护理              |   100 |
+----------+-----------+-----------------------------+-------+
4 rows in set (0.00 sec)
</code></pre>
<p>从上述查询结果中我们可以看到，各个同学对应的各科成绩就都被查出来了，可是有个问题，史珍香同学，也就是学号为20180103的同学因为某些原因没有参加考试，所以在score表中没有对应的成绩记录。那如果老师想查看所有同学的考试成绩，即使是缺考的同学也应该展示出来，但是到目前为止我们介绍的连接查询是无法完成这样的需求的。</p>
<p>我们稍微思考一下这个需求，其本质是想：<mark>驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集</mark>。为了解决这个问题，就有了<code>内连接</code>和<code>外连接</code>的概念：</p>
<ol>
<li>对于内连接的两个表，驱动表中的记录在被驱动表中找不到匹配的记录，该记录不会加入到最后的结果集，我们上面提到的连接都是所谓的内连接。</li>
<li>对于外连接的两个表，驱动表中的记录即使在被驱动表中没有匹配的记录，也仍然需要加入到结果集。在MySQL中，根据选取驱动表的不同，外连接仍然可以细分为2种：
<ol>
<li>左外连接：选取左侧的表为驱动表。</li>
<li>右外连接：选取右侧的表为驱动表。</li>
</ol>
</li>
</ol>
<p>放在不同地方的过滤条件是有不同语义的：</p>
<ol>
<li>WHERE子句中的过滤条件就是我们平时见的那种，不论是内连接还是外连接，凡是不符合WHERE子句中的过滤条件的记录都不会被加入最后的结果集。</li>
<li>ON子句中的过滤条件
<ul>
<li>对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表记录的各个字段使用NULL值填充。</li>
<li>需要注意的是，这个ON子句是专门为外连接驱动表中的记录在被驱动表找不到匹配记录时应不应该把该记录加入结果集这个场景下提出的，所以如果把ON子句放到内连接中，MySQL会把它和WHERE子句一样对待，也就是说：<mark>内连接中的WHERE子句和ON子句是等价的</mark>。</li>
<li>一般情况下，我们都把只涉及单表的过滤条件放到WHERE子句中，把涉及两表的过滤条件都放到ON子句中，我们也一般把放到ON子句中的过滤条件也称之为<code>连接条件</code>。</li>
</ul>
</li>
</ol>
<h4 id="1121-左外连接的语法">11.2.1 左（外）连接的语法</h4>
<pre><code class="language-mysql">SELECT * FROM t1 LEFT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件];
</code></pre>
<p>其中，中括号里的OUTER单词是可以省略的。对于LEFT JOIN类型的连接来说，我们把放在左边的表称之为外表或者驱动表，右边的表称之为内表或者被驱动表。</p>
<p>需要注意的是，对于左（外）连接和右（外）连接来说，必须使用ON子句来指出连接条件。</p>
<p>再次回到我们上面那个现实问题中来，看看怎样写查询语句才能把所有的学生的成绩信息都查询出来，即使是缺考的考生也应该被放到结果集中：</p>
<pre><code class="language-mysql">mysql&gt; SELECT s1.number, s1.name, s2.subject, s2.score FROM student AS s1 LEFT JOIN score AS s2 ON s1.number = s2.number;
+----------+-----------+-----------------------------+-------+
| number   | name      | subject                     | score |
+----------+-----------+-----------------------------+-------+
| 20180101 | 杜子腾    | 母猪的产后护理              |    78 |
| 20180101 | 杜子腾    | 论萨达姆的战争准备          |    88 |
| 20180102 | 范统      | 论萨达姆的战争准备          |    98 |
| 20180102 | 范统      | 母猪的产后护理              |   100 |
| 20180103 | 史珍香    | NULL                        |  NULL |
+----------+-----------+-----------------------------+-------+
5 rows in set (0.04 sec)
</code></pre>
<h4 id="1122-右外连接的语法">11.2.2 右（外）连接的语法</h4>
<p>右（外）连接和左（外）连接的原理是一样一样的，语法也只是把LEFT换成RIGHT而已：</p>
<pre><code class="language-mysql">SELECT * FROM t1 RIGHT [OUTER] JOIN t2 ON 连接条件 [WHERE 普通过滤条件];
</code></pre>
<h4 id="1123-内连接的语法">11.2.3 内连接的语法</h4>
<p><mark>内连接和外连接的根本区别就是在驱动表中的记录不符合ON子句中的连接条件时不会把该记录加入到最后的结果集</mark>，我们最开始介绍的那些连接查询的类型都是内连接。不过之前仅仅提到了一种最简单的内连接语法，就是直接把需要连接的多个表都放到FROM子句后边。其实针对内连接，MySQL提供了好多不同的语法，我们以t1和t2表为例看看：</p>
<pre><code class="language-mysql">SELECT * FROM t1 [INNER | CROSS] JOIN t2 [ON 连接条件] [WHERE 普通过滤条件];
</code></pre>
<p>也就是说在MySQL中，下面这几种内连接的写法都是等价的：</p>
<pre><code class="language-mysql">SELECT * FROM t1 JOIN t2;
SELECT * FROM t1 INNER JOIN t2;
SELECT * FROM t1 CROSS JOIN t2;
SELECT * FROM t1, t2;
</code></pre>
<p><mark>由于在内连接中ON子句和WHERE子句是等价的，所以内连接中不要求强制写明ON子句。</mark></p>
<p>我们前面说过，连接的本质就是把各个连接表中的记录都取出来依次匹配的组合加入结果集并返回给用户。不论哪个表作为驱动表，两表连接产生的笛卡尔积肯定是一样的。</p>
<p>而对于内连接来说，由于凡是不符合ON子句或WHERE子句中的条件的记录都会被过滤掉，其实也就相当于从两表连接的笛卡尔积中把不符合过滤条件的记录给踢出去，所以<mark>对于内连接来说，驱动表和被驱动表是可以互换的，并不会影响最后的查询结果</mark>。但是对于外连接来说，由于驱动表中的记录即使在被驱动表中找不到符合ON子句连接条件的记录，所以此时驱动表和被驱动表的关系就很重要了，也就是说<mark>左外连接和右外连接的驱动表和被驱动表不能轻易互换。</mark></p>
<h4 id="1124-小结">11.2.4 小结</h4>
<pre><code class="language-mysql">mysql&gt; SELECT * FROM t1 INNER JOIN t2 ON t1.m1 = t2.m2;
+------+------+------+------+
| m1   | n1   | m2   | n2   |
+------+------+------+------+
|    2 | b    |    2 | b    |
|    3 | c    |    3 | c    |
+------+------+------+------+
2 rows in set (0.00 sec)

mysql&gt; SELECT * FROM t1 LEFT JOIN t2 ON t1.m1 = t2.m2;
+------+------+------+------+
| m1   | n1   | m2   | n2   |
+------+------+------+------+
|    2 | b    |    2 | b    |
|    3 | c    |    3 | c    |
|    1 | a    | NULL | NULL |
+------+------+------+------+
3 rows in set (0.00 sec)

mysql&gt; SELECT * FROM t1 RIGHT JOIN t2 ON t1.m1 = t2.m2;
+------+------+------+------+
| m1   | n1   | m2   | n2   |
+------+------+------+------+
|    2 | b    |    2 | b    |
|    3 | c    |    3 | c    |
| NULL | NULL |    4 | d    |
+------+------+------+------+
3 rows in set (0.00 sec)
</code></pre>
<h3 id="113-连接的原理">11.3 连接的原理</h3>
<h4 id="1131-嵌套循环连接nested-loop-join">11.3.1 嵌套循环连接（Nested-Loop Join）</h4>
<p>我们上面已经大致介绍过t1表和t2表执行内连接查询的大致过程，我们温习一下：</p>
<ol>
<li>步骤1：选取驱动表，使用与驱动表相关的过滤条件，选取代价最低的单表访问方法来执行对驱动表的单表查询。</li>
<li>步骤2：对上一步骤中查询驱动表得到的结果集中每一条记录，都分别到被驱动表中查找匹配的记录。</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1673923077168.png" alt="" loading="lazy"></figure>
<p>如果有3个表进行连接的话，那么步骤2中得到的结果集就像是新的驱动表，然后第三个表就成为了被驱动表，重复上面过程，也就是步骤2中得到的结果集中的每一条记录都需要到t3表中找一找有没有匹配的记录，用伪代码表示一下这个过程就是这样：</p>
<pre><code class="language-mysql">for each row in t1 {   #此处表示遍历满足对t1单表查询结果集中的每一条记录
    
    for each row in t2 {   #此处表示对于某条t1表的记录来说，遍历满足对t2单表查询结果集中的每一条记录
    
        for each row in t3 {   #此处表示对于某条t1和t2表的记录组合来说，对t3表进行单表查询
            if row satisfies join conditions, send to client
        }
    }
}
</code></pre>
<p>这个过程就像是一个嵌套的循环，所以这种<mark>驱动表只访问一次，但被驱动表却可能被多次访问，访问次数取决于对驱动表执行单表查询后的结果集中的记录条数</mark>的连接执行方式称之为<code>嵌套循环连接（Nested-Loop Join）</code>，这是最简单，也是最笨拙的一种连接查询算法。</p>
<h4 id="1132-使用索引加快连接速度">11.3.2 使用索引加快连接速度</h4>
<p>我们知道在嵌套循环连接的步骤2中可能需要访问多次被驱动表，如果访问被驱动表的方式都是全表扫描的话，那得要扫描好多次。但是别忘了，查询t2表其实就相当于一次单表扫描，我们可以利用索引来加快查询速度。</p>
<pre><code class="language-mysql">SELECT * FROM t1, t2 WHERE t1.m1 &gt; 1 AND t1.m1 = t2.m2 AND t2.n2 &lt; 'd';
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://q456qq520.github.io/post-images/1673920484438.png" alt="" loading="lazy"></figure>
<p>查询驱动表t1后的结果集中有两条记录，嵌套循环连接算法需要对被驱动表查询2次：</p>
<ol>
<li>当t1.m1 = 2时，去查询一遍t2表，对t2表的查询语句相当于：</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM t2 WHERE t2.m2 = 2 AND t2.n2 &lt; 'd';
</code></pre>
<ol start="2">
<li>当t1.m1 = 3时，再去查询一遍t2表，此时对t2表的查询语句相当于：</li>
</ol>
<pre><code class="language-mysql">SELECT * FROM t2 WHERE t2.m2 = 3 AND t2.n2 &lt; 'd';
</code></pre>
<p>可以看到，原来的t1.m1 = t2.m2这个涉及两个表的过滤条件在针对t2表做查询时关于t1表的条件就已经确定了，所以我们只需要单单优化对t2表的查询了，上述两个对t2表的查询语句中利用到的列是m2和n2列，我们可以：</p>
<ol>
<li>在m2列上建立索引，因为对m2列的条件是等值查找，比如t2.m2 = 2、t2.m2 = 3等，所以可能使用到ref的访问方法，假设使用ref的访问方法去执行对t2表的查询的话，需要回表之后再判断t2.n2 &lt; d这个条件是否成立。<br>
这里有一个比较特殊的情况，就是假设m2列是t2表的主键或者唯一二级索引列，那么使用t2.m2 = 常数值这样的条件从t2表中查找记录的过程的代价就是常数级别的。我们知道在单表中使用主键值或者唯一二级索引列的值进行等值查找的方式称之为const，而MySQL把在连接查询中对被驱动表使用主键值或者唯一二级索引列的值进行等值查找的查询执行方式称之为：<code>eq_ref</code>。</li>
<li>在n2列上建立索引，涉及到的条件是t2.n2 &lt; 'd'，可能用到range的访问方法，假设使用range的访问方法对t2表的查询的话，需要回表之后再判断在m2列上的条件是否成立。</li>
</ol>
<p>假设m2和n2列上都存在索引的话，那么就需要从这两个里边儿挑一个代价更低的去执行对t2表的查询。当然，建立了索引不一定使用索引，只有在二级索引 + 回表的代价比全表扫描的代价更低时才会使用索引。</p>
<p>另外，有时候连接查询的查询列表和过滤条件中可能只涉及被驱动表的部分列，而这些列都是某个索引的一部分，这种情况下即使不能使用<code>eq_ref</code>、<code>ref</code>、<code>ref_or_null</code>或者<code>range</code>这些访问方法执行对被驱动表的查询的话，也可以使用索引扫描，也就是<code>index</code>的访问方法来查询被驱动表。所以我们建议在真实工作中最好不要使用*作为查询列表，最好把真实用到的列作为查询列表。</p>
<h4 id="1133-基于块的嵌套循环连接block-nested-loop-join">11.3.3 基于块的嵌套循环连接（Block Nested-Loop Join）</h4>
<p>现实生活中的表可不像t1、t2这种只有3条记录，成千上万条记录都是少的，几百万、几千万甚至几亿条记录的表到处都是。内存里可能并不能完全存放的下表中所有的记录，所以在扫描表前面记录的时候后边的记录可能还在磁盘上，等扫描到后边记录的时候可能内存不足，所以需要把前面的记录从内存中释放掉。我们前面又说过，采用<code>嵌套循环连接</code>算法的两表连接过程中，被驱动表可是要被访问好多次的，如果这个被驱动表中的数据特别多而且不能使用索引进行访问，那就相当于要从磁盘上读好几次这个表，这个I/O代价就非常大了，所以我们得想办法：<mark>尽量减少访问被驱动表的次数</mark>。</p>
<p>当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录只会和驱动表结果集的一条记录做匹配，之后就会被从内存中清除掉。然后再从驱动表结果集中拿出另一条记录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从磁盘上加载到内存中多少次。</p>
<p>所以我们可不可以在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载被驱动表的代价了。所以MySQL提出了一个<code>join buffer</code>的概念，join buffer就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的I/O代价。使用join buffer的过程如下图所示：</p>
<figure data-type="image" tabindex="5"><img src="https://q456qq520.github.io/post-images/1673923811333.png" alt="" loading="lazy"></figure>
<p>最好的情况是join buffer足够大，能容纳驱动表结果集中的所有记录，这样只需要访问一次被驱动表就可以完成连接操作了。设计MySQL的大佬把这种加入了join buffer的嵌套循环连接算法称之为<code>基于块的嵌套连接（Block Nested-Loop Join）算法</code>。</p>
<p>这个join buffer的大小是可以通过启动参数或者系统变量<code>join_buffer_size</code>进行配置，默认大小为262144字节（也就是256KB），最小可以设置为128字节。当然，对于优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，如果实在不能使用索引，并且自己的机器的内存也比较大可以尝试调大join_buffer_size的值来对连接查询进行优化。</p>
<p>另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer中，只有查询列表中的列和过滤条件中的列才会被放到join buffer中，所以再次提醒我们，最好不要把*作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在join buffer中放置更多的记录呢。</p>
<h2 id="第十二章-谁最便宜就选谁-mysql基于成本的优化">第十二章 谁最便宜就选谁-MySQL基于成本的优化</h2>
<h3 id="121-什么是成本">12.1 什么是成本</h3>
<p>MySQL中一条查询语句的执行成本是由下面这两个方面组成的：</p>
<ol>
<li>I/O成本<br>
  我们的表经常使用的MyISAM、InnoDB存储引擎都是将数据和索引都存储到磁盘上的，当我们想查询表中的记录时，需要先把数据或者索引加载到内存中然后再操作。这个从磁盘到内存这个加载的过程损耗的时间称之为I/O成本。</li>
<li>CPU成本<br>
  读取以及检测记录是否满足对应的搜索条件、对结果集进行排序等这些操作损耗的时间称之为CPU成本。</li>
</ol>
<p>对于InnoDB存储引擎来说，页是磁盘和内存之间交互的基本单位，设计MySQL的大佬规定读取一个页面花费的成本默认是1.0，读取以及检测一条记录是否符合搜索条件的成本默认是0.2。1.0、0.2这些数字称之为<code>成本常数</code>。</p>
<h3 id="122-单表查询的成本">12.2 单表查询的成本</h3>
<pre><code class="language-mysql">CREATE TABLE single_table (
    id INT NOT NULL AUTO_INCREMENT,
    key1 VARCHAR(100),
    key2 INT,
    key3 VARCHAR(100),
    key_part1 VARCHAR(100),
    key_part2 VARCHAR(100),
    key_part3 VARCHAR(100),
    common_field VARCHAR(100),
    PRIMARY KEY (id),
    KEY idx_key1 (key1),
    UNIQUE KEY idx_key2 (key2),
    KEY idx_key3 (key3),
    KEY idx_key_part(key_part1, key_part2, key_part3)
) Engine=InnoDB CHARSET=utf8;
</code></pre>
<p>假设这个表里边儿有10000条记录，除id列外其余的列都插入随机值。</p>
<h4 id="1221-基于成本的优化步骤">12.2.1 基于成本的优化步骤</h4>
<p>在一条单表查询语句真正执行之前，MySQL的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出成本最低的方案，这个成本最低的方案就是所谓的<code>执行计划</code>，之后才会调用存储引擎提供的接口真正的执行查询，这个过程总结一下就是这样：</p>
<ol>
<li>根据搜索条件，找出所有可能使用的索引</li>
<li>计算全表扫描的代价</li>
<li>计算使用不同索引执行查询的代价</li>
<li>对比各种执行方案的代价，找出成本最低的那一个</li>
</ol>
<p>下面我们就以一个实例来分析一下这些步骤，单表查询语句如下：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE 
    key1 IN ('a', 'b', 'c') AND 
    key2 &gt; 10 AND key2 &lt; 1000 AND 
    key3 &gt; key2 AND 
    key_part1 LIKE '%hello%' AND
    common_field = '123';
</code></pre>
<h5 id="根据搜索条件找出所有可能使用的索引">根据搜索条件，找出所有可能使用的索引</h5>
<p>对于B+树索引来说，只要索引列和常数使用=、&lt;=&gt;、IN、NOT IN、IS NULL、IS NOT NULL、&gt;、&lt;、&gt;=、&lt;=、BETWEEN、!=（不等于也可以写成&lt;&gt;）或者LIKE操作符连接起来，就可以产生一个所谓的范围区间（LIKE匹配字符串前缀也行），也就是说这些搜索条件都可能使用到索引，设计MySQL的大佬把一个查询中可能使用到的索引称之为<code>possible keys</code>。</p>
<p>我们分析一下上面查询中涉及到的几个搜索条件：</p>
<ul>
<li>key1 IN ('a', 'b', 'c')，这个搜索条件可以使用二级索引idx_key1。</li>
<li>key2 &gt; 10 AND key2 &lt; 1000，这个搜索条件可以使用二级索引idx_key2。</li>
<li>key3 &gt; key2，这个搜索条件的索引列由于没有和常数比较，所以并不能使用到索引。</li>
<li>key_part1 LIKE '%hello%'，key_part1通过LIKE操作符和以通配符开头的字符串做比较，不可以适用索引。</li>
<li>common_field = '123'，由于该列上压根儿没有索引，所以不会用到索引。<br>
综上所述，上面的查询语句可能用到的索引，也就是possible keys只有idx_key1和idx_key2。</li>
</ul>
<h5 id="计算全表扫描的代价">计算全表扫描的代价</h5>
<p>对于InnoDB存储引擎来说，全表扫描的意思就是把聚簇索引中的记录都依次和给定的搜索条件做一下比较，把符合搜索条件的记录加入到结果集，所以需要将聚簇索引对应的页面加载到内存中，然后再检测记录是否符合搜索条件。由于查询成本=I/O成本+CPU成本，所以计算全表扫描的代价需要两个信息：</p>
<ul>
<li>聚簇索引占用的页面数</li>
<li>该表中的记录数</li>
</ul>
<p>这两个信息从哪来呢？MySQL为每个表维护了一系列的统计信息，MySQL给我们提供了SHOW TABLE STATUS语句来查看表的统计信息，如果要看指定的某个表的统计信息，在该语句后加对应的LIKE语句就好了，比方说我们要查看single_table这个表的统计信息可以这么写：</p>
<pre><code class="language-mysql">mysql&gt; SHOW TABLE STATUS LIKE 'single_table'\G
*************************** 1. row ***************************
           Name: single_table
         Engine: InnoDB
        Version: 10
     Row_format: Dynamic
           Rows: 9693
 Avg_row_length: 163
    Data_length: 1589248
Max_data_length: 0
   Index_length: 2752512
      Data_free: 4194304
 Auto_increment: 10001
    Create_time: 2018-12-10 13:37:23
    Update_time: 2018-12-10 13:38:03
     Check_time: NULL
      Collation: utf8_general_ci
       Checksum: NULL
 Create_options:
        Comment:
1 row in set (0.01 sec)
</code></pre>
<ol>
<li>Rows<br>
本选项表示表中的记录条数。对于使用MyISAM存储引擎的表来说，该值是准确的，对于使用InnoDB存储引擎的表来说，该值是一个估计值。从查询结果我们也可以看出来，由于我们的single_table表是使用InnoDB存储引擎的，所以虽然实际上表中有10000条记录，但是SHOW TABLE STATUS显示的Rows值只有9693条记录。</li>
<li>Data_length<br>
本选项表示表占用的存储空间字节数。使用MyISAM存储引擎的表来说，该值就是数据文件的大小，对于使用InnoDB存储引擎的表来说，该值就相当于聚簇索引占用的存储空间大小，也就是说可以这样计算该值的大小：<mark>Data_length = 聚簇索引的页面数量 x 每个页面的大小</mark><br>
我们的single_table使用默认16KB的页面大小，而上面查询结果显示Data_length的值是1589248，所以我们可以反向来推导出聚簇索引的页面数量：<mark>聚簇索引的页面数量 = 1589248 ÷ 16 ÷ 1024 = 97</mark></li>
</ol>
<p>我们现在已经得到了聚簇索引占用的页面数量以及该表记录数的估计值，所以就可以计算全表扫描成本了，但是MySQL在真实计算成本时会进行一些微调，这些微调的值是直接硬编码到代码里的，，但是由于这些微调的值十分的小，并不影响我们分析，所以我们也没有必要在这些微调值上纠结了。现在可以看一下全表扫描成本的计算过程：</p>
<ol>
<li>I/O成本<br>
97 x 1.0 + 1.1 = 98.1<br>
97指的是聚簇索引占用的页面数，1.0指的是加载一个页面的成本常数，后边的1.1是一个微调值，我们不用在意。</li>
<li>CPU成本：<br>
9693 x 0.2 + 1.0 = 1939.6<br>
9693指的是统计数据中表的记录数，对于InnoDB存储引擎来说是一个估计值，0.2指的是访问一条记录所需的成本常数，后边的1.0是一个微调值，我们不用在意。</li>
<li>总成本<br>
98.1 + 1939.6 = 2037.7</li>
</ol>
<h5 id="计算使用不同索引执行查询的代价">计算使用不同索引执行查询的代价</h5>
<p>从第1步分析我们得到，上述查询可能使用到idx_key1和idx_key2这两个索引，我们需要分别分析单独使用这些索引执行查询的成本，最后还要分析是否可能使用到索引合并。这里需要提一点的是，MySQL查询优化器先分析使用唯一二级索引的成本，再分析使用普通索引的成本，所以我们也先分析idx_key2的成本，然后再看使用idx_key1的成本。</p>
<p><strong>使用idx_key2执行查询的成本分析</strong><br>
idx_key2对应的搜索条件是：key2 &gt; 10 AND key2 &lt; 1000，也就是说对应的范围区间就是：(10, 1000)。</p>
<p>对于使用二级索引 + 回表方式的查询，MySQL计算这种查询的成本依赖两个方面的数据：</p>
<ol>
<li>范围区间数量<br>
不论某个范围区间的二级索引到底占用了多少页面，查询优化器粗暴的认为读取索引的一个范围区间的I/O成本和读取一个页面是相同的。</li>
<li>需要回表的记录数<br>
优化器需要计算二级索引的某个范围区间到底包含多少条记录，对于本例来说就是要计算idx_key2在(10, 1000)这个范围区间中包含多少二级索引记录，计算过程是这样的：</li>
</ol>
<ul>
<li>步骤1：先根据key2 &gt; 10这个条件访问一下idx_key2对应的B+树索引，找到满足key2 &gt; 10这个条件的第一条记录，我们把这条记录称之为区间最左记录。我们前头说过在B+数树中定位一条记录的过程是贼快的，是常数级别的，所以这个过程的性能消耗是可以忽略不计的。</li>
<li>步骤2：然后再根据key2 &lt; 1000这个条件继续从idx_key2对应的B+树索引中找出第一条满足这个条件的记录，我们把这条记录称之为区间最右记录，这个过程的性能消耗也可以忽略不计的。</li>
<li>步骤3：如果区间最左记录和区间最右记录相隔不太远（在MySQL 5.7.21这个版本里，只要相隔不大于10个页面即可），那就可以精确统计出满足key2 &gt; 10 AND key2 &lt; 1000条件的二级索引记录条数。否则只沿着区间最左记录向右读10个页面，计算平均每个页面中包含多少记录，然后用这个平均值乘以区间最左记录和区间最右记录之间的页面数量就可以了。</li>
</ul>
<p>根据上述算法测得idx_key2在区间(10, 1000)之间大约有95条记录。读取这95条二级索引记录需要付出的CPU成本就是：95 x 0.2 + 0.01 = 19.01，其中95是需要读取的二级索引记录条数，0.2是读取一条记录成本常数，0.01是微调。</p>
<p>在通过二级索引获取到记录之后，还需要干两件事儿：</p>
<ol>
<li>根据这些记录里的主键值到聚簇索引中做回表操作</li>
<li>回表操作后得到的完整用户记录，然后再检测其他搜索条件是否成立</li>
</ol>
<p><strong>使用idx_key1执行查询的成本分析</strong></p>
<h5 id="对比各种执行方案的代价找出成本最低的那一个">对比各种执行方案的代价，找出成本最低的那一个</h5>
<h3 id="123-基于索引统计数据的成本计算">12.3 基于索引统计数据的成本计算</h3>
<h3 id="基于索引统计数据的成本计算">基于索引统计数据的成本计算</h3>
<p>有时候使用索引执行查询时会有许多单点区间，比如使用<code>IN</code>语句就很容易产生非常多的单点区间，比如下面这个查询（下面查询语句中的<code>...</code>表示还有很多参数）：</p>
<pre><code class="language-mysql">SELECT * FROM single_table WHERE key1 IN ('aa1', 'aa2', 'aa3', ... , 'zzz');
</code></pre>
<p>很显然，这个查询可能使用到的索引就是<code>idx_key1</code>，由于这个索引并不是唯一二级索引，所以并不能确定一个单点区间对应的二级索引记录的条数有多少，需要我们去计算。计算方式我们上面已经介绍过了，就是先获取索引对应的<code>B+</code>树的<code>区间最左记录</code>和<code>区间最右记录</code>，然后再计算这两条记录之间有多少记录（记录条数少的时候可以做到精确计算，多的时候只能估算）。设计<code>MySQL</code>的大佬把这种通过直接访问索引对应的<code>B+</code>树来计算某个范围区间对应的索引记录条数的方式称之为<code>index dive</code>。</p>
<p>有零星几个单点区间的话，使用<code>index dive</code>的方式去计算这些单点区间对应的记录数也不是什么问题，可是你架不住憋足了劲往<code>IN</code>语句里塞东西呀，我就见过有的同学写的<code>IN</code>语句里有20000个参数的，这就意味着<code>MySQL</code>的查询优化器为了计算这些单点区间对应的索引记录条数，要进行20000次<code>index dive</code>操作，这性能损耗可就大了，搞不好计算这些单点区间对应的索引记录条数的成本比直接全表扫描的成本都大了。设计<code>MySQL</code>的大佬们多聪明啊，他们当然考虑到了这种情况，所以提供了一个系统变量<code>eq_range_index_dive_limit</code>，我们看一下在<code>MySQL 5.7.21</code>中这个系统变量的默认值：</p>
<pre><code class="language-mysql">mysql&gt; SHOW VARIABLES LIKE '%dive%'; 
+---------------------------+-------+ | Variable_name | Value | +---------------------------+-------+ | eq_range_index_dive_limit | 200 | +---------------------------+-------+ 
1 row in set (0.08 sec)
</code></pre>
<p>也就是说如果我们的<code>IN</code>语句中的参数个数小于200个的话，将使用<code>index dive</code>的方式计算各个单点区间对应的记录条数，如果大于或等于200个的话，可就不能使用<code>index dive</code>了，要使用所谓的索引统计数据来进行估算。</p>
<p>像会为每个表维护一份统计数据一样，<code>MySQL</code>也会为表中的每一个索引维护一份统计数据，查看某个表中索引的统计数据可以使用<code>SHOW INDEX FROM 表名</code>的语法，比如我们查看一下<code>single_table</code>的各个索引的统计数据可以这么写：</p>
<pre><code class="language-mysql">SHOW INDEX FROM single_table; 
</code></pre>
<table>
<thead>
<tr>
<th style="text-align:center">属性名</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>Table</code></td>
<td>索引所属表的名称。</td>
</tr>
<tr>
<td style="text-align:center"><code>Non_unique</code></td>
<td>索引列的值是否是唯一的，聚簇索引和唯一二级索引的该列值为<code>0</code>，普通二级索引该列值为<code>1</code>。</td>
</tr>
<tr>
<td style="text-align:center"><code>Key_name</code></td>
<td>索引的名称。</td>
</tr>
<tr>
<td style="text-align:center"><code>Seq_in_index</code></td>
<td>索引列在索引中的位置，从1开始计数。比如对于联合索引<code>idx_key_part</code>，来说，<code>key_part1</code>、<code>key_part2</code>和<code>key_part3</code>对应的位置分别是1、2、3。</td>
</tr>
<tr>
<td style="text-align:center"><code>Column_name</code></td>
<td>索引列的名称。</td>
</tr>
<tr>
<td style="text-align:center"><code>Collation</code></td>
<td>索引列中的值是按照何种排序方式存放的，值为<code>A</code>时代表升序存放，为<code>NULL</code>时代表降序存放。</td>
</tr>
<tr>
<td style="text-align:center"><code>Cardinality</code></td>
<td>索引列中不重复值的数量。后边我们会重点看这个属性的。</td>
</tr>
<tr>
<td style="text-align:center"><code>Sub_part</code></td>
<td>对于存储字符串或者字节串的列来说，有时候我们只想对这些串的前<code>n</code>个字符或字节建立索引，这个属性表示的就是那个<code>n</code>值。如果对完整的列建立索引的话，该属性的值就是<code>NULL</code>。</td>
</tr>
<tr>
<td style="text-align:center"><code>Packed</code></td>
<td>索引列如何被压缩，<code>NULL</code>值表示未被压缩。这个属性我们暂时不了解，可以先忽略掉。</td>
</tr>
<tr>
<td style="text-align:center"><code>Null</code></td>
<td>该索引列是否允许存储<code>NULL</code>值。</td>
</tr>
<tr>
<td style="text-align:center"><code>Index_type</code></td>
<td>使用索引的类型，我们最常见的就是<code>BTREE</code>，其实也就是<code>B+</code>树索引。</td>
</tr>
<tr>
<td style="text-align:center"><code>Comment</code></td>
<td>索引列注释信息。</td>
</tr>
<tr>
<td style="text-align:center"><code>Index_comment</code></td>
<td>索引注释信息。</td>
</tr>
</tbody>
</table>
<h3 id="124-连接查询的成本">12.4 连接查询的成本</h3>
<h4 id="1241-准备工作">12.4.1 准备工作</h4>
<p>连接查询至少是要有两个表的，只有一个<code>single_table</code>表是不够的，所以为了故事的顺利发展，我们直接构造一个和<code>single_table</code>表一模一样的<code>single_table2</code>表。为了简便起见，我们把<code>single_table</code>表称为<code>s1</code>表，把<code>single_table2</code>表称为<code>s2</code>表。</p>
<h4 id="1242-condition-filtering介绍">12.4.2 Condition filtering介绍</h4>
<p>我们前面说过，<code>MySQL</code>中连接查询采用的是嵌套循环连接算法，驱动表会被访问一次，被驱动表可能会被访问多次，所以对于两表连接查询来说，它的查询成本由下面两个部分构成：</p>
<ul>
<li>单次查询驱动表的成本</li>
<li>多次查询被驱动表的成本（<span style="color:red">具体查询多少次取决于对驱动表查询的结果集中有多少条记录</span>）</li>
</ul>
<p>  我们把对驱动表进行查询后得到的记录条数称之为驱动表的<code>扇出</code>（英文名：<code>fanout</code>）。很显然驱动表的扇出值越小，对被驱动表的查询次数也就越少，连接查询的总成本也就越低。当查询优化器想计算整个连接查询所使用的成本时，就需要计算出驱动表的扇出值，有的时候扇出值的计算是很容易的，比如下面这两个查询：</p>
<ul>
<li>查询一：<pre><code>SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2;
</code></pre>
假设使用<code>s1</code>表作为驱动表，很显然对驱动表的单表查询只能使用全表扫描的方式执行，驱动表的扇出值也很明确，那就是驱动表中有多少记录，扇出值就是多少。我们前面说过，统计数据中<code>s1</code>表的记录行数是<code>9693</code>，也就是说优化器就直接会把<code>9693</code>当作在<code>s1</code>表的扇出值。</li>
<li>查询二：<pre><code>SELECT * FROM single_table AS s1 INNER JOIN single_table2 AS s2 
WHERE s1.key2 &gt;10 AND s1.key2 &lt; 1000;
</code></pre>
仍然假设<code>s1</code>表是驱动表的话，很显然对驱动表的单表查询可以使用<code>idx_key2</code>索引执行查询。此时<code>idx_key2</code>的范围区间<code>(10, 1000)</code>中有多少条记录，那么扇出值就是多少。我们前面计算过，满足<code>idx_key2</code>的范围区间<code>(10, 1000)</code>的记录数是95条，也就是说本查询中优化器会把<code>95</code>当作驱动表<code>s1</code>的扇出值。</li>
</ul>
<p>这两种情况下计算驱动表扇出值时需要靠<code>猜</code>：</p>
<ul>
<li>如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要猜满足搜索条件的记录到底有多少条。</li>
<li>如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要猜满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。</li>
</ul>
<p>设计<code>MySQL</code>的大佬把这个<code>猜</code>的过程称之为<code>condition filtering</code>。</p>
<h3 id="125-两表连接的成本分析">12.5 两表连接的成本分析</h3>
<p>连接查询的成本计算公式是这样的：连接查询总成本 = 单次访问驱动表的成本 + 驱动表扇出数 x 单次访问被驱动表的成本</p>
<p>对于左（外）连接和右（外）连接查询来说，它们的驱动表是固定的，所以想要得到最优的查询方案只需要：</p>
<ul>
<li>分别为驱动表和被驱动表选择成本最低的访问方法。<br>
可是对于内连接来说，驱动表和被驱动表的位置是可以互换的，所以需要考虑两个方面的问题：</li>
<li>不同的表作为驱动表最终的查询成本可能是不同的，也就是需要考虑最优的表连接顺序。</li>
<li>然后分别为驱动表和被驱动表选择成本最低的访问方法。</li>
</ul>
<p>最后优化器会比较最优访问成本，选取那个成本更低的连接顺序去真正的执行查询，连接查询成本占大头的其实是<code>驱动表扇出数 x 单次访问被驱动表的成本</code>，所以我们的优化重点其实是下面这两个部分：</p>
<ul>
<li>尽量减少驱动表的扇出</li>
<li>对被驱动表的访问成本尽量低</li>
</ul>
<p>这一点对于我们实际书写连接查询语句时十分有用，我们需要<span style="color:red">尽量在被驱动表的连接列上建立索引</span>，这样就可以使用<code>ref</code>访问方法来降低访问被驱动表的成本了。如果可以，被驱动表的连接列最好是该表的主键或者唯一二级索引列，这样就可以把访问被驱动表的成本降到更低了。</p>
<h3 id="126-多表连接的成本分析">12.6 多表连接的成本分析</h3>
<p>首先要考虑一下多表连接时可能产生出多少种连接顺序：</p>
<ul>
<li>
<p>对于两表连接，比如表A和表B连接<br>
  只有 AB、BA这两种连接顺序。其实相当于<code>2 × 1 = 2</code>种连接顺序。</p>
</li>
<li>
<p>对于三表连接，比如表A、表B、表C进行连接<br>
  有ABC、ACB、BAC、BCA、CAB、CBA这么6种连接顺序。其实相当于<code>3 × 2 × 1 = 6</code>种连接顺序。</p>
</li>
<li>
<p>对于四表连接的话，则会有<code>4 × 3 × 2 × 1 = 24</code>种连接顺序。</p>
</li>
<li>
<p>对于<code>n</code>表连接的话，则有 <code>n × (n-1) × (n-2) × ··· × 1</code>种连接顺序，就是n的阶乘种连接顺序，也就是<code>n!</code>。<br>
  有<code>n</code>个表进行连接，<code>MySQL</code>查询优化器要每一种连接顺序的成本都计算一遍么？那可是<code>n!</code>种连接顺序呀。其实真的是要都算一遍，不过设计<code>MySQL</code>的大佬们想了很多办法减少计算非常多种连接顺序的成本的方法：</p>
</li>
<li>
<p>提前结束某种顺序的成本评估<br>
  <code>MySQL</code>在计算各种链接顺序的成本之前，会维护一个全局的变量，这个变量表示当前最小的连接查询成本。如果在分析某个连接顺序的成本时，该成本已经超过当前最小的连接查询成本，那就压根儿不对该连接顺序继续往下分析了。比方说A、B、C三个表进行连接，已经得到连接顺序<code>ABC</code>是当前的最小连接成本，比方说<code>10.0</code>，在计算连接顺序<code>BCA</code>时，发现<code>B</code>和<code>C</code>的连接成本就已经大于<code>10.0</code>时，就不再继续往后分析<code>BCA</code>这个连接顺序的成本了。</p>
</li>
<li>
<p>系统变量<code>optimizer_search_depth</code><br>
  为了防止无穷无尽的分析各种连接顺序的成本，MySQL提出了<code>optimizer_search_depth</code>系统变量，如果连接表的个数小于该值，那么就继续穷举分析每一种连接顺序的成本，否则只对与<code>optimizer_search_depth</code>值相同数量的表进行穷举分析。很显然，该值越大，成本分析的越精确，越容易得到好的执行计划，但是消耗的时间也就越长，否则得到不是很好的执行计划，但可以省掉很多分析连接成本的时间。</p>
</li>
<li>
<p>根据某些规则压根儿就不考虑某些连接顺序<br>
  即使是有上面两条规则的限制，但是分析多个表不同连接顺序成本花费的时间还是会很长，所以设计<code>MySQL</code>的大佬干脆提出了一些所谓的<code>启发式规则</code>（就是根据以往经验指定的一些规则），凡是不满足这些规则的连接顺序压根儿就不分析，这样可以极大的减少需要分析的连接顺序的数量，但是也可能造成错失最优的执行计划。他们提供了一个系统变量<code>optimizer_prune_level</code>来控制到底是不是用这些启发式规则。</p>
</li>
</ul>
<h4 id="1261-调节成本常数">12.6.1 调节成本常数</h4>
<p>我们前面之介绍了两个<code>成本常数</code>：</p>
<ul>
<li>读取一个页面花费的成本默认是<code>1.0</code></li>
<li>检测一条记录是否符合搜索条件的成本默认是<code>0.2</code></li>
</ul>
<p>其实除了这两个成本常数，<code>MySQL</code>还支持好多呢，它们被存储到了<code>mysql</code>数据库的两个表中：<code>engine_cost | | server_cost</code></p>
<p>一条语句的执行其实是分为两层的：在<code>server</code>层进行连接管理、查询缓存、语法解析、查询优化等操作，在存储引擎层执行具体的数据存取操作。也就是说一条语句在<code>server</code>层中执行的成本是和它操作的表使用的存储引擎是没关系的，所以关于这些操作对应的<code>成本常数</code>就存储在了<code>server_cost</code>表中，而依赖于存储引擎的一些操作对应的<code>成本常数</code>就存储在了<code>engine_cost</code>表中。</p>
<h5 id="mysqlserver_cost表">mysql.server_cost表</h5>
<table>
<thead>
<tr>
<th style="text-align:center">成本常数名称</th>
<th style="text-align:center">默认值</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>disk_temptable_create_cost</code></td>
<td style="text-align:center"><code>40.0</code></td>
<td style="text-align:left">创建基于磁盘的临时表的成本，如果增大这个值的话会让优化器尽量少的创建基于磁盘的临时表。</td>
</tr>
<tr>
<td style="text-align:center"><code>disk_temptable_row_cost</code></td>
<td style="text-align:center"><code>1.0</code></td>
<td style="text-align:left">向基于磁盘的临时表写入或读取一条记录的成本，如果增大这个值的话会让优化器尽量少的创建基于磁盘的临时表。</td>
</tr>
<tr>
<td style="text-align:center"><code>key_compare_cost</code></td>
<td style="text-align:center"><code>0.1</code></td>
<td style="text-align:left">两条记录做比较操作的成本，多用在排序操作上，如果增大这个值的话会提升<code>filesort</code>的成本，让优化器可能更倾向于使用索引完成排序而不是<code>filesort</code>。</td>
</tr>
<tr>
<td style="text-align:center"><code>memory_temptable_create_cost</code></td>
<td style="text-align:center"><code>2.0</code></td>
<td style="text-align:left">创建基于内存的临时表的成本，如果增大这个值的话会让优化器尽量少的创建基于内存的临时表。</td>
</tr>
<tr>
<td style="text-align:center"><code>memory_temptable_row_cost</code></td>
<td style="text-align:center"><code>0.2</code></td>
<td style="text-align:left">向基于内存的临时表写入或读取一条记录的成本，如果增大这个值的话会让优化器尽量少的创建基于内存的临时表。</td>
</tr>
<tr>
<td style="text-align:center"><code>row_evaluate_cost</code></td>
<td style="text-align:center"><code>0.2</code></td>
<td style="text-align:left">这个就是我们之前一直使用的检测一条记录是否符合搜索条件的成本，增大这个值可能让优化器更倾向于使用索引而不是直接全表扫描。</td>
</tr>
</tbody>
</table>
<h5 id="mysqlengine_cost表">mysql.engine_cost表</h5>
<p>与<code>server_cost</code>相比，<code>engine_cost</code>多了两个列：</p>
<ul>
<li><code>engine_name</code>列：指成本常数适用的存储引擎名称。如果该值为<code>default</code>，意味着对应的成本常数适用于所有的存储引擎。</li>
<li><code>device_type</code>列：指存储引擎使用的设备类型，这主要是为了区分常规的机械硬盘和固态硬盘，不过在<code>MySQL 5.7.21</code>这个版本中并没有对机械硬盘的成本和固态硬盘的成本作区分，所以该值默认是<code>0</code>。</li>
</ul>
<h2 id="第十三章-兵马未动粮草先行-innodb统计数据是如何收集的">第十三章 兵马未动，粮草先行-InnoDB统计数据是如何收集的</h2>
<p>我们前面介绍查询成本的时候经常用到一些统计数据，比如通过SHOW TABLE STATUS可以看到关于表的统计数据，通过SHOW INDEX可以看到关于索引的统计数据，那么这些统计数据是怎么来的呢？它们是以什么方式收集的呢？</p>
<h3 id="131-两种不同的统计数据存储方式">13.1 两种不同的统计数据存储方式</h3>
<p>InnoDB提供了两种存储统计数据的方式：</p>
<ol>
<li>永久性的统计数据<br>
  这种统计数据存储在磁盘上，也就是服务器重启之后这些统计数据还在。</li>
<li>非永久性的统计数据<br>
  这种统计数据存储在内存中，当服务器关闭时这些这些统计数据就都被清除掉了，等到服务器重启之后，在某些适当的场景下才会重新收集这些统计数据。</li>
</ol>
<p>MySQL给我们提供了系统变量innodb_stats_persistent来控制到底采用哪种方式去存储统计数据。在MySQL 5.6.6之前，innodb_stats_persistent的值默认是OFF，也就是说InnoDB的统计数据默认是存储到内存的，之后的版本中innodb_stats_persistent的值默认是ON，也就是统计数据默认被存储到磁盘中。</p>
<p>不过InnoDB默认是<mark>以表为单位来收集和存储统计数据的</mark>，也就是说我们可以把某些表的统计数据（以及该表的索引统计数据）存储在磁盘上，把另一些表的统计数据存储在内存中。怎么做到的呢？我们可以在创建和修改表的时候通过指定STATS_PERSISTENT属性来指明该表的统计数据存储方式：</p>
<pre><code class="language-mysql">CREATE TABLE 表名 (...) Engine=InnoDB, STATS_PERSISTENT = (1|0);
ALTER TABLE 表名 Engine=InnoDB, STATS_PERSISTENT = (1|0);
</code></pre>
<p>当 STATS_PERSISTENT=1 时，表明我们想把该表的统计数据永久的存储到磁盘上，当 STATS_PERSISTENT=0 时，表 明我们想把该表的统计数据临时的存储到内存中。如果我们在创建表时未指定 STATS_PERSISTENT 属性，那默认 采用系统变量 innodb_stats_persistent 的值作为该属性的值。</p>
<h3 id="132-基于磁盘的永久性统计数据">13.2 基于磁盘的永久性统计数据</h3>
<p>当我们选择把某个表以及该表索引的统计数据存放到磁盘上时，实际上是把这些统计数据存储到了两个表里:</p>
<pre><code class="language-mysql">mysql&gt; SHOW TABLES FROM mysql LIKE 'innodb%';
    +---------------------------+
    | Tables_in_mysql (innodb%) |
    +---------------------------+
    | innodb_index_stats        |
    | innodb_table_stats        |
    +---------------------------+
    2 rows in set (0.01 sec)
</code></pre>
<p>可以看到，这两个表都位于 mysql 系统数据库下边，其中:</p>
<ul>
<li>innodb_table_stats 存储了关于表的统计数据，每一条记录对应着一个表的统计数据。</li>
<li>innodb_index_stats 存储了关于索引的统计数据，每一条记录对应着一个索引的一个统计项的统计数据。</li>
</ul>
<h4 id="1321-innodb_table_stats">13.2.1 innodb_table_stats</h4>
<table>
<thead>
<tr>
<th style="text-align:center">字段名</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>database_name</code></td>
<td style="text-align:left">数据库名</td>
</tr>
<tr>
<td style="text-align:center"><code>table_name</code></td>
<td style="text-align:left">表名</td>
</tr>
<tr>
<td style="text-align:center"><code>last_update</code></td>
<td style="text-align:left">本条记录最后更新时间</td>
</tr>
<tr>
<td style="text-align:center"><code>n_rows</code></td>
<td style="text-align:left">表中记录的条数</td>
</tr>
<tr>
<td style="text-align:center"><code>clustered_index_size</code></td>
<td style="text-align:left">表的聚簇索引占用的页面数量</td>
</tr>
<tr>
<td style="text-align:center"><code>sum_of_other_index_sizes</code></td>
<td style="text-align:left">表的其他索引占用的页面数量</td>
</tr>
</tbody>
</table>
<p>注意这个表的主键是 (database_name,table_name) ，也就是innodb_table_stats表的每条记录代表着一个表的统计信息。</p>
<h5 id="n_rows统计项的收集">n_rows统计项的收集</h5>
<p>InnoDB 统计一个表中有多少行记录的套路是这样的:</p>
<ul>
<li>按照一定算法(并不是纯粹随机的)选取几个叶子节点页面，计算每个页面中主键值记录数量，然后计算平<br>
均一个页面中主键值的记录数量乘以全部叶子节点的数量就算是该表的 n_rows 值。</li>
</ul>
<p>可以看出来这个<code>n_rows</code>值精确与否取决于统计时采样的页面数量，设计MySQL很贴心的为我们准备了一个名为<code>innodb_stats_persistent_sample_pages</code>的系统变量来控制<span style="color:red">使用永久性的统计数据时，计算统计数据时采样的页面数量</span>。该值设置的越大，统计出的<code>n_rows</code>值越精确，但是统计耗时也就最久；该值设置的越小，统计出的<code>n_rows</code>值越不精确，但是统计耗时特别少。所以在实际使用是需要我们去权衡利弊，该系统变量的默认值是<code>20</code>。</p>
<p>我们前面说过，不过<code>InnoDB</code>默认是<span style="color:red">以表为单位来收集和存储统计数据的</span>，我们也可以单独设置某个表的采样页面的数量，设置方式就是在创建或修改表的时候通过指定<code>STATS_SAMPLE_PAGES</code>属性来指明该表的统计数据存储方式：</p>
<pre><code class="language-mysql">    CREATE TABLE 表名 (...) Engine=InnoDB, STATS_SAMPLE_PAGES = 具体的采样页面数量;   
    ALTER TABLE 表名 Engine=InnoDB, STATS_SAMPLE_PAGES = 具体的采样页面数量;
</code></pre>
<h5 id="clustered_index_size和sum_of_other_index_sizes统计项的收集">clustered_index_size和sum_of_other_index_sizes统计项的收集</h5>
<p>这两个统计项的收集过程如下：</p>
<ol>
<li>从数据字典里找到表的各个索引对应的根页面位置。<br>
系统表<code>SYS_INDEXES</code>里存储了各个索引对应的根页面信息。</li>
<li>从根页面的<code>Page Header</code>里找到叶子节点段和非叶子节点段对应的<code>Segment Header</code>。<br>
在每个索引的根页面的<code>Page Header</code>部分都有两个字段：
<ul>
<li><code>PAGE_BTR_SEG_LEAF</code>：表示B+树叶子段的<code>Segment Header</code>信息。</li>
<li><code>PAGE_BTR_SEG_TOP</code>：表示B+树非叶子段的<code>Segment Header</code>信息。</li>
</ul>
</li>
<li>从叶子节点段和非叶子节点段的<code>Segment Header</code>中找到这两个段对应的<code>INODE Entry</code>结构。</li>
</ol>
<h4 id="1322-innodb_index_stats">13.2.2 innodb_index_stats</h4>
<table>
<thead>
<tr>
<th style="text-align:center">字段名</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>database_name</code></td>
<td style="text-align:left">数据库名</td>
</tr>
<tr>
<td style="text-align:center"><code>table_name</code></td>
<td style="text-align:left">表名</td>
</tr>
<tr>
<td style="text-align:center"><code>index_name</code></td>
<td style="text-align:left">索引名</td>
</tr>
<tr>
<td style="text-align:center"><code>last_update</code></td>
<td style="text-align:left">本条记录最后更新时间</td>
</tr>
<tr>
<td style="text-align:center"><code>stat_name</code></td>
<td style="text-align:left">统计项的名称</td>
</tr>
<tr>
<td style="text-align:center"><code>stat_value</code></td>
<td style="text-align:left">对应的统计项的值</td>
</tr>
<tr>
<td style="text-align:center"><code>sample_size</code></td>
<td style="text-align:left">为生成统计数据而采样的页面数量</td>
</tr>
<tr>
<td style="text-align:center"><code>stat_description</code></td>
<td style="text-align:left">对应的统计项的描述</td>
</tr>
</tbody>
</table>
<p>注意这个表的主键是<code>(database_name,table_name,index_name,stat_name)</code>，其中的<code>stat_name</code>是指统计项的名称，也就是说<span style="color:red">innodb_index_stats表的每条记录代表着一个索引的一个统计项</span>。</p>
<h4 id="1323-定期更新统计数据">13.2.3 定期更新统计数据</h4>
<p>随着我们不断的对表进行增删改操作，表中的数据也一直在变化，<code>innodb_table_stats</code>和<code>innodb_index_stats</code>表里的统计数据是不是也应该跟着变一变了？当然要变了，不变的话<code>MySQL</code>查询优化器计算的成本可就差老鼻子远了。<code>MySQL</code>提供了如下两种更新统计数据的方式：</p>
<ul>
<li>开启<code>innodb_stats_auto_recalc</code>。<br>
系统变量<code>innodb_stats_auto_recalc</code>决定着服务器是否自动重新计算统计数据，它的默认值是<code>ON</code>，也就是该功能默认是开启的。每个表都维护了一个变量，该变量记录着对该表进行增删改的记录条数，如果发生变动的记录数量超过了表大小的<code>10%</code>，并且自动重新计算统计数据的功能是打开的，那么服务器会重新进行一次统计数据的计算，并且更新<code>innodb_table_stats</code>和<code>innodb_index_stats</code>表。不过<span style="color:red">自动重新计算统计数据的过程是异步发生的</span>，也就是即使表中变动的记录数超过了<code>10%</code>，自动重新计算统计数据也不会立即发生，可能会延迟几秒才会进行计算。</li>
</ul>
<p>再一次强调，<code>InnoDB</code>默认是<span style="color:red">以表为单位来收集和存储统计数据的</span>，我们也可以单独为某个表设置是否自动重新计算统计数的属性，设置方式就是在创建或修改表的时候通过指定<code>STATS_AUTO_RECALC</code>属性来指明该表的统计数据存储方式：</p>
<pre><code class="language-mysql">CREATE TABLE 表名 (...) Engine=InnoDB, STATS_AUTO_RECALC = (1|0);
ALTER TABLE 表名 Engine=InnoDB, STATS_AUTO_RECALC = (1|0);
</code></pre>
<p>当<code>STATS_AUTO_RECALC=1</code>时，表明我们想让该表自动重新计算统计数据，当<code>STATS_PERSISTENT=0</code>时，表明不想让该表自动重新计算统计数据。如果我们在创建表时未指定<code>STATS_AUTO_RECALC</code>属性，那默认采用系统变量<code>innodb_stats_auto_recalc</code>的值作为该属性的值。</p>
<ul>
<li>手动调用<code>ANALYZE TABLE</code>语句来更新统计信息<br>
如果<code>innodb_stats_auto_recalc</code>系统变量的值为<code>OFF</code>的话，我们也可以手动调用<code>ANALYZE TABLE</code>语句来重新计算统计数据，比如我们可以这样更新关于<code>single_table</code>表的统计数据：</li>
</ul>
<pre><code class="language-mysql">mysql&gt; ANALYZE TABLE single_table;
</code></pre>
<p>需要注意的是，<span style="color:red">ANALYZE TABLE语句会立即重新计算统计数据，也就是这个过程是同步的</span>，在表中索引多或者采样页面特别多时这个过程可能会特别慢，请不要没事儿就运行一下<code>ANALYZE TABLE</code>语句，最好在业务不是很繁忙的时候再运行。</p>
<h4 id="1324-手动更新innodb_table_stats和innodb_index_stats表">13.2.4 手动更新<code>innodb_table_stats</code>和<code>innodb_index_stats</code>表</h4>
<p>其实<code>innodb_table_stats</code>和<code>innodb_index_stats</code>表就相当于一个普通的表一样，我们能对它们做增删改查操作。这也就意味着我们可以<span style="color:red">手动更新某个表或者索引的统计数据</span>。</p>
<p>更新完<code>innodb_table_stats</code>只是单纯的修改了一个表的数据，需要让<code>MySQL</code>查询优化器重新加载我们更改过的数据，运行下面的命令就可以了：</p>
<pre><code class="language-mysql">FLUSH TABLE single_table;
</code></pre>
<h3 id="133-基于内存的非永久性统计数据">13.3 基于内存的非永久性统计数据</h3>
<p>当我们把系统变量<code>innodb_stats_persistent</code>的值设置为<code>OFF</code>时，之后创建的表的统计数据默认就都是非永久性的了，或者我们直接在创建表或修改表时设置<code>STATS_PERSISTENT</code>属性的值为<code>0</code>，那么该表的统计数据就是非永久性的了。</p>
<p>与永久性的统计数据不同，非永久性的统计数据采样的页面数量是由<code>innodb_stats_transient_sample_pages</code>控制的，这个系统变量的默认值是<code>8</code>。</p>
<p>另外，由于非永久性的统计数据经常更新，所以导致<code>MySQL</code>查询优化器计算查询成本的时候依赖的是经常变化的统计数据，也就会<span style="color:red">生成经常变化的执行计划</span>。</p>
<h3 id="134-innodb_stats_method的使用">13.4 innodb_stats_method的使用</h3>
<p>我们知道<code>索引列不重复的值的数量</code>这个统计数据对于MySQL查询优化器十分重要，因为通过它可以计算出在索引列中平均一个值重复多少行，它的应用场景主要有两个：</p>
<ul>
<li>单表查询中单点区间太多，当<code>IN</code>里的参数数量过多时，采用<code>index dive</code>的方式直接访问<code>B+</code>树索引去统计每个单点区间对应的记录的数量就太耗费性能了，所以直接依赖统计数据中的平均一个值重复多少行来计算单点区间对应的记录数量。</li>
<li>连接查询时，如果有涉及两个表的等值匹配连接条件，该连接条件对应的被驱动表中的列又拥有索引时，则可以使用<code>ref</code>访问方法来对被驱动表进行查询。</li>
</ul>
<h3 id="135-总结">13.5 总结</h3>
<ul>
<li><code>InnoDB</code>以表为单位来收集统计数据，这些统计数据可以是基于磁盘的永久性统计数据，也可以是基于内存的非永久性统计数据。</li>
<li><code>innodb_stats_persistent</code>控制着使用永久性统计数据还是非永久性统计数据；<code>innodb_stats_persistent_sample_pages</code>控制着永久性统计数据的采样页面数量；<code>innodb_stats_transient_sample_pages</code>控制着非永久性统计数据的采样页面数量；<code>innodb_stats_auto_recalc</code>控制着是否自动重新计算统计数据。</li>
<li>我们可以针对某个具体的表，在创建和修改表时通过指定<code>STATS_PERSISTENT</code>、<code>STATS_AUTO_RECALC</code>、<code>STATS_SAMPLE_PAGES</code>的值来控制相关统计数据属性。</li>
<li><code>innodb_stats_method</code>决定着在统计某个索引列不重复值的数量时如何对待<code>NULL</code>值。</li>
</ul>
<h2 id="第十四章-不好看就要多整容-mysql基于规则的优化内含关于子查询优化二三事儿">第十四章 不好看就要多整容-MySQL基于规则的优化（内含关于子查询优化二三事儿）</h2>
<h3 id="141-条件化简">14.1 条件化简</h3>
<h4 id="1411-移除不必要的括号">14.1.1 移除不必要的括号</h4>
<p>有时候表达式里有许多无用的括号，优化器会把那些用不到的括号给干掉。</p>
<pre><code class="language-mysql">((a = 5 AND b = c) OR ((a &gt; c) AND (c &lt; 5)))
(a = 5 and b = c) OR (a &gt; c AND c &lt; 5)
</code></pre>
<h4 id="1412-常量传递constant_propagation">14.1.2 常量传递（constant_propagation）</h4>
<p>有时候某个表达式是某个列和某个常量做等值匹配，比如这样：</p>
<pre><code class="language-mysql">a = 5 AND b &gt; a
a = 5 AND b &gt; 5
</code></pre>
<p>当这个表达式和其他涉及列a的表达式使用AND连接起来时，可以将其他表达式中的a的值替换为5。</p>
<h4 id="1413-等值传递equality_propagation">14.1.3 等值传递（equality_propagation）</h4>
<p>有时候多个列之间存在等值匹配的关系，比如这样：</p>
<pre><code class="language-mysql">a = b and b = c and c = 5
a = 5 and b = 5 and c = 5
</code></pre>
<h4 id="1414-移除没用的条件trivial_condition_removal">14.1.4 移除没用的条件（trivial_condition_removal）</h4>
<p>对于一些明显永远为TRUE或者FALSE的表达式，优化器会移除掉它们，比如这个表达式：</p>
<pre><code class="language-mysql">(a &lt; 1 and b = b) OR (a = 6 OR 5 != 5)
(a &lt; 1 and TRUE) OR (a = 6 OR FALSE)
a &lt; 1 OR a = 6
</code></pre>
<h4 id="1415-表达式计算">14.1.5 表达式计算</h4>
<p>在查询开始执行之前，如果表达式中只包含常量的话，它的值会被先计算出来，比如这个：a = 5 + 1,所以就会被化简成：a = 6,但是这里需要注意的是，如果某个列并不是以单独的形式作为表达式的操作数时，比如出现在函数中，出现在某个更复杂表达式中，就像这样：ABS(a) &gt; 5 或者-a &lt; -8。</p>
<p>优化器是不会尝试对这些表达式进行化简的。我们前面说过只有搜索条件中索引列和常数使用某些运算符连接起来才可能使用到索引，所以如果可以的话，最好让索引列以单独的形式出现在表达式中。</p>
<h4 id="1416-having子句和where子句的合并">14.1.6 HAVING子句和WHERE子句的合并</h4>
<p>如果查询语句中没有出现诸如SUM、MAX等等的聚集函数以及GROUP BY子句，优化器就把HAVING子句和WHERE子句合并起来。</p>
<h4 id="1417-常量表检测">14.1.7 常量表检测</h4>
<p>下面这两种查询运行的特别快：</p>
<ol>
<li>查询的表中一条记录没有，或者只有一条记录。</li>
<li>使用主键等值匹配或者唯一二级索引列等值匹配作为搜索条件来查询某个表。</li>
</ol>
<p>这两种查询花费的时间特别少，少到可以忽略，所以也把通过这两种方式查询的表称之为常量表（英文名：constant tables）。优化器在分析一个查询语句时，先首先执行常量表查询，然后把查询中涉及到该表的条件全部替换成常数，最后再分析其余表的查询成本。</p>
<h3 id="142-外连接消除">14.2 外连接消除</h3>
<p>我们前面说过，内连接的驱动表和被驱动表的位置可以相互转换，而左（外）连接和右（外）连接的驱动表和被驱动表是固定的。这就导致内连接可能通过优化表的连接顺序来降低整体的查询成本，而外连接却无法优化表的连接顺序。</p>
<p>我们把这种在外连接查询中，指定的WHERE子句中包含被驱动表中的列不为NULL值的条件称之为空值拒绝（英文名：reject-NULL）。在被驱动表的WHERE子句符合空值拒绝的条件后，外连接和内连接可以相互转换。这种转换带来的好处就是查询优化器可以通过评估表的不同连接顺序的成本，选出成本最低的那种连接顺序来执行查询。</p>
<h3 id="143-子查询优化">14.3 子查询优化</h3>
<h4 id="1431-子查询语法">14.3.1 子查询语法</h4>
<p>在一个查询语句里的某个位置也可以有另一个查询语句，这个出现在某个查询语句的某个位置中的查询就被称为子查询。子查询可以在一个外层查询的各种位置出现，比如：</p>
<ol>
<li>SELECT子句中</li>
<li>FROM子句中</li>
<li>WHERE或ON子句中</li>
<li>ORDER BY子句中</li>
<li>GROUP BY子句中</li>
</ol>
<h5 id="按返回的结果集区分子查询">按返回的结果集区分子查询</h5>
<p>因为子查询本身也算是一个查询，所以可以按照它们返回的不同结果集类型而把这些子查询分为不同的类型：</p>
<ol>
<li>标量子查询<br>
那些只返回一个单一值的子查询称之为标量子查询</li>
<li>行子查询<br>
就是返回一条记录的子查询，不过这条记录需要包含多个列（只包含一个列就成了标量子查询了）</li>
<li>列子查询<br>
列子查询自然就是查询出一个列的数据喽，不过这个列的数据需要包含多条记录（只包含一条记录就成了标量子查询了）</li>
<li>表子查询<br>
就是子查询的结果既包含很多条记录，又包含很多个列</li>
</ol>
<h5 id="按与外层查询关系来区分子查询">按与外层查询关系来区分子查询</h5>
<ol>
<li>不相关子查询<br>
  如果子查询可以单独运行出结果，而不依赖于外层查询的值，我们就可以把这个子查询称之为不相关子查询。</li>
<li>相关子查询<br>
  如果子查询的执行需要依赖于外层查询的值，我们就可以把这个子查询称之为相关子查询。</li>
</ol>
<h5 id="子查询在布尔表达式中的使用">子查询在布尔表达式中的使用</h5>
<ol>
<li>
<p>使用=、&gt;、&lt;、&gt;=、&lt;=、&lt;&gt;、!=、&lt;=&gt;作为布尔表达式的操作符<br>
我们就把这些操作符称为comparison_operator吧，所以子查询组成的布尔表达式就长这样：<mark>操作数 comparison_operator (子查询)</mark><br>
这里的操作数可以是某个列名，或者是一个常量，或者是一个更复杂的表达式，甚至可以是另一个子查询。但是需要注意的是，这里的子查询只能是标量子查询或者行子查询，也就是子查询的结果只能返回一个单一的值或者只能是一条记录。</p>
</li>
<li>
<p>[NOT] IN/ANY/SOME/ALL子查询<br>
对于列子查询和表子查询来说，它们的结果集中包含很多条记录，这些记录相当于是一个集合，所以就不能单纯的和另外一个操作数使用comparison_operator来组成布尔表达式了，MySQL通过下面的语法来支持某个操作数和一个集合组成一个布尔表达式：</p>
<ul>
<li>IN或者NOT IN</li>
<li>ANY/SOME（ANY和SOME是同义词）</li>
<li>ALL</li>
</ul>
</li>
<li>
<p>EXISTS子查询</p>
</li>
</ol>
<h5 id="子查询语法注意事项">子查询语法注意事项</h5>
<ol>
<li>子查询必须用小括号扩起来。</li>
<li>在SELECT子句中的子查询必须是标量子查询</li>
<li>在想要得到标量子查询或者行子查询，但又不能保证子查询的结果集只有一条记录时，应该使用LIMIT 1语句来限制记录数量。</li>
<li>对于[NOT] IN/ANY/SOME/ALL子查询来说，子查询中不允许有LIMIT语句。<br>
正因为[NOT] IN/ANY/SOME/ALL子查询不支持LIMIT语句，所以子查询中的这些语句也就是多余的了：
<ul>
<li>ORDER BY子句</li>
<li>DISTINCT语句</li>
<li>没有聚集函数以及HAVING子句的GROUP BY子句。</li>
</ul>
</li>
<li>不允许在一条语句中增删改某个表的记录时同时还对该表进行子查询。</li>
</ol>
<h4 id="1432-子查询在mysql中是怎么执行的">14.3.2 子查询在MySQL中是怎么执行的</h4>
<pre><code class="language-mysql">CREATE TABLE single_table (
    id INT NOT NULL AUTO_INCREMENT,
    key1 VARCHAR(100),
    key2 INT,
    key3 VARCHAR(100),
    key_part1 VARCHAR(100),
    key_part2 VARCHAR(100),
    key_part3 VARCHAR(100),
    common_field VARCHAR(100),
    PRIMARY KEY (id),
    KEY idx_key1 (key1),
    UNIQUE KEY idx_key2 (key2),
    KEY idx_key3 (key3),
    KEY idx_key_part(key_part1, key_part2, key_part3)
) Engine=InnoDB CHARSET=utf8;
</code></pre>
<h5 id="标量子查询-行子查询的执行方式">标量子查询、行子查询的执行方式</h5>
<p>我们经常在下面两个场景中使用到标量子查询或者行子查询：</p>
<ol>
<li>SELECT子句中，在查询列表中的子查询必须是标量子查询。</li>
<li>子查询使用=、&gt;、&lt;、&gt;=、&lt;=、&lt;&gt;、!=、&lt;=&gt;等操作符和某个操作数组成一个布尔表达式，这样的子查询必须是标量子查询或者行子查询。</li>
</ol>
<p>对于上述两种场景中的不相关标量子查询或者行子查询来说，它们的执行方式是简单的，比方说下面这个查询语句：</p>
<pre><code class="language-mysql">SELECT * FROM s1 
    WHERE key1 = (SELECT common_field FROM s2 WHERE key3 = 'a' LIMIT 1);
</code></pre>
<ul>
<li>先单独执行(SELECT common_field FROM s2 WHERE key3 = 'a' LIMIT 1)这个子查询。</li>
<li>然后在将上一步子查询得到的结果当作外层查询的参数再执行外层查询SELECT * FROM s1 WHERE key1 = ...。</li>
</ul>
<p><code>对于包含不相关的标量子查询或者行子查询的查询语句来说，MySQL会分别独立的执行外层查询和子查询，就当作两个单表查询就好了。</code></p>
<p>对于相关的标量子查询或者行子查询来说，比如下面这个查询：</p>
<pre><code class="language-mysql">SELECT * FROM s1 WHERE 
    key1 = (SELECT common_field FROM s2 WHERE s1.key3 = s2.key3 LIMIT 1);
</code></pre>
<ul>
<li>先从外层查询中获取一条记录</li>
<li>然后从上一步骤中获取的那条记录中找出子查询中涉及到的值，然后执行子查询。</li>
<li>最后根据子查询的查询结果来检测外层查询WHERE子句的条件是否成立，如果成立，就把外层查询的那条记录加入到结果集，否则就丢弃。</li>
<li>再次执行第一步，获取第二条外层查询中的记录，依次类推～</li>
</ul>
<h5 id="in子查询优化">IN子查询优化</h5>
<p>对于不相关的IN子查询，比如这样：</p>
<pre><code class="language-mysql">SELECT * FROM s1 
    WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a');
</code></pre>
<p>如果in中的结果参数比较多时，用表中的每条记录判断一下它的column列是否符合in中的参数，这样效率是十分低下的，MySQL给出的解决办法是<code>不直接将不相关子查询的结果集当作外层查询的参数，而是将该结果集写入一个临时表里</code>。写入临时表的过程是这样的：</p>
<ul>
<li>该临时表的列就是子查询结果集中的列。</li>
<li>写入临时表的记录会被去重。</li>
<li>一般情况下子查询结果集不会大的离谱，所以会为它建立基于内存的使用Memory存储引擎的临时表，而且会为该表建立希索引。</li>
</ul>
<blockquote>
<p>如果子查询的结果集非常大，超过了系统变量tmp_table_size或者max_heap_table_size，临时表会转而使用基于磁盘的存储引擎来保存结果集中的记录，索引类型也对应转变为B+树索引。</p>
</blockquote>
<p>MySQL把这个将子查询结果集中的记录保存到临时表的过程称之为<code>物化</code>（英文名：Materialize）。为了方便起见，我们就把那个存储子查询结果集的临时表称之为<code>物化表</code>。正因为物化表中的记录都建立了索引（基于内存的物化表有哈希索引，基于磁盘的有B+树索引），通过索引执行IN语句判断某个操作数在不在子查询结果集中变得非常快，从而提升了子查询语句的性能。</p>
<p>当我们把子查询进行物化之后，假设子查询物化表的名称为materialized_table，该物化表存储的子查询结果集的列为m_val，那么这个查询其实可以从下面两种角度来看待：</p>
<ol>
<li>从表s1的角度来看待，整个查询的意思其实是：对于s1表中的每条记录来说，如果该记录的key1列的值在子查询对应的物化表中，则该记录会被加入最终的结果集。画个图表示一下就是这样：</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://q456qq520.github.io/post-images/1676885693905.png" alt="" loading="lazy"></figure>
<ol start="2">
<li>从子查询物化表的角度来看待，整个查询的意思其实是：对于子查询物化表的每个值来说，如果能在s1表中找到对应的key1列的值与该值相等的记录，那么就把这些记录加入到最终的结果集。画个图表示一下就是这样：</li>
</ol>
<figure data-type="image" tabindex="7"><img src="https://q456qq520.github.io/post-images/1676885722023.png" alt="" loading="lazy"></figure>
<p>也就是说其实上面的查询就相当于表s1和子查询物化表materialized_table进行内连接：</p>
<pre><code class="language-mysql">SELECT s1.* FROM s1 INNER JOIN materialized_table ON key1 = m_val;
</code></pre>
<p>转化成内连接之后，查询优化器可以评估不同连接顺序需要的成本是多少，选取成本最低的那种查询方式执行查询。我们分析一下上述查询中使用外层查询的表s1和物化表materialized_table进行内连接的成本都是由哪几部分组成的：<br>
如果使用s1表作为驱动表的话，总查询成本由下面几个部分组成：</p>
<ul>
<li>物化子查询时需要的成本</li>
<li>扫描s1表时的成本</li>
<li>s1表中的记录数量 × 通过m_val = xxx对materialized_table表进行单表访问的成本</li>
</ul>
<p>如果使用materialized_table表作为驱动表的话，总查询成本由下面几个部分组成：</p>
<ul>
<li>物化子查询时需要的成本</li>
<li>扫描物化表时的成本</li>
<li>物化表中的记录数量 × 通过key1 = xxx对s1表进行单表访问的成本</li>
</ul>
<p>MySQL查询优化器会通过运算来选择上述成本更低的方案来执行查询。</p>
<h5 id="将子查询转换为semi-join">将子查询转换为semi-join</h5>
<p>虽然将子查询进行物化之后再执行查询都会有建立临时表的成本，但是不管怎么说，我们见识到了将子查询转换为连接的强大作用，但是能不能不进行物化操作直接把子查询转换为连接呢？让我们重新审视一下上面的查询语句：</p>
<p>我们可以把这个查询理解成：对于s1表中的某条记录，如果我们能在s2表（准确的说是执行完WHERE s2.key3 = 'a'之后的结果集）中找到一条或多条记录，这些记录的common_field的值等于s1表记录的key1列的值，那么该条s1表的记录就会被加入到最终的结果集。这个过程其实和把s1和s2两个表连接起来的效果很像：</p>
<pre><code class="language-mysql">SELECT s1.* FROM s1 INNER JOIN s2 
    ON s1.key1 = s2.common_field 
    WHERE s2.key3 = 'a';
</code></pre>
<p>只不过我们不能保证对于s1表的某条记录来说，在s2表（准确的说是执行完WHERE s2.key3 = 'a'之后的结果集）中有多少条记录满足s1.key1 = s2.common_field这个条件，不过我们可以分三种情况讨论：</p>
<ul>
<li>情况一：对于s1表的某条记录来说，s2表中没有任何记录满足s1.key1 = s2.common_field这个条件，那么该记录自然也不会加入到最后的结果集。</li>
<li>情况二：对于s1表的某条记录来说，s2表中有且只有记录满足s1.key1 = s2.common_field这个条件，那么该记录会被加入最终的结果集。</li>
<li>情况三：对于s1表的某条记录来说，s2表中至少有2条记录满足s1.key1 = s2.common_field这个条件，那么该记录会被多次加入最终的结果集。</li>
</ul>
<p>对于s1表的某条记录来说，由于我们只关心s2表中是否存在记录满足s1.key1 = s2.common_field这个条件，而不关心具体有多少条记录与之匹配，又因为有情况三的存在，我们上面所说的IN子查询和两表连接之间并不完全等价。但是将子查询转换为连接又真的可以充分发挥优化器的作用，MySQL在这里提出了一个新概念 --- <code>半连接（英文名：semi-join）</code>。将s1表和s2表进行半连接的意思就是：<code>对于s1表的某条记录来说，我们只关心在s2表中是否存在与之匹配的记录是否存在，而不关心具体有多少条记录与之匹配，最终的结果集中只保留s1表的记录</code>。为了让大家有更直观的感受，我们假设MySQL内部是这么改写上面的子查询的：</p>
<pre><code class="language-mysql">SELECT s1.* FROM s1 SEMI JOIN s2
    ON s1.key1 = s2.common_field
    WHERE key3 = 'a';
</code></pre>
<p>概念是有了，怎么实现这种所谓的半连接呢？</p>
<ul>
<li><code>Table pullout （子查询中的表上拉）</code><br>
当子查询的查询列表处<code>只有主键或者唯一索引列</code>时，可以直接把子查询中的表上拉到外层查询的FROM子句中，并把子查询中的搜索条件合并到外层查询的搜索条件中，比如这个<br>
假设key2列是s2表的唯一二级索引列，所以我们可以直接把s2表上拉到外层查询的FROM子句中，并且把子查询中的搜索条件合并到外层查询的搜索条件中，上拉之后的查询就是这样的：</li>
</ul>
<pre><code class="language-mysql">SELECT s1.* FROM s1 INNER JOIN s2 
    ON s1.key2 = s2.key2 
    WHERE s2.key3 = 'a';
</code></pre>
<ul>
<li>
<p><code>DuplicateWeedout execution strategy （重复值消除）</code><br>
在执行连接查询的过程中，每当某条s1表中的记录要加入结果集时，就首先把这条记录的id值加入到这个临时表里，如果添加成功，说明之前这条s1表中的记录并没有加入最终的结果集，现在把该记录添加到最终的结果集；如果添加失败，说明这条之前这条s1表中的记录已经加入过最终的结果集，这里直接把它丢弃就好了，这种使用临时表消除semi-join结果集中的重复值的方式称之为DuplicateWeedout。</p>
</li>
<li>
<p><code>LooseScan execution strategy （松散索引扫描）</code><br>
假设有如下查询</p>
</li>
</ul>
<pre><code class="language-mysql">  SELECT * FROM s1 
    WHERE key3 IN (SELECT key1 FROM s2 WHERE key1 &gt; 'a' AND key1 &lt; 'b');
</code></pre>
<p>在子查询中，对于s2表的访问可以使用到key1列的索引，而恰好子查询的查询列表处就是key1列，这样在将该查询转换为半连接查询后，如果将s2作为驱动表执行查询的话，那么执行过程就是这样：<br>
<img src="https://q456qq520.github.io/post-images/1676886879861.png" alt="" loading="lazy"><br>
在s2表的idx_key1索引中，值为'aa'的二级索引记录一共有3条，那么只需要取第一条的值到s1表中查找s1.key3 = 'aa'的记录，如果能在s1表中找到对应的记录，那么就把对应的记录加入到结果集。依此类推，其他值相同的二级索引记录，也只需要取第一条记录的值到s1表中找匹配的记录，这种虽然是扫描索引，但只取值相同的记录的第一条去做匹配操作的方式称之为松散索引扫描。</p>
<ul>
<li>
<p><code>Semi-join Materialization execution strategy</code><br>
先把外层查询的IN子句中的不相关子查询进行物化，然后再进行外层查询的表和物化表的连接本质上也算是一种semi-join，只不过由于物化表中没有重复的记录，所以可以直接将子查询转为连接查询。</p>
</li>
<li>
<p><code>FirstMatch execution strategy （首次匹配）</code><br>
FirstMatch是一种最原始的半连接执行方式，就是说先取一条外层查询的中的记录，然后到子查询的表中寻找符合匹配条件的记录，如果能找到一条，则将该外层查询的记录放入最终的结果集并且停止查找更多匹配的记录，如果找不到则把该外层查询的记录丢弃掉；然后再开始取下一条外层查询中的记录，重复上面这个过程。</p>
</li>
</ul>
<p>如果子查询的查询列表处只有主键或者唯一二级索引列，还可以直接使用table pullout的策略来执行查询，但是需要大家注意的是，由于<code>相关子查询并不是一个独立的查询，所以不能转换为物化表来执行查询</code>。</p>
<p><strong>semi-join的适用条件</strong><br>
并不是所有包含IN子查询的查询语句都可以转换为semi-join，只有形如这样的查询才可以被转换为semi-join：</p>
<pre><code class="language-mysql">SELECT ... FROM outer_tables 
    WHERE expr IN (SELECT ... FROM inner_tables ...) AND ...

SELECT ... FROM outer_tables 
    WHERE (oe1, oe2, ...) IN (SELECT ie1, ie2, ... FROM inner_tables ...) AND ...
</code></pre>
<p>1、 该子查询必须是和IN语句组成的布尔表达式，并且在外层查询的WHERE或者ON子句中出现。<br>
2、外层查询也可以有其他的搜索条件，只不过和IN子查询的搜索条件必须使用AND连接起来。<br>
3、该子查询必须是一个单一的查询，不能是由若干查询由UNION连接起来的形式。<br>
4、该子查询不能包含GROUP BY或者HAVING语句或者聚集函数。</p>
<p><strong>不适用于semi-join的情况</strong><br>
1、外层查询的WHERE条件中有其他搜索条件与IN子查询组成的布尔表达式使用OR连接起来<br>
2、使用NOT IN而不是IN的情况<br>
3、在SELECT子句中的IN子查询的情况（SELECT key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a') FROM s1 ;）<br>
4、子查询中包含GROUP BY、HAVING或者聚集函数的情况<br>
5、子查询中包含UNION的情况</p>
<p>MySQL仍然留了两手绝活来优化不能转为semi-join查询的子查询，那就是：<br>
1、对于不相关子查询来说，可以尝试把它们物化之后再参与查询</p>
<pre><code class="language-mysql">SELECT * FROM s1 
    WHERE key1 NOT IN (SELECT common_field FROM s2 WHERE key3 = 'a')
</code></pre>
<p>先将子查询物化，然后再判断key1是否在物化表的结果集中可以加快查询执行的速度。这里将子查询物化之后不能转为和外层查询的表的连接，只能是先扫描s1表，然后对s1表的某条记录来说，判断该记录的key1值在不在物化表中。<br>
2、不管子查询是相关的还是不相关的，都可以把IN子查询尝试专为EXISTS子查询<br>
链接:<a href="/post/lesslesscong-gen-er-shang-li-jie-mysqlgreatergreater-du-shu-bi-ji-si">《从根儿上理解MySQL》读书笔记(四)</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[动态线程池]]></title>
        <id>https://q456qq520.github.io/post/dong-tai-xian-cheng-chi/</id>
        <link href="https://q456qq520.github.io/post/dong-tai-xian-cheng-chi/">
        </link>
        <updated>2023-01-10T06:41:44.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="一-动态线程池">一 动态线程池</h2>
<h3 id="11-线程池是什么">1.1 线程池是什么</h3>
]]></summary>
        <content type="html"><![CDATA[<h2 id="一-动态线程池">一 动态线程池</h2>
<h3 id="11-线程池是什么">1.1 线程池是什么</h3>
<!-- more -->
<p>线程池（Thread Pool）是一种基于池化思想管理线程的工具。</p>
<p>线程过多会带来额外的开销，其中包括创建销毁线程的开销、调度线程的开销等等，同时也降低了计算机的整体性能。线程池维护多个线程，等待监督管理者分配可并发执行的任务。线程池一方面避免了处理任务时创建销毁线程开销的代价，另一方面避免了线程数量膨胀导致的过分调度问题，保证了对内核的充分利用。</p>
<p>而本文描述线程池是JDK中提供的ThreadPoolExecutor类。当然，使用线程池可以带来一系列好处：</p>
<ol>
<li>降低资源消耗：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。</li>
<li>提高响应速度：任务到达时，无需等待线程创建即可立即执行。</li>
<li>提高线程的可管理性：线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控。</li>
<li>提供更多更强大的功能：线程池具备可拓展性，允许开发人员向其中增加更多的功能。比如延时定时线程池ScheduledThreadPoolExecutor，就允许任务延期执行或定期执行。</li>
</ol>
<h3 id="12-线程池解决的问题是什么">1.2 线程池解决的问题是什么</h3>
<p>线程池解决的核心问题就是资源管理问题。在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入。这种不确定性将带来以下若干问题：</p>
<ol>
<li>频繁申请/销毁资源和调度资源，将带来额外的消耗，可能会非常巨大。</li>
<li>对资源无限申请缺少抑制手段，易引发系统资源耗尽的风险。</li>
<li>系统无法合理管理内部的资源分布，会降低系统的稳定性。</li>
</ol>
<p>为解决资源分配这个问题，线程池采用了“池化”（Pooling）思想。池化，顾名思义，是为了最大化收益并最小化风险，而将资源统一在一起管理的一种思想。除去线程池，还有其他比较典型的几种使用策略包括：</p>
<ol>
<li>内存池(Memory Pooling)：预先申请内存，提升申请内存速度，减少内存碎片。</li>
<li>连接池(Connection Pooling)：预先申请数据库连接，提升申请连接的速度，降低系统的开销。</li>
<li>实例池(Object Pooling)：循环使用对象，减少资源在初始化和释放时的昂贵损耗。</li>
</ol>
<h2 id="二-线程池核心设计与实现">二 线程池核心设计与实现</h2>
<p>线程池是一种通过“池化”思想，帮助我们管理线程而获取并发性的工具，在Java中的体现是ThreadPoolExecutor类。那么它的的详细设计与实现是什么样的呢？</p>
<h3 id="21-总体设计">2.1 总体设计</h3>
<p>ThreadPoolExecutor实现的顶层接口是Executor，顶层接口Executor提供了一种思想：将任务提交和任务执行进行解耦。用户无需关注如何创建线程，如何调度线程来执行任务，用户只需提供Runnable对象，将任务的运行逻辑提交到执行器(Executor)中，由Executor框架完成线程的调配和任务的执行部分。</p>
<p>ExecutorService接口增加了一些能力：<br>
（1）扩充执行任务的能力，补充可以为一个或一批异步任务生成Future的方法；<br>
（2）提供了管控线程池的方法，比如停止线程池的运行。</p>
<p>AbstractExecutorService则是上层的抽象类，将执行任务的流程串联了起来，保证下层的实现只需关注一个执行任务的方法即可。最下层的实现类ThreadPoolExecutor实现最复杂的运行部分，ThreadPoolExecutor将会一方面维护自身的生命周期，另一方面同时管理线程和任务，使两者良好的结合从而执行并行任务。</p>
<p>ThreadPoolExecutor是如何运行，如何同时维护线程和执行任务的呢？其运行机制如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1676015194049.png" alt="" loading="lazy"></p>
<p>线程池在内部实际上构建了一个生产者消费者模型，将线程和任务两者解耦，并不直接关联，从而良好的缓冲任务，复用线程。线程池的运行主要分成两部分：</p>
<ul>
<li>任务管理</li>
<li>线程管理</li>
</ul>
<p>任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转：<br>
（1）直接申请线程执行该任务；<br>
（2）缓冲到队列中等待线程执行；<br>
（3）拒绝该任务。</p>
<p>线程管理部分是消费者，它们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后则会继续获取新的任务去执行，最终当线程获取不到任务的时候，线程就会被回收。</p>
<h3 id="22-生命周期管理">2.2 生命周期管理</h3>
<p>线程池运行的状态，并不是用户显式设置的，而是伴随着线程池的运行，由内部来维护。线程池内部使用一个变量维护两个值：<code>运行状态(runState)</code>和<code>线程数量 (workerCount)</code>。在具体实现中，线程池将运行状态(runState)、线程数量 (workerCount)两个关键参数的维护放在了一起，如下代码所示：</p>
<blockquote>
<p>ThreadPoolExecutor</p>
</blockquote>
<pre><code class="language-java">private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
private static int ctlOf(int rs, int wc) { return rs | wc; }
private static final int COUNT_BITS = Integer.SIZE - 3;
private static final int RUNNING    = -1 &lt;&lt; COUNT_BITS;
</code></pre>
<p>ctl这个AtomicInteger类型，是对线程池的运行状态和线程池中有效线程的数量进行控制的一个字段， 它同时包含两部分的信息：线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，高3位保存runState，低29位保存workerCount，两个变量之间互不干扰。用一个变量去存储两个值，可避免在做相关决策时，出现不一致的情况，不必为了维护两者的一致，而占用锁资源。</p>
<p>关于内部封装的获取生命周期状态、获取线程池线程数量的计算方法如以下代码所示：</p>
<blockquote>
<p>ThreadPoolExecutor</p>
</blockquote>
<pre><code class="language-java">private static int runStateOf(int c) { return c &amp; ~CAPACITY; } //计算当前运行状态
private static int workerCountOf(int c) { return c &amp; CAPACITY; } //计算当前线程数量
private static int ctlOf(int rs, int wc) { return rs | wc; } //通过状态和线程数生成ctl
</code></pre>
<p>ThreadPoolExecutor的运行状态有5种，分别为：</p>
<table>
<thead>
<tr>
<th>运行状态</th>
<th>状态描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>RUNNING</td>
<td>能接受新提交的任务，并且也能处理阻塞队列中的任务</td>
</tr>
<tr>
<td>SHOTDOWN</td>
<td>关闭状态，不再能接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务</td>
</tr>
<tr>
<td>STOP</td>
<td>不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程</td>
</tr>
<tr>
<td>TIDYING</td>
<td>所有任务都已终止了，workerCount为0</td>
</tr>
<tr>
<td>TERMINATED</td>
<td>在terminated()方法执行完后进入该状态</td>
</tr>
</tbody>
</table>
<p>其生命周期转换如下入所示：<br>
<img src="https://q456qq520.github.io/post-images/1676016704968.png" alt="" loading="lazy"></p>
<h3 id="23-任务执行机制">2.3 任务执行机制</h3>
<h4 id="231-任务调度">2.3.1 任务调度</h4>
<p>当用户提交了一个任务，接下来这个任务将如何执行都是由这个阶段决定的。所有任务的调度都是由<code>execute</code>方法完成的，这部分完成的工作是：<strong>检查现在线程池的运行状态、运行线程数、运行策略，决定接下来执行的流程，是直接申请线程执行，或是缓冲到队列中执行，亦或是直接拒绝该任务</strong>。其执行过程如下：</p>
<ol>
<li>首先检测线程池运行状态，如果不是RUNNING，则直接拒绝，线程池要保证在RUNNING的状态下执行任务。</li>
<li>如果workerCount &lt; corePoolSize，则创建并启动一个线程来执行新提交的任务。</li>
<li>如果workerCount &gt;= corePoolSize，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中。</li>
<li>如果workerCount &gt;= corePoolSize &amp;&amp; workerCount &lt; maximumPoolSize，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务。</li>
<li>如果workerCount &gt;= maximumPoolSize，并且线程池内的阻塞队列已满, 则根据拒绝策略来处理该任务, 默认的处理方式是直接抛异常。</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1676016939974.png" alt="" loading="lazy"></figure>
<h4 id="232-任务缓冲">2.3.2 任务缓冲</h4>
<p>任务缓冲模块是线程池能够管理任务的核心部分。线程池的本质是对任务和线程的管理，而做到这一点最关键的思想就是将任务和线程两者解耦，不让两者直接关联，才可以做后续的分配工作。线程池中是以生产者消费者模式，通过一个<code>阻塞队列</code>来实现的。阻塞队列缓存任务，工作线程从阻塞队列中获取任务。</p>
<p>阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作是：<strong>在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用</strong>。</p>
<p>阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p>
<p>下图中展示了线程1往阻塞队列中添加元素，而线程2从阻塞队列中移除元素：<br>
<img src="https://q456qq520.github.io/post-images/1676017060970.png" alt="" loading="lazy"></p>
<p>使用不同的队列可以实现不一样的任务存取策略。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>ArrayBlockingQueue</td>
<td>用数组实现的有界阻塞队列，此队列按照先进先出(FIFO)的原则对元素进行排序。支持公平锁和非公平锁。</td>
</tr>
<tr>
<td>LinkedBlockingQueue</td>
<td>由链表结构组成的有界队列，此队列按照先进先出(FIFO)的原则对元素进行排序。此队列的默认长度为Integer.MAX_VALUE，所以默认创建的该队列有容量危险。</td>
</tr>
<tr>
<td>PriorityBlockingQueue</td>
<td>支持线程优先级排序的无界队列，默认自然序进行排序，也可以自定义实现compareTo()方法来指定元素排序规则，不能保证同优先级元素的顺序。</td>
</tr>
<tr>
<td>DelayQueue</td>
<td>实现PriorityBlockingQueue实现延迟获取的无界队列，在创建元素时，可以指定多久才能从队列中获取当前元素。只有延时期满后才能从队列中获取元素。</td>
</tr>
<tr>
<td>SynchronousQueue</td>
<td>一个不存储元素的阻塞队列，每一个put操作必须等待take操作，否则不能添加元素。支持公平锁和非公平锁。SynchronousQueue的一个使用场景是在线程池里。Executors.newCachedThreadPool()就使用了SynchronousQueue，这个线程池根据需要(新任务到来时)创建新的线程，如果有空闲线程则会重复使用线程空闲了60秒后会被回收。</td>
</tr>
<tr>
<td>LinkedTransferQueue</td>
<td>一个由链表结构组成的无界阻塞队列，相比于其它队列，LinkedTransferQueue队列多了transfer和tryTransfer方法</td>
</tr>
<tr>
<td>LinkedBlockingDeque</td>
<td>一个由链表结构组成的双向阻塞队列。队列头部和尾部都可以添加和移除元素，多线程并发时，可以将锁的竞争最多降到一半。</td>
</tr>
</tbody>
</table>
<h4 id="233-任务申请">2.3.3 任务申请</h4>
<p>任务的执行有两种可能：一种是任务直接由新创建的线程执行。另一种是线程从任务队列中获取任务然后执行，执行完任务的空闲线程会再次去从队列中申请任务再去执行。第一种情况仅出现在线程初始创建的时候，第二种是线程获取任务绝大多数的情况。</p>
<p>线程需要从任务缓存模块中不断地取任务执行，帮助线程从阻塞队列中获取任务，实现线程管理模块和任务管理模块之间的通信。这部分策略由getTask方法实现，其执行流程如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1676019003211.png" alt="" loading="lazy"></p>
<p>getTask这部分进行了多次判断，为的是控制线程的数量，使其符合线程池的状态。如果线程池现在不应该持有那么多线程，则会返回null值。工作线程Worker会不断接收新任务去执行，而当工作线程Worker接收不到任务的时候，就会开始被回收。</p>
<h4 id="234-任务拒绝">2.3.4 任务拒绝</h4>
<p>任务拒绝模块是线程池的保护部分，线程池有一个最大的容量，当线程池的任务缓存队列已满，并且线程池中的线程数目达到maximumPoolSize时，就需要拒绝掉该任务，采取任务拒绝策略，保护线程池。</p>
<p>拒绝策略是一个接口，其设计如下：</p>
<pre><code class="language-java">public interface RejectedExecutionHandler {
    void rejectedExecution(Runnable r, ThreadPoolExecutor executor);
}
</code></pre>
<p>用户可以通过实现这个接口去定制拒绝策略，也可以选择JDK提供的四种已有拒绝策略，其特点如下：</p>
<ol>
<li>CallerRunsPolicy（调用者运行策略）<br>
当触发拒绝策略时，只要线程池没有关闭，就由提交任务的当前线程处理。</li>
<li>AbortPolicy（中止策略）<br>
当触发拒绝策略时，直接抛出拒绝执行的异常，中止策略的意思也就是打断当前执行流程</li>
<li>DiscardPolicy（丢弃策略）<br>
直接静悄悄的丢弃这个任务，不触发任何动作</li>
<li>DiscardOldestPolicy（弃老策略）<br>
如果线程池未关闭，就弹出队列头部的元素，然后尝试执行</li>
</ol>
<h3 id="24-worker线程管理">2.4 Worker线程管理</h3>
<h4 id="241-worker线程">2.4.1 Worker线程</h4>
<p>线程池为了掌握线程的状态并维护线程的生命周期，设计了线程池内的工作线程Worker。我们来看一下它的部分代码：</p>
<pre><code class="language-java">private final class Worker extends AbstractQueuedSynchronizer implements Runnable{
    final Thread thread;//Worker持有的线程
    Runnable firstTask;//初始化的任务，可以为null
}
</code></pre>
<p>Worker这个工作线程，实现了Runnable接口，并持有一个线程thread，一个初始化的任务firstTask。thread是在调用构造方法时通过ThreadFactory来创建的线程，可以用来执行任务；firstTask用它来保存传入的第一个任务，这个任务可以有也可以为null。如果这个值是非空的，那么线程就会在启动初期立即执行这个任务，也就对应核心线程创建时的情况；如果这个值是null，那么就需要创建一个线程去执行任务列表（workQueue）中的任务，也就是非核心线程的创建。</p>
<p>Worker执行任务的模型如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1676019681958.png" alt="" loading="lazy"></p>
<p>线程池需要管理线程的生命周期，需要在线程长时间不运行的时候进行回收。线程池使用一张Hash表去持有线程的引用，这样可以通过添加引用、移除引用这样的操作来控制线程的生命周期。这个时候重要的就是如何判断线程是否在运行。</p>
<p>​Worker是通过继承<code>AQS</code>，使用AQS来实现独占锁这个功能。没有使用可重入锁ReentrantLock，而是使用AQS，为的就是实现不可重入的特性去反应线程现在的执行状态。</p>
<ol>
<li>lock方法一旦获取了独占锁，表示当前线程正在执行任务中。</li>
<li>如果正在执行任务，则不应该中断线程。</li>
<li>如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断。</li>
<li>线程池在执行shutdown方法或tryTerminate方法时会调用interruptIdleWorkers方法来中断空闲的线程，interruptIdleWorkers方法会使用tryLock方法来判断线程池中的线程是否是空闲状态；如果线程是空闲状态则可以安全回收。</li>
</ol>
<p>在线程回收过程中就使用到了这种特性，回收过程如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1676019951439.png" alt="" loading="lazy"></p>
<h4 id="242-worker线程增加">2.4.2 Worker线程增加</h4>
<p>增加线程是通过线程池中的<code>addWorker</code>方法，该方法的功能就是增加一个线程，该方法不考虑线程池是在哪个阶段增加的该线程，这个分配线程的策略是在上个步骤完成的，该步骤仅仅完成增加线程，并使它运行，最后返回是否成功这个结果。addWorker方法有两个参数：<code>firstTask</code>、<code>core</code>。firstTask参数用于指定新增的线程执行的第一个任务，该参数可以为空；core参数为true表示在新增线程时会判断当前活动线程数是否少于<code>corePoolSize</code>，false表示新增线程前需要判断当前活动线程数是否少于<code>maximumPoolSize</code>，其执行流程如下图所示：<br>
<img src="https://q456qq520.github.io/post-images/1676020127904.png" alt="" loading="lazy"></p>
<h4 id="243-worker线程回收">2.4.3 Worker线程回收</h4>
<p>线程池中线程的销毁依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程被JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可。</p>
<p>Worker被创建出来后，就会不断地进行轮询，然后获取任务去执行，核心线程可以无限等待获取任务，非核心线程要限时获取任务。当Worker无法获取到任务，也就是获取的任务为空时，循环会结束，Worker会主动消除自身在线程池内的引用。</p>
<pre><code class="language-java">try {
  while (task != null || (task = getTask()) != null) {
    //执行任务
  }
} finally {
 //线程回收的工作是在processWorkerExit方法完成的。
  processWorkerExit(w, completedAbruptly);//获取不到任务时，主动回收自己
}
</code></pre>
<p>事实上，在这个方法中，将线程引用移出线程池就已经结束了线程销毁的部分。但由于引起线程销毁的可能性有很多，线程池还要判断是什么引发了这次销毁，是否要改变线程池的现阶段状态，是否要根据新状态，重新分配线程。</p>
<h4 id="244-worker线程执行任务">2.4.4 Worker线程执行任务</h4>
<p>在Worker类中的run方法调用了runWorker方法来执行任务，runWorker方法的执行过程如下：</p>
<ol>
<li>while循环不断地通过getTask()方法获取任务。</li>
<li>getTask()方法从阻塞队列中取任务。</li>
<li>如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态。</li>
<li>执行任务。</li>
<li>如果getTask结果为null则跳出循环，执行processWorkerExit()方法，销毁线程。</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1676021083966.png" alt="" loading="lazy"></figure>
<h2 id="三-线程池在业务中的实践">三 线程池在业务中的实践</h2>
<h3 id="31-动态化线程池">3.1 动态化线程池</h3>
<p>我们是否可以将线程池的参数从代码中迁移到分布式配置中心上，实现线程池参数可动态配置和即时生效。</p>
<h4 id="331-整体设计">3.3.1 整体设计</h4>
<p>动态化线程池的核心设计包括以下三个方面：</p>
<ol>
<li>简化线程池配置：线程池构造参数有8个，但是最核心的是3个：corePoolSize、maximumPoolSize，workQueue，它们最大程度地决定了线程池的任务分配和线程分配策略。考虑到在实际应用中我们获取并发性的场景主要是两种：<br>
（1）并行执行子任务，提高响应速度。这种情况下，应该使用同步队列，没有什么任务应该被缓存下来，而是应该立即执行。<br>
（2）并行执行大批次任务，提升吞吐量。这种情况下，应该使用有界队列，使用队列去缓冲大批量的任务，队列容量必须声明，防止任务无限制堆积。<br>
所以线程池只需要提供这三个关键参数的配置，并且提供两种队列的选择，就可以满足绝大多数的业务需求，Less is More。</li>
<li>参数可动态修改：为了解决参数不好配，修改参数成本高等问题。在Java线程池留有高扩展性的基础上，封装线程池，允许线程池监听同步外部的消息，根据消息进行修改配置。将线程池的配置放置在平台侧，允许开发简单的查看、修改线程池配置。</li>
<li>增加线程池监控：对某事物缺乏状态的观测，就对其改进无从下手。在线程池执行任务的生命周期添加监控能力，帮助开发了解线程池状态。</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1676021380968.png" alt="" loading="lazy"></figure>
<h4 id="332-功能架构">3.3.2 功能架构</h4>
<ol>
<li>动态调参：支持线程池参数动态调整、界面化操作；包括修改线程池核心大小、最大核心大小、队列长度等；参数修改后及时生效。</li>
<li>任务监控：支持应用粒度、线程池粒度、任务粒度的Transaction监控；可以看到线程池的任务执行情况、最大任务执行时间、平均任务执行时间、95/99线等。</li>
<li>负载告警：线程池队列任务积压到一定值的时候会通知应用开发负责人；当线程池负载数达到一定阈值的时候会通知应用开发负责人。</li>
<li>操作监控：创建/修改和删除线程池都会通知到应用的开发负责人。</li>
<li>操作日志：可以查看线程池参数的修改记录，谁在什么时候修改了线程池参数、修改前的参数值是什么。</li>
<li>权限校验：只有应用开发负责人才能够修改应用的线程池参数。</li>
</ol>
<p><strong>参数动态化</strong><br>
JDK允许线程池使用方通过ThreadPoolExecutor的实例来动态设置线程池的核心策略，以<code>setCorePoolSize</code>为方法例，在运行期线程池使用方调用此方法设置<code>corePoolSize</code>之后，线程池会直接覆盖原来的corePoolSize值，并且基于当前值和原始值的比较结果采取不同的处理策略。对于当前值小于当前工作线程数的情况，说明有多余的worker线程，此时会向当前<code>idle</code>的<code>worker</code>线程发起中断请求以实现回收，多余的worker在下次idel的时候也会被回收；对于当前值大于原始值且当前队列中有待执行任务，则线程池会创建新的worker线程来执行队列任务。</p>
<p><strong>线程池监控</strong><br>
除了参数动态化之外，为了更好地使用线程池，我们需要对线程池的运行状况有感知，比如当前线程池的负载是怎么样的？分配的资源够不够用？任务的执行情况是怎么样的？是长任务还是短任务？基于对这些问题的思考，动态化线程池提供了多个维度的监控和告警能力，包括：线程池活跃度、任务的执行Transaction（频率、耗时）、Reject异常、线程池内部统计信息等等，既能帮助用户从多个维度分析线程池的使用情况，又能在出现问题第一时间通知到用户，从而避免故障或加速故障恢复。</p>
<ol>
<li>
<p>负载监控和告警<br>
线程池负载关注的核心问题是：基于当前线程池参数分配的资源够不够。对于这个问题，我们可以从事前和事中两个角度来看。事前，线程池定义了“活跃度”这个概念，来让用户在发生Reject异常之前能够感知线程池负载问题，线程池活跃度计算公式为：<code>线程池活跃度 = activeCount/maximumPoolSize</code>。这个公式代表<mark>当活跃线程数趋向于maximumPoolSize的时候，代表线程负载趋高</mark>。事中，也可以从两方面来看线程池的过载判定条件，一个是发生了Reject异常，一个是队列中有等待任务（支持定制阈值）。以上两种情况发生了都会触发告警，告警信息会通过大象推送给服务所关联的负责人。</p>
</li>
<li>
<p>任务级精细化监控<br>
在传统的线程池应用场景中，线程池中的任务执行情况对于用户来说是不透明的。比如在一个具体的业务场景中，业务开发申请了一个线程池同时用于执行两种任务，一个是发消息任务、一个是发短信任务，这两类任务实际执行的频率和时长对于用户来说没有一个直观的感受，很可能这两类任务不适合共享一个线程池，但是由于用户无法感知，因此也无从优化。动态化线程池内部实现了任务级别的埋点，且允许为不同的业务任务指定具有业务含义的名称，线程池内部基于这个名称做<code>Transaction打点</code>，基于这个功能，用户可以看到线程池内部任务级别的执行情况，且区分业务。</p>
</li>
<li>
<p>运行时状态实时查看<br>
用户基于JDK原生线程池ThreadPoolExecutor提供的几个public的getter方法，可以读取到当前线程池的运行状态以及参数。用户基于这个功能可以了解线程池的实时状态，比如当前有多少个工作线程，执行了多少个任务，队列中等待的任务数等等。</p>
</li>
</ol>
]]></content>
    </entry>
</feed>